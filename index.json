[{"categories":["macOS","scripting"],"content":"Spotlight is a system wide metadata indexing system that Apple has been shipping since OS X 10.4 Tiger, and has been improved over the years with each OS. I like using Spotlight for some tasks and general searching for files in Terminal.app. I also use it for finding anything on my Mac. Typically, I do not use my mouse or trackpad to find files or navigate to folders. One just needs to hit cmd + spacebar on their Mac to pull up the Spotlight search menu and start typing. If you use a Mac, there is a good chance you also use this regularly like I do. This isn’t anything groundbreaking or new, but I have enjoyed using Spotlight over the years when it fits as a good tool. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:0","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Spotlight in the Terminal getting attributes There are two binaries Apple supplies on macOS which you can leverage. They are mdls and mdfind, there is also another tool called xattr which also allows you to manipulate metadata in code. To start using these binaries we can just take a look at an application like say Firefox. Refer to their man pages for the documentation. % mdls /Applications/Firefox.app _kMDItemDisplayNameWithExtensions = \"Firefox.app\" kMDItemAlternateNames = ( \"Firefox.app\" ) kMDItemAppStoreCategory = \"Productivity\" kMDItemAppStoreCategoryType = \"public.app-category.productivity\" kMDItemCFBundleIdentifier = \"org.mozilla.firefox\" kMDItemContentCreationDate = 2020-08-31 18:05:40 +0000 kMDItemContentCreationDate_Ranking = 2020-08-31 00:00:00 +0000 kMDItemContentModificationDate = 2020-09-08 17:37:19 +0000 kMDItemContentModificationDate_Ranking = 2020-09-08 00:00:00 +0000 kMDItemContentType = \"com.apple.application-bundle\" kMDItemContentTypeTree = ( \"com.apple.application-bundle\", \"com.apple.application\", \"public.executable\", \"com.apple.localizable-name-bundle\", \"com.apple.bundle\", \"public.directory\", \"public.item\", \"com.apple.package\" ) kMDItemDateAdded = 2020-09-08 17:35:57 +0000 kMDItemDateAdded_Ranking = 2020-09-08 00:00:00 +0000 kMDItemDisplayName = \"Firefox\" kMDItemDocumentIdentifier = 0 kMDItemExecutableArchitectures = ( \"x86_64\" ) kMDItemFSContentChangeDate = 2020-09-08 17:37:19 +0000 kMDItemFSCreationDate = 2020-08-31 18:05:40 +0000 kMDItemFSCreatorCode = \"\" kMDItemFSFinderFlags = 0 kMDItemFSHasCustomIcon = (null) kMDItemFSInvisible = 0 kMDItemFSIsExtensionHidden = 1 kMDItemFSIsStationery = (null) kMDItemFSLabel = 0 kMDItemFSName = \"Firefox.app\" kMDItemFSNodeCount = 1 kMDItemFSOwnerGroupID = 80 kMDItemFSOwnerUserID = 0 kMDItemFSSize = 210713091 kMDItemFSTypeCode = \"\" kMDItemInterestingDate_Ranking = 2020-09-08 00:00:00 +0000 kMDItemKind = \"Application\" kMDItemLastUsedDate = 2020-09-08 17:37:09 +0000 kMDItemLastUsedDate_Ranking = 2020-09-08 00:00:00 +0000 kMDItemLogicalSize = 210713091 kMDItemPhysicalSize = 211038208 kMDItemUseCount = 1 kMDItemUsedDates = ( \"2020-09-08 07:00:00 +0000\" ) kMDItemVersion = \"80.0.1\" There are many metadata tags we can get right from the shell as well. % mdls /Applications/Firefox.app -name kMDItemVersion kMDItemVersion = \"80.0.1\" You can see this is easy to get the metadata from an object on disk, it is also very fast as it searches the indexes and returns the indexed data. % mdls /Applications/Firefox.app -name kMDItemVersion -raw 80.0.1 There is also no need to pipe to awk or grep as the tool gives you an argument of -raw to just return the data. Easily get multiple metadata attributes by passing multiple arguments. % mdls /Applications/Firefox.app -name kMDItemVersion -name kMDItemFSName -name kMDItemLastUsedDate kMDItemFSName = \"Firefox.app\" kMDItemLastUsedDate = 2020-09-08 17:37:09 +0000 kMDItemVersion = \"80.0.1\" ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:1","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Spotlight in the Terminal searching mdls is great for listing the metadata for objects on disk, and you can probably guess what mdfind is used for. After getting the metadata tags from mdls we can use them in mdfind you can also pass strings to mdfind and it performs searches similar to when you use it in the Finder. % mdfind \"using-spotlight\" /Users/tlarkin/blog/tlark/content/posts/using-spotlight-macos.md /Users/tlarkin/Library/Application Support/JetBrains/IdeaIC2020.1/workspace/1hksXRhYWu8MI2RnUXoH3oitxRW.xml Like finding the blog post I am currently working on, which my IDE is also storing data about on disk. This output makes sense considering I do all my blog work in my IDE with hugo along with built in tools. We can also use the metadata tags which we see in mdls output. % mdfind \"kMDItemFSName = Firefox.app\" /Applications/Firefox.app You can specify file paths if you have an idea or what to limit search scope for metadata searching. % mdfind -onlyin /Users/tlarkin \"kMDItemContentType = public.python-script\" | wc -l 11582 I have a lot of Python scripts on my Mac, so I just did a line count of the output. I have a lot of repos cloned, open source tools cloned, and there are a lot of apps that use Python, so a big chunk of these results are from the software I have installed. However, every object in my home folder that has kMDItemContentType = public.python-script tag will return from this search. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:2","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Using mdfind with mdls Of course with things like pipes in the shell you can easily use these tools together. % mdfind -0 -onlyin /Applications \"kMDItemContentType = com.apple.application-bundle\" | xargs -0 mdls -name kMDItemCFBundleIdentifier -name kMDItemDisplayName -name kMDItemVersion There is a lot of output I won’t paste into a code block here, but you can play around with this to see the different output you can get from these binaries. Last year there was a pretty nasty iTerm2 Vuln that we detected through our security tools, and the fact it was all over Twitter and the rest of the Internet. We wanted to detect how many vulnerable versions we had adn then plan on patching after assessing how many vuln versions we had. A simple spotlight script was deployed, and anything that wasn’t teh current version got flagged. Since iTerm2 is just a zipped App bundle, we had no idea where the user installed it. We also do not block users from installing their own software and tools, so there could be multiple versions present. Tools like Jamf Pro will not search for Apps outside of a default path unless you specify so in the inventory collection preferences. A quick Spotlight script made sense and it didn’t require much effort. There are tools out there like OSquery which is probably a better answer to get data about your fleet on a regular basis. #!/bin/zsh results=$(mdfind -name \"kMDItemCFBundleIdentifier = com.googlecode.iterm2 \u0026\u0026 kMDItemVersion != 3.3.6\") if [[ \"${results}\" == \"\" ]] then echo \"\u003cresult\u003efalse\u003c/result\u003e\" else echo \"\u003cresult\u003etrue\u003c/result\u003e\" fi With a day we had the data back, and we had very few users with a vulnerable version on their system as most users were updating their versions on their own. So, we just had to send an update to a few systems. I do think there are better ways and better tools to manage application state out there, but if you don’t have those every Mac has Spotlight on it. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:3","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Metadata Tagging None of this is new, and I am certainly not the first person to use these tools or blog about this stuff. The linked blog has some pretty good reading material you can reference. A quick reference how you can do this below. # create a file touch file.txt # add a custom tag xattr -w com.apple.metadata:\"_customTag\" \"customVal1\" file.txt # get the attribute mdls -name _customTag file.txt -raw customVal1 # search for it mdfind \"_customTag = customVal1\" /Users/tlarkin/file.txt There are a lot of things you could do with metadata tagging in macOS. You can programmatically search for metadata tags, as well as set your own tags. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:4","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Caveats with Custom Tagging When tagging things on disk that get overwritten, like contents of an installer package, the tags could be deleted. So, if you were to say tag an app you installed with your automation tools, then a user downloads another version and overwrites it, your tags, are now probably gone. Some file paths are not indexed by Spotlight. Paths like /tmp are not indexed. So, make sure you test the paths you are using if you use these tools. I have definitely forgotten this before and tested custom tags with xattr in /tmp and completely spaced that this doesn’t work. Some file names are error prone with certain characters. Apple, I think has fixed this bug as I can now tag a file located in a folder with a . in the folder name. This was broken at some point in time, Mac Mule discovered it. I was not able to locate his blog post on it, but will update with a link if I can find it. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:5","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Python Objc Bridge You can also do all of these neat things in Python as well. I wrote this script not too long ago to search and find all the Parallels VMs on a Mac. The end goal was to collect the OS versions of all VMs and ship that data to Snowflake, so we could report on them for vulnerabilities on the OS. #!/usr/bin/python \"\"\" this is a script to detect what VMs are on a system and escrow what OS they are You can run this daily or in another workflow It will use Spotlight to find files with the .pvm extension to locate where the files are on the file system then parse the PvInfo file for Parallels VMs credit: https://github.com/munki/munki/blob/b6a4b015297c262124fb80086b6b55e329a4fec0/code/client/munkilib/info.py#L362-L396 \"\"\" # import modules import xml.etree.ElementTree as et from Foundation import NSMetadataQuery, NSPredicate, NSRunLoop, NSDate # start functions def get_vms(): \"\"\"use spotlight to find parallels VM files\"\"\" file_list = [] query = NSMetadataQuery.alloc().init() query.setPredicate_( NSPredicate.predicateWithFormat_( \"(kMDItemContentType == 'com.parallels.vm.vmpackage')\" ) ) query.setSearchScopes_([\"/Applications\", \"/Users\"]) query.startQuery() start_time = 0 max_time = 20 while query.isGathering() and start_time \u003c= max_time: start_time += 0.3 NSRunLoop.currentRunLoop().runUntilDate_( NSDate.dateWithTimeIntervalSinceNow_(0.3) ) query.stopQuery() # get the results of the file names, and find their file path via spotlight attribute for item in query.results(): pathname = item.valueForAttribute_(\"kMDItemPath\") if pathname: file_list.append(pathname) return file_list def get_vm_os(vm_list): \"\"\"feed this function a list of results from Spotlight to parse what VM is running what OS\"\"\" # blank list to populate data in os_list = [] # loop through return from spotlight and grab needed info for vm in vm_list: path = str(vm + \"/VmInfo.pvi\") # load XML file into parser tree = et.ElementTree(file=path) root = tree.getroot() # find the text of the tags we want for element in root.iter(): if element.tag == \"RealOsType\" and element.text is not None: # only want the data up until the first comma os_type = element.text.split(\",\")[0] if element.tag == \"RealOsVersion\" and element.text is not None: os_ver = element.text # combine the two pieces of data into single string for the list cstring = os_type + \" \" + os_ver os_list.append(cstring) return os_list def main(): \"\"\"main to rule them all\"\"\" # get VM XML files files = get_vms() # parse XML files results = get_vm_os(files) # loop through and print out multi value EA for jamf inventory print(\"\u003cresult\u003e\") for item in results: print(item) print(\"\u003c/result\u003e\") if __name__ == \"__main__\": main() The above code was pretty much stolen from the Munki Project Munki is probably the best online resource for Python ObjectiveC code. It is filled with tons of gems from the maintainer as well as the community. I think that BYOD deployments are a very bad idea for many reasons. To sum up the biggest two reasons I will simply put: Good luck putting that BYOD device on term hold Good luck putting that BYOD device on legal hold That being stated, over a few beers a year or two ago I was chatting with a buddy of mine about this topic. Spotlight tagging came into the conversation and I found myself bored about a week later one evening so decided to just add something like this to a DEP Notify workflow. So, I posted it here on my GitHub That project is not tested (except for like once in a VM), nor maintained and was really just meant as a code example. Use at your own risk, and think about not doing BYOD if you can. The code uses similar tooling as the binaries above, but allows for the flexibility of Python. It also interacts with the macOS APIs, so some extended functionality is there. So, there are many uses for Spotlight one can use in IT/Ops workflows. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:6","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","Apple","python"],"content":"With the announcement at Catalina’s release that some third party run times will be removed, and the fact that Python 2 is end of life it is time to ship your own. Just like everything in tech, there are many ways to accomplish this. I have been using a tool for about over a year now called relocatable python. The reasons I chose to use relocatable python were pretty good ones in my opinion. They are: Easy to use Builds full self contained Python environment Easily able to wrap it up in a standard installer PKG Once you have it in an installer package, you can use whatever tools you want to distribute it. Every management tool and application deployment tool should be able to deploy an installer pkg. quick start guide Download the repo from the link above from the directory you wish to download it to git clone https://github.com/gregneagle/relocatable-python.git Look at the --help argument to see what we can do. Ensure you are in the repo folder. % ./make_relocatable_python_framework.py --help Usage: make_relocatable_python_framework.py [options] Options: -h, --help show this help message and exit --destination=DESTINATION Directory destination for the Python.framework --baseurl=BASEURL Override the base URL used to download the framework. --os-version=OS_VERSION Override the macOS version of the downloaded pkg. Current supported versions are \"10.6\" and \"10.9\". Not all Python version and macOS version combinations are valid. --python-version=PYTHON_VERSION Override the version of the Python framework to be downloaded. See available versions at https://www.python.org/downloads/mac-osx/ --pip-requirements=PIP_REQUIREMENTS Path to a pip freeze requirements.txt file that describes extra Python modules to be installed. If not provided, certain useful modules for macOS will be installed. Lets make the folders where we want to create our Python environment % sudo mkdir -p /usr/local/acme/ /usr/local/acme/bin Note: I made two directories there and will explain later why. You can put this anywhere you want. In this example I am using /usr/local but if you want it away from user space you can place it in say something like /opt Now lets build our first Relocatable Python Package sudo ./make_relocatable_python_framework.py --destination=/usr/local/acme --python-version=3.8.5 Note: you will see the tool output a bunch fo stuff in the terminal, let it finish Next we will create our symbolic link to make this easier when we want to call this environment in code # go to the bin folder we created cd /usr/local/acme/bin # create a symbolic link to our new framework sudo ln -s /usr/local/acme/Python.framework/Versions/3.8/bin/python3 python3 # now test it ./python3 Python 3.8.5 (v3.8.5:580fbb018f, Jul 20 2020, 12:11:27) [Clang 6.0 (clang-600.0.57)] on darwin Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. # exit when you are done \u003e\u003e\u003e exit() Finally, you just need to use your preferred packaging tool to add this folder path, including the symbolic link in an Apple Installer Package. ","date":"2020-09-20","objectID":"/posts/shipping-python/:0:0","tags":["macOS","python","python3"],"title":"Shipping Your Own Python to macOS","uri":"/posts/shipping-python/"},{"categories":["macOS","Apple","python"],"content":"Caveats, and things to consider As XKCD has already pointed out, managing a Python Environment can be a chore Installation Path In the above example there are definitely some things to know and consider. To keep the quick start guide less esoteric I just used /usr/local as the directory to deploy my Python 3 environment into. In practice, I actually do not do this. I deploy to /opt/myorg instead to my fleet. I have worked with lots of clever developers over the years, and they need to often install tools to do their job. Those tools often go into /usr/local, thus I stay out of that area. I let user’s have that space to themselves. Which Python? I also have Xcode on this Mac, and I also installed the Python3 Xcode tools as well. % which python3 /usr/bin/python3 When you install that, it does put python3 in the standard $PATH and if your code calls that path or if you just type python3 in the shell, without typing the full path, you will get that Python environment. Some planning will be needed as well as some decision-making by the IT Admin/Engineer that wants to deploy their own Python. There is no gold standard answer, so you will need to figure out what is the best answer for your team and Org. Requirements.txt file The author of Relocatable Python did something clever, but some may have missed it. They made an assumption that if you were to build a Python3 environment to deploy to your macOS fleet you might want the objc bridge, and tools like xattr So, those tools are just included in the default argument. You should note this just in case you plan on customizing your Python3 package to add more Python packages to it. You will need to add those back. One can view the documentation on how this works. To see what packages you have you can use pip to do so: % cd /usr/local/acme/Python.framework/Versions/3.8/bin % ./pip3 list This will output a giant list of packages you have installed. If you want to add more packages you should also include the ones the author of Relocatable Python gave you. When using a requirements.txt file it strictly follows that file. Anything not listed will not get installed. Refer to the --help output we used earlier to add the requirements.txt file sudo ./make_relocatable_python_framework.py --destination=/usr/local/acme --python-version=3.8.5 --pip-requirements=/path/to/requirements.txt Mac Admin Community Python There also those in the Mac Admin community who are already maintaining a public Python3 package which anyone can just go download and/or contribute to. You can find the repo here. This method does make design choices for you. If these design choices are okay with you, then this would be the easiest and fastest way to just ship your own Python3 environment to your fleet. Virtual Environments This is also an option, but it seems most Orgs want to ship their own isolated Ptyhon and if they need to create a venv they can do so from the Python they have shipped. Personally, I hae never used a venv outside of my on personal development. I chose to ship the environments, so I can control it. If you want to control venv to a fleet I think shipping your own is the best to start and then build your venv off of that. ","date":"2020-09-20","objectID":"/posts/shipping-python/:0:1","tags":["macOS","python","python3"],"title":"Shipping Your Own Python to macOS","uri":"/posts/shipping-python/"},{"categories":["macOS","Apple","python"],"content":"Tracking Versions, upgrades, and remediation I think in the past ~2 years I have found 2-4 systems total that had bad Python environments I was shipping. I am not exactly sure why they broke, it could have just been a failed install. So, I started tracking the status and version I was shipping in a simple Jamf EA. Here is a quick example I wrote in the shell: if results=$(/usr/local/acme/bin/python3 -V | awk '{ print $2 }') if\u003e then echo \"\u003cresult\u003e${results}\u003c/result\u003e\" then\u003e else echo \"\u003cresult\u003efalse\u003c/result\u003e\" else\u003e fi \u003cresult\u003e3.8.5\u003c/result\u003e This script will return a false value for any broken Python environment. So, I have an ongoing policy that will reinstall my Python3 environment scoped to this EA with a value of false so this is intentional. I highly recommend you never use blank values in anything you use. Always be explicit with your data, you will never know what a null or blank string value will affect, down or up stream. So, you can track it by the version and create policies to auto remediate any broken Python3 environment you encounter ","date":"2020-09-20","objectID":"/posts/shipping-python/:0:2","tags":["macOS","python","python3"],"title":"Shipping Your Own Python to macOS","uri":"/posts/shipping-python/"},{"categories":["macOS","Apple","python"],"content":"Conclusion and acknowledgements You have many options to choose from. This is not too difficult as I have been doing this for coming up on 2 years now. If I can do this, so can everyone else. Acknowledgements: Greg Neagle Mac Admins Slack Mac Admin Community ","date":"2020-09-20","objectID":"/posts/shipping-python/:0:3","tags":["macOS","python","python3"],"title":"Shipping Your Own Python to macOS","uri":"/posts/shipping-python/"},{"categories":["webhooks","jamf","snowflake"],"content":"Shipping Jamf Pro Webhooks to Snowflake Jamf Pro has a builtin feature where you have the ability to ship a webhook event when specific events happen in their application. This allows for a near real time feed of event based data which you can consume and leverage to gain insights about your fleet. Jamf provides some documentation online, found here. Webhooks are generated automatically, and near instantly when an event occurs. In the linked documentation above you will see all the possible webhooks and sample data of what each webhook ships. There are many ways to ship webhooks, and you can choose from many paths to take. You can roll your own webhook receiver, you can ship webhooks directly to cloud storage (S3, Azure Blob, GCP Cloud Storage), use one of many data ingestion tools, and so forth. Right now we are leveraging a tool called Fivetran. Fivetran has built in integration to many SaaS and cloud services. What tools work best for your Organization will be up to your Org to decide. Here is how the data flow looks: First we need to configure Fivetran Snowflake Connector After that create the database in Snowflake, for reference here is the documentation This blog post assumes you have this operating and working, as every Org might have different configurations or requirements in their apps, please refer to the official documentation to ensure this is working. With in Snowflake you can have many warehouses, databases, schemas, and a wide range of RBAC controls in place. So, you may need to adjust some knobs to get this to work. Here is what I have created in my dev account: If you have your database setup now we need to log into Fivetran and create our connector. Find the Webhook Conector with in Fivetran and create a new one. You will want to make sure you already have the database in place and Fivetran has the proper RBAC on that table for data ingestion. Here is an example: There will be a Webhook URL once created which you will need to configure in the Jamf Pro Server. Navigate to Settings \u003e Global Management \u003e Webhooks. Create a new webhook, select the event you want to ship (above example was ComputerCheckin), and make sure you select JSON as the data type. Input the URL you generated in Fivetran and any other options you would like to tweak. You can see that I have my schema set and the table set that matches my dev account in the screenshots. For reference: In Fivetran you can select how often data synchronizes with Snowflake. Since Webhooks are event based data, it is my opinion that the faster you can consume the event, the more valuable that data is. So, I have chosen to ingest every 5 minutes. If you want to pick a different time, you may do so. Your Org may have different needs or requirements, but I do strongly suggest you ingest the webhooks as fast as you can. For example, if you are building data around events, the event based data is more valuable if you can get it in near-realtime. Below are the settings I have configured. So that is it! Just repeat this process for each webhook event you wish to ship to Snowflake. Now you can let the data flow right in, and you are ready to query some of the data and get intelligence off of it. Now for some fun. If you want to find out how many times a specific Policy ID has ran on your fleet with in a specific time frame you can do this quite easily. Since Snowflake is highly scalable and allows one to store massive amounts of data you can keep all your historic data as you see fit. Here is an example query: ALTER SESSION SET TIMEZONE = 'UTC'; select count(*), EVENT:policyId as policy_id , EVENT:computer.jssID as jss_id , EVENT:successful as state from \"JAMF_EVENTSDB\".\"JAMF_EVENTS\".\"POLICIES\" where policy_id = '256' and TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(days, -1, current_timestamp()) group by state, policy_id, jss_id; NOTE: Jamf Pro Webhooks ship with UTC millisecond epoch time stamps COUNT(*) POLICY_ID JSS_ID STATE 2 256 2240 true In the ab","date":"2020-09-19","objectID":"/posts/shipping-jamf-webhooks/:0:0","tags":["data","webhooks","fivetran","jamf","snowflake"],"title":"Shipping Jamf Webhooks with Fivetran","uri":"/posts/shipping-jamf-webhooks/"},{"categories":["data sharing","snowflake"],"content":"Data Sharing is Data Caring ❤️ I have come to the conclusion that there are essentially two types of people when it comes to data. The people that have data, and the people that wish they had the data. Another thought is that I would rather have the data and not need it versus need the data and not have it. For almost the past two years I have had the privilege to work for a great data platform company. The amount of data we have access to increases our return on what we do in the IT/Ops space exponentially. It also helps drive collaboration between teams, allows for centralized data sharing, and enables everyone to make data driven decisions. In a previous life I did have some opportunity to work with various data tools, and use data to help me accomplish my goals. However, I have never had data like I do now. When I worked in vendor space a lot of my exposure to data tools was around what our customer’s wanted. Many of the customers I engaged with wanted to get all their data from my employer’s product. This typically resulted in large projects, and many labor hours to accomplish. There were also almost always caveats with most of the tools we were using that made us make hard decisions. At one previous job I did have more direct exposure and responsibility around a data tool. It took me four to six weeks to get our test instance setup. I had to configure TLS communication for the elastic cluster. Generate certificates, which was a trial by fire process. Then setup the filebeat \u003e logstash \u003e elastic pipeline. At the Logstash level you had to grok your data and create multiple indices, so you were shaping the data before ingesting it. This had a pretty high learning curve and took a lot of time and effort to just proof of concept. Do not get me wrong here, I do think Elastic is a great tool. When I first showed up at Snowflake, I honestly did not know too much about the product other than the basic PR stuff that I had read online, some blog posts, and some Internet posts on various sites. When I wanted to ship webhooks from our MDM solution, I got access to our Fivetran Instance and with in 30-45 minutes of looking at some documentation and tinkering in my dev environment I had webhooks shipping. I could not believe it took me under a hour to figure this out. I was prepared for it to take weeks from previous experiences. One can also just ship the raw data. No need to grok, no need to transform my data before ingesting it, and I have all of it. Enter Data Sharing ❄️ All of our business units ship their data to our data platform. So, every department has their own database, with their own schemas, and their own tables containing all the data they need access to. Since Snowflake’s data platform allows one to have as many databases, warehouses, tables, views, schemas and so forth as they see fit, it allows for easy data sharing. This means we can all share data to each other, and only the data we want to share. All of the data is on the same platform, so you aren’t spinning up a plethora of servers and then configuring them to access each other. The return you get on saved time and labor is already worth it. In the past, I dealt with gatekeepers of data. I was a data gatekeeper myself. IT and Security typically work with each other at most Organizations. Their goals often align with what the business wants. So, to get data you had to deal with each gatekeeper of each system. IT/Ops and Security typically own several systems if not more on each side. Often you would end up with the data gatekeeper emailing you a spreadsheet of the data you requested. If you were lucky you got API access to consume the data on your own. This is not a good experience, and it was definitely not efficient. With Snowflake, we can freely share data between IT/Ops and Security. When the raw data is updated from ingest, all the data shares among our teams is also updated. There is no more always dealing with a gatekeeper and getting a spreadsheet emailed ","date":"2020-09-19","objectID":"/posts/data-sharing/:0:0","tags":["data sharing","snowflake"],"title":"Data Sharing","uri":"/posts/data-sharing/"}]