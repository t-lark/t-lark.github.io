[{"categories":["osquery","snowflake","data","licensed software"],"content":"Turn an IT Cost Center into an IT Cost Savings Center! So many times in my career I have heard IT/Ops teams be referred to as overhead. I have also heard IT being described as “just keeping the lights on,” and all the adjacent things one can say similar to this. These type of statements has always made me want to prove that IT isn’t really just those type of things. I have always been trying to implement some sort of cost savings where we can definitively decide where to cut costs. You never want to cut costs around things that have a high positive impact. Since in many ways you do get what you pay for at times, and what you pay for may be very worth it. So, IT is a cost savings center, a service provider, an enabler, an enhancer and much more than some of the stereotypical things you might hear about how people perceive IT. How many times have you implemented a software stack that was mid to low bidder range over the more expensive stacks and regretted your purchase months and months later? How many times have you just guessed at how many licenses you need during a true up? Are you buying the right license bundle? Companies like Adobe and Microsoft love to shoehorn you into mass license bundles for discounts at higher volumes of purchasing. How often have you made mistakes in this area and over purchased or under purchased licenses? Previously, I blogged about how we track licensed software from ingesting the application usage data from Munki. That previous blog post goes into details on how we integrated our osquery config to consume the Munki SQLite database of application usage. This is how we build the baseline of our application usage metrics. ","date":"2025-04-03","objectID":"/posts/managing_licensed_software/:1:0","tags":["data","data apps","snowflake","licensed software"],"title":"Managing Licensed Software with Data","uri":"/posts/managing_licensed_software/"},{"categories":["osquery","snowflake","data","licensed software"],"content":"Defining the Problems What we are striving to achieve here are a few simple things. Enable cost savings around licensed software Enable procurement teams with better data to forecast true ups and renewals Enable our asset team to build data driven licensed software lifecycle workflows That is really it. We want to save money where we can, and where it makes sense the most backed by the data we collect. We also want to ensure that the user experience here is good. You need a copy of Adobe Photoshop? Just file a request ticket and we will automatically provision you a license no questions asked. You get the license and the access very quickly. However, if you stop using Photoshop over an arbitrary number of days, we will revoke that license and save money. If you need the license again, just file a ticket and get another license automatically. This balances a good UX with a cost savings initiative. We want to enable end users to have access to the best tools we can provide them to do their job with the least amount of friction to get them, while also enforcing cost savings initiatives wherever we can. ","date":"2025-04-03","objectID":"/posts/managing_licensed_software/:2:0","tags":["data","data apps","snowflake","licensed software"],"title":"Managing Licensed Software with Data","uri":"/posts/managing_licensed_software/"},{"categories":["osquery","snowflake","data","licensed software"],"content":"How the Donuts are Made Last month I went to India to visit our team and offices in Pune. While onsite I tasked my India side of the team to build an app that can help us make our application usage data more actionable. As a team building exercise I asked them to design and build an app that can help us solve the problem of managing licensed software lifecycle here at Snowflake. So, the team brainstormed and built this app as a team exercise over the week while I was in town. I thought this could be a great way for all of us to collaborate in person since we were all going to be in the office at the same time. What the team built basically comes down to these specific data points joined in Snowflake and then built into a Streamlit app to help make it actionable: Munki application usage data HR directory data (email, department, cost center, manager name, etc) Asset data (primary, secondary, asset function, etc) App group membership (what users are in groups scoped to apps) These are the only building blocks you need to get started. This should get you enough context to replicate what we have built here at Snowflake. More Info: Note that this is for macOS data only. We do have Windows application usage data telemetry already enabled and ingesting into Snowflake, but we have not had time to implement this into our App just yet. That will come in the next release of ours. We collect the user assist data on the Windows side which tracks execution time of all exe files on disk. We don’t really have this problem on Linux right now, but in the future if we need to heavily purchase licensed software for our Linux boxes, we will have to figure out how to track it there. ","date":"2025-04-03","objectID":"/posts/managing_licensed_software/:3:0","tags":["data","data apps","snowflake","licensed software"],"title":"Managing Licensed Software with Data","uri":"/posts/managing_licensed_software/"},{"categories":["osquery","snowflake","data","licensed software"],"content":"Streamlit Interface In the above screenshot we will see all the copies of Adobe Acrobat Pro that have not been used in greater than 90 days. If you didn’t read my previous blog post on Munki application usage data, the data tracks activations of an application. Meaning every time you tab to an app and make that app the more foreground running application in macOS that generates an activate event. This way we can assume every time you make an app the most foreground application you are actively using it. This should mitigate edge case in the data where a human has an app open running in the background but isn’t actually using it. This is me with Excel. Anytime I get emailed a spreadsheet is really the only time I ever open up Excel. Then Excel just sits there running until I quit it, or I get emailed another spreadsheet. This means I have long run times for Excel running in the background, but I honestly barely ever use it. Which is why my O365 license got retired as I pretty much only use Google Docs these days. ","date":"2025-04-03","objectID":"/posts/managing_licensed_software/:4:0","tags":["data","data apps","snowflake","licensed software"],"title":"Managing Licensed Software with Data","uri":"/posts/managing_licensed_software/"},{"categories":["osquery","snowflake","data","licensed software"],"content":"What is that Cortex Analyst? My team surprised me with this feature. They integrated Snowflake’s Cortex AI Analyst which can enable users to query data using natural language. This will enable consumers of this app that may not have deep technical SQL/Python skills to simply ask Snowflake a question in natural language and it will generate a response. Upon using this feature, you may immediately notice it generates the SQL code based off a prompt input by the user. This is also quite handy for those of us that do write code. I consider myself moderately good at SQL. I can write queries, CTEs, build data models, dashboards, data apps, and do generalist analyst type work. Even though my background is classically in IT, I have picked these skills up over the years of being volun-told I was the new DBA since I had “some SQL experience.” However, I can never generate a query in a matter of seconds, which Cortex can. The fact it generates the query code is great because if something is slightly off or wrong from my prompt I now have a fully functional query code I can tweak. Lets assume this is our prompt: Show me all the data where Docker has not been used in greater than 90 days and mask all PIl data including serial numbers and replace the characters with a snowflake emoji Cortex Analyst in return would output a query like this and execute it: WITH __license_usage_dt AS ( SELECT app_version, bundle_id, app_path, serial_number, email, division, title, department, substatus, last_event_time, query_exe_time, number_times FROM db.schema.license_usage_dt ) SELECT app_version, bundle_id, app_path, REGEXP_REPLACE(serial_number, '.', '❄️') AS serial_number, REGEXP_REPLACE(email, '.', '❄️') AS email, division, title, department, substatus, last_event_time, query_exe_time, number_times FROM __license_usage_dt WHERE LOWER(bundle_id) LIKE '%docker%' AND last_event_time \u003c DATEADD(DAY, -90, CURRENT_TIMESTAMP()) ORDER BY last_event_time DESC NULLS LAST -- Generated by Cortex Analyst ; So, it generated the above query in a few seconds. I could never write that query in a few seconds. So, from an engineering perspective Cortex Analyst can also very quickly generate syntactically correct SQL code for you to tweak if it doesn’t get your desired end state from a prompt. I also see this as a huge accessibility feature for folks who want to leverage all the sweet data we have but may not have SQL analyst or developer skills to tweak the queries on the fly. The results: So a couple of really cool things happened here. I was able to mask the PII data with a ❄️ by simply just asking Cortex to do so. This allows me to share this data on a blog post, or even say I need to share this data with a third party like a reseller or a direct vendor. I could scrape the PII data from the query results and now this is more sharable outside my org, if I needed or wanted to share that data with someone else. Now all the PII data is just a bunch of snowflakes! ","date":"2025-04-03","objectID":"/posts/managing_licensed_software/:5:0","tags":["data","data apps","snowflake","licensed software"],"title":"Managing Licensed Software with Data","uri":"/posts/managing_licensed_software/"},{"categories":["osquery","snowflake","data","licensed software"],"content":"Just the beginning This was our first attempt and integrating AI into one of our data apps. I can already see us iterating over this and improving it as well as adding more AI features to future products. We also plan on building more cost savings tools that will be data driven in the future. My team is already thinking about adding more than just licensed software as well to this app. We want to look at also tracking Windows Enterprise licenses (OS upgrades), jamf and other MDM licensing, agent licenses, and more. So, not just third party apps end users use, but all of our licensed tools we want to eventually track and use in this app. Thanks for reading! ","date":"2025-04-03","objectID":"/posts/managing_licensed_software/:6:0","tags":["data","data apps","snowflake","licensed software"],"title":"Managing Licensed Software with Data","uri":"/posts/managing_licensed_software/"},{"categories":["osquery","snowflake","data","performance"],"content":"Many organizations today still struggle with hardware refresh, right sizing hardware specs, and understanding the performance impact of tools and software across their fleet of end user devices. This has been a problem since the beginning of humans incorporating technology into their professional lives. In my 25+ year career I don’t think I have ever had a job where these problems were considered solved, and I don’t think we ever got close to solving them. A major factor we have all faced while trying to solve this problem is the lack of data compounded by the problem of being able to use and leverage the data in a meaningful way. Here at Snowflake I feel we have just met our first milestone at solving this problem, but to be clear we still have quite the journey ahead of us to fully solve these problems. We have just launched our first version of a Streamlit app that contextualizes the data we collect in a meaningful way to tackle these problems. ","date":"2025-01-14","objectID":"/posts/building_endpoint_performance_metrics/:0:0","tags":["data","data apps","snowflake","performance metrics"],"title":"Building Endpoint Performance Metrics","uri":"/posts/building_endpoint_performance_metrics/"},{"categories":["osquery","snowflake","data","performance"],"content":"Common Struggles \u0026 Problems How much RAM does a developer need? How much disk space does a video editor need? Will a MacBook Air have enough compute power for folks that do mid to high level workloads on their laptop? When is a good time to refresh end user hardware to employees? Every 2 years, 4 years, maybe 5 years? The answers to all of these questions will be relative to your Org and how your Org uses technology. There isn’t a magic answer that we can just ask for that would be guaranteed to fit all of your needs. We can, however, create frameworks and metrics with data and start iterate over these things to try to find those answers. I have had jobs that allowed for a 2-year hardware refresh, other jobs it was 4 years. I even had a job one time when it was 5 years before you could get a refresh. When looking at the cost of this, you also have many ways to address the cost, and the potential cost savings. If we take a $3,000.00 laptop, add the cost of required software, and any additional costs (extended warranty, peripherals, etc.) and then divide that by the number of months and get the cost per a month for that employee. So let’s assume the following costs and timelines: Laptop: $3,000.00 required Software: $150.00 Required Software = cost per a license of required software (agents, apps, etc.) Extended Warranty: $150.00 Hardware Refresh Time: 3 years This would be a total of $3,300.00 over a 36-month (3 year) period. Thus, it would cost approximately $91.67 a month to provide that laptop to that type of employee. If we adjust the timeline to 4 years for a hardware refresh that lowers the monthly cost of that employee’s laptop to $68.75. Which is a cost savings of $22.92 per a month. Now lets scale that out to an entire division of employees doing similar type work on their computer. Let’s assume a large department of 2,000 employees are evaluated at this scale. That $22.92 per a month savings turns into $45,840.00 per a month, or $550,080.00 a year. How do we ensure in IT and in Procurement we are making the right decisions that balance user experience, productivity, and trying to keep costs down? We will want to use data for this, otherwise everyone will just be guessing. A half a million dollar savings by extending hardware refresh one extra year for 2,000 employees may look fantastic on paper in regard to overhead costs, but does it impact the employees in a way it hinders their productivity? Will that impact the organization in a negative way and hinder the ability to produce end results the Org wants? What if adding that extra year also adds tons of support costs, and increases labor demand of the help desk and IT support? If you aren’t collecting these data points you simply will not know. There are many ways to calculate total costs, this was just an example that we toyed around with. There are always many other factors to figure out, and each Org will approach those differently. The above was merely an example and not necessarily one we use, or will use. There are more discussions to have and more data to analyze to find a final solution. ","date":"2025-01-14","objectID":"/posts/building_endpoint_performance_metrics/:1:0","tags":["data","data apps","snowflake","performance metrics"],"title":"Building Endpoint Performance Metrics","uri":"/posts/building_endpoint_performance_metrics/"},{"categories":["osquery","snowflake","data","performance"],"content":"Getting Started with Data We knew this would be a long journey to get to this point, but before we could start the journey we had to build a plan with a roadmap to get to our final destination. This all started a couple of years ago when we decided to onboard FleetDM as our main tool to deploy and manage osquery. We wanted a good cross-platform data collector tool we could easily ship data to Snowflake with. FleetDM met our needs and at the time we were only interested in the osquery management itself. So, we started with setting up FleetDM, creating queries and osquery configurations, and a kinesis data stream to Snowflake. This took some time to POC the product, pilot it, then scale out the deployment across all departments. We also did a lot of slow moving testing to ensure we weren’t going to hog-up the available compute with this tool on end user devices. During this time when we were building and scaling osquery our leaders wanted to build some sort of Laptop UX score with data, similar to other UX scores we have created here. So we started designing performance metrics queries and data models, we rolled them out, tested them, monitored them, then we finally created data models and shared those data models with our data science team. If you are unsure on where to start, here are some ideas you can research to see if this could be a good fit at your Org. First and foremost you will need data collection. I will state, that in my professional opinion, MDM is not a good tool for this. MDMs cannot collect inventory data at a high scale and frequency. Also, MDM products don’t have data streaming abilities for the most part (if some MDM’s offer this please let me know). Lastly, there is no real good cross-platform MDM. I wanted to avoid having to deal with multiple tools, with multiple different ways to collect data, which downstream would result in likely more labor to data model each different data pipeline. We also have to deal with Linux here, which also tosses another complexity in the grand scheme of things. This is what made osquery a pretty easy answer to solve these data collection problems. So, I would strongly suggest you plan for all platforms you have at your Org, all the data collection frequencies you think you will need, and finally you will need a destination to ship all this data into some sort of data platform. Snowflake is an excellent choice for this. ","date":"2025-01-14","objectID":"/posts/building_endpoint_performance_metrics/:2:0","tags":["data","data apps","snowflake","performance metrics"],"title":"Building Endpoint Performance Metrics","uri":"/posts/building_endpoint_performance_metrics/"},{"categories":["osquery","snowflake","data","performance"],"content":"Developing Data Collection Tools With metrics data you must find a balance of collection frequency and what data you collect. If you were to collect data on compute say once or twice a day, that isn’t very meaningful. Likewise, if you collect data every 5 minutes that is likely going to cause overhead in many areas, and could also impact user experience. So, you will want to balance this out in a way where you can collect data at a meaningful frequency, but also not get too aggressive to impact end user experience on their devices. There isn’t really a magic number I can give anyone, and it will be relative to your goals. However, we can share our experiences here and hope that it may help others figure out what works best for them and their Org. We collect data at various intervals. Some data collections happen every hour, some every few hours, some are just once a day or perhaps a few times per a day. When it comes to running processes and their resources you may want to take more frequent snapshots to get better telemetry on the datapoint you wish to track into a metric. For example, battery capacity data we collect every hour as an end user on a laptop can be on the go throughout the day. They will dock their laptop, plug it into the AC adapter to charge it, and of course they will run it off the battery while running to meetings or collaborating with others in person. Due to the constant fluctuation of states the battery can be in, we do want to collect this specific data point at a frequent enough level that it is meaningful. If we collect battery capacity data in a laptop once or twice a day that is not very meaningful, and it reduces our chances of capturing data throughout all the different states a battery can be in. In FleetDM we set up labels to mark devices as early development testers, pilots groups, and the rest of the population. We use this as a scoping mechanism to performance and impact test queries from the development cycle, the stage cycle and the production cycle. Queries go through this process to test impact of the query, and we leverage FleetDM’s performance impact metrics to determine if a query has anything over a minimal impact on end user devices. We also have a data ingest that monitors osquery’s deny list. This is when the local watchdog denies a query due to the host already in a busy state running other queries. These type of things should be considered, developed, and in place before you start to deploy queries to your fleet of end user devices to mitigate impact. This type of work does take time and effort. My team spent about an entire year developing queries, testing queries, optimizing SQL code to reduce compute costs, and then slowly rolled them out to our fleet of laptops over time to collect data. We also needed a decent chunk of historical data to trend over time. Historical data in this type of project is quite valuable. You can then build daily, weekly, monthly and so forth performance metrics. If an end user device shows poor performance for 1 week out of an entire quarter, you now have that data point to investigate. Perhaps it was a bug in the software you deploy, or an unoptimized configuration. Perhaps it was one my favorites, a race condition bug! Which are difficult to reproduce and annoying to diagnose and troubleshoot. In contrast, if an end user device performs poorly for the majority of the quarter that is a better signal that the device could have problems, or perhaps should be prioritized for refresh. So if you go down this road you should set some expectations for yourself, your teams, and your Org that this will take time, and you will need to collect data constantly to build meaningful metrics out of it. ","date":"2025-01-14","objectID":"/posts/building_endpoint_performance_metrics/:3:0","tags":["data","data apps","snowflake","performance metrics"],"title":"Building Endpoint Performance Metrics","uri":"/posts/building_endpoint_performance_metrics/"},{"categories":["osquery","snowflake","data","performance"],"content":"Data Collection \u0026 Data Modeling One of the struggles we had in the beginning was that there are a good number of blogs and online resources that use osquery to track computer performance, but they lack in substance, and they don’t really deep dive into the subject. Also, modern Apple hardware has changed a lot which makes this also very complex. Which I will touch on later. So, we did a lot of research and development with these queries. Even when reaching out to the community online I found not many folks were really doing a lot of this type of work. So, after looking at a lot of security and IT/Ops blogs around osquery we borrowed some ideas and added our own. This is another reason that this type of work takes time to develop. It seems that is not a very common use of osquery, or perhaps the folks that are doing these things just don’t share what they do in the public space. A query to track macOS compute: SELECT p.cmdline , p.cwd , p.name , p.path , p.pid , p.on_disk , p.disk_bytes_read , p.disk_bytes_written , p.resident_size , p.wired_size , p.start_time , p.total_size as ram_used_bytes , p.uid , p.user_time , ci.model , ci.number_of_cores , ci.logical_processors , ci.max_clock_speed , ci.number_of_efficiency_cores , ci.number_of_performance_cores , CAST(ROUND(((user_time + system_time) / (cpu_time.tsb - cpu_time.itsb)) * 100, 2) as text) AS percent_cpu_process , CAST(ROUND(((total_size * 1.0) / (1024 * 1024)), 2) as text) AS total_ram_used_mb , cpu_time.percent_cpu_sys , cpu_time.percent_cpu_user , cpu_time.percent_cpu_idle , cpu_time.steal as steal , cpu_time.core as core FROM processes as p,( SELECT ( SUM(user) + SUM(nice) + SUM(system) + SUM(idle) * 1.0) AS tsb , SUM(COALESCE(idle, 0)) + SUM(COALESCE(iowait, 0)) AS itsb , printf(ROUND((CAST(SUM(system) AS FLOAT)/(SUM(cpu_time.idle)+SUM(cpu_time.system)+SUM(cpu_time.user)))*100,2)) AS percent_cpu_sys , printf(ROUND((CAST(SUM(user) AS FLOAT)/(SUM(idle)+SUM(system)+SUM(user)))*100,2)) AS percent_cpu_user , printf(ROUND((CAST(SUM(idle) AS FLOAT)/(SUM(idle)+SUM(system)+SUM(user)))*100,2)) AS percent_cpu_idle , steal , core , user , idle , system FROM cpu_time ) AS cpu_time cross join cpu_info as ci ; I did have to chat with some folks in the official osquery Slack to get this query to work how we wanted it to. I also decided to join the cpu_info table to it, so I could get make and model of the CPU. It also allowed us to get how many CPU cores an end user device has. The raw output of the query looks like this (truncated): { \"action\": \"snapshot\", \"calendarTime\": \"Sat Jun 22 10:47:05 2024 UTC\", \"counter\": 0, \"decorations\": { \"host_uuid\": \"UUID of host\", \"hostname\": \"host name\", \"serial_number\": \"serial number\" }, \"epoch\": 0, \"hostIdentifier\": \"UUID\", \"name\": \"pack/Global/ingest-macos-compute-metrics\", \"numerics\": false, \"snapshot\": [ { \"cmdline\": \"\", \"core\": \"0\", \"cwd\": \"/\", \"disk_bytes_read\": \"4110402048\", \"disk_bytes_written\": \"147008774144\", \"logical_processors\": \"12\", \"max_clock_speed\": \"3504\", \"model\": \"Apple M2 Pro\", \"name\": \"kernel_task\", \"number_of_cores\": \"12\", \"number_of_efficiency_cores\": \"4\", \"number_of_performance_cores\": \"8\", \"on_disk\": \"-1\", \"path\": \"\", \"percent_cpu_idle\": \"93.43\", \"percent_cpu_process\": \"63.47\", \"percent_cpu_sys\": \"1.82\", \"percent_cpu_user\": \"4.75\", \"pid\": \"0\", \"ram_used_bytes\": \"29507584\", \"resident_size\": \"0\", \"start_time\": \"1717571471\", \"steal\": \"\", \"total_ram_used_mb\": \"28.14\", \"uid\": \"0\", \"user_time\": \"0\", \"wired_size\": \"0\" }, The above snip of JSON output from osquery is just one process of about 12,000 lines of output from the query on a single host computer. Luckily in Snowflake we can model this into a view which turns the JSON output into relational data. The JSON output does have a caveat with it. If you look at the FleetDM docs on the processes table, you will see it supplies the data types for each data value. This is great, because we want to keep the data the same in the local SQLite database that osquery uses in Snowflake. T","date":"2025-01-14","objectID":"/posts/building_endpoint_performance_metrics/:4:0","tags":["data","data apps","snowflake","performance metrics"],"title":"Building Endpoint Performance Metrics","uri":"/posts/building_endpoint_performance_metrics/"},{"categories":["osquery","snowflake","data","performance"],"content":"Building Metrics My team has partnered with many other teams internally to build this Streamlit App. We also have asked teams we work with closely, like the IT Support Org, to also look at the app and data to ensure it makes sense and could be actionable. The CPE team here worked closely with our assets team, data science team, along with leadership to ensure we were building something that could be effectively and leverage our data. When building a metric there are many ways to approach it. Perhaps, there are actually too many ways, and this can be confusing and likely take time to figure out. You could copy the idea of a “credit score”, or simply pick an arbitrary range of numbers to score the metric on. For example, you could score a metric between 1 and 10, with 10 being the best possible outcome and 1 being the worst outcome. How you pick your number range to score your metric isn’t necessarily important, but if you are going to build series of metrics it would make sense to be consistent across the board. Our teams here have already build service desk metrics, and have opted ot use a 1 through 5 scoring range. To keep the metric consistent we decided to also use this range. Once we established our metric system we started looking at scores. Our asset team would reach out to people that had specific score ranges. We also decided that a 5.0 was really not obtainable as it would just be a completely idle computer. We expect every computer to have some level of load on it, and for end users to consume some level of compute no matter what the tasks being done were. Our asset team reached out to folks in various ranges and were receiving feedback. We took that feedback and correlated it to UX scores. We discovered that a vast majority of devices that had under a 3.0 score the end user would always report performance issues. The data did back this up. When we reviewed the data, we could observe higher consumption of CPU and RAM and less free resources on these devices. We also had historical data, so we could also observe week over week, month over month and even quarter over quarter the performance metrics of the device. This helped us contextualize the scores. I would like to take a quick moment here and point out a few things. While our early established baseline for a healthy device would be anything over 3.0 or 3.25 we are still working through these correlations of scoring to actual performance. So, this could change, and we are open to it changing over time. This is the approach one should take when building something like this. You will need to collect data, run analysis, iterate over the data and improve it over time. We could discover later on that a 2.75 score is really the bare minimum a laptop needs to be considered performant, or it could go up to day 3.5. We also plan on iterating over the app, adding in more data to contextualize the overall scoring metric system, and improving things as we go. For example, we might start to add ticket data to each asset where we track how many support tickets a specific device generates. This could help us discover a device that has intermittent hardware failure, or is just a good old-fashioned “lemon.” The main takeaway should be that when you start to build these type of systems there is always likely going to be iteration and adjustment over time. ","date":"2025-01-14","objectID":"/posts/building_endpoint_performance_metrics/:5:0","tags":["data","data apps","snowflake","performance metrics"],"title":"Building Endpoint Performance Metrics","uri":"/posts/building_endpoint_performance_metrics/"},{"categories":["osquery","snowflake","data","performance"],"content":"Data Collection Points We have decided to collect some specific data points for the fleet of both Windows and Mac laptops we manage here. This was our starting point to gather enough data to make a meaningful metric as our output. We will likely expand on these over time, and I would also expect us to improve on existing data collection and telemetry. Our starting points: CPU usage RAM usage Disk I/O Disk Utilization Laptop battery health Number of running processes Department and Cost Center comparisons Hardware specifications Each platform is different, so you likely won’t be able to develop a query in osquery that can track every statistic across Linux, macOS and Windows. You will have to split some of the data collectors up by platform they are collecting against. Likewise, when collecting CPU usage data, it is also very nice to include the make and model of the CPU along with how many CPU cores the device has. This can help provide more context to your data. We also included Department and Cost Center comparisons in our app. This allows anyone who is analyzing the data to quickly compare what employees have what metrics within the same group. Since the same group of humans are likely doing some overlapping work, and possibly almost the same work across devices. This will help us profile say a sales rep versus a software engineer’s needs when it comes to hardware specification. If 90% of a Cost Center has acceptable scores with the same spec laptop and 10% have unacceptable scores we can use that as a starting point to investigate why. Is that 10% doing slightly different and more demanding work? Is that 10% of devices experiencing some sort of software or hardware issue? Since we will not be able to really track every aspect of how a human uses a computer, we can group the data together to help us contextualize it in a way that makes it more actionable. ","date":"2025-01-14","objectID":"/posts/building_endpoint_performance_metrics/:6:0","tags":["data","data apps","snowflake","performance metrics"],"title":"Building Endpoint Performance Metrics","uri":"/posts/building_endpoint_performance_metrics/"},{"categories":["osquery","snowflake","data","performance"],"content":"Apple Silicon Hardware Caveats With the introduction of Apple Silicon, we have observed Apple bring in an entire new level of performance to our beloved Apple laptops. While the advancements are definitely something to marvel at, they do introduce new challenges. With the advances in Unified Memory Architecture a lot of the classic memory performance metrics aren’t as applicable. Compound that with the fact that the kernel can now compress and swap memory on the fly between processes, this makes tracking memory usage somewhat challenging. Apple has moved over to a stat they have developed called Memory Pressure, and it is now present in Activity Monitor. There isn’t a great way to track this statistic from a binary, nor from osquery either. There is a binary to simulate memory pressure, but that is just to simulate it, not collect the data on it. So, until Apple provides us a more meaningful way to collect memory pressure from an IT tool, we must use what we can collect today. More Info: If anyone has methods to collect memory pressure accurately and at scale, we would be very happy to hear how you have accomplished this. I have also filed a ticket with Apple Care asking for better features around this data point. Same goes with Thermal Pressure, if you are collecting that in a way to store historical data at scale with this, we would also be very happy to hear how you have accomplished this. Thermal sensors are in a similar boat. In the older Intel based hardware you could query the SMC controller to get actual temperatures of an Apple device. In Apple Silicon this has moved to Thermal Pressure, and there is a way to collect this from a binary, but osquery doesn’t have a supported table for this yet. We cannot really use MDM to collect this data since we will want a faster frequency of data collection and historical data to trend it into a meaningful metric. So, Thermal Pressure is a “future us,” problem to solve at the moment. We will circle back to this one when we are able to find time to do so. ","date":"2025-01-14","objectID":"/posts/building_endpoint_performance_metrics/:7:0","tags":["data","data apps","snowflake","performance metrics"],"title":"Building Endpoint Performance Metrics","uri":"/posts/building_endpoint_performance_metrics/"},{"categories":["osquery","snowflake","data","performance"],"content":"Looking at the Data If you have read this massive wall of text blog post, I applaud you! Now we can look at some visuals and go over them. The above visual shows that a majority of our systems are scoring in the 4.0 to 5.0 range, and the secondary majority are in the 3.0 to 4.0 range. This aligns with our initial assessment of anything under a 3.0 should be investigated for hardware refresh, as well as any IT troubleshooting to help optimize user experience and performance of an employee laptop. This allows us to quickly visualize an overview of Laptop UX Scores and since it was built in Streamlit we can interact with the visuals to get more information. In the above screenshot we can observe the peaks and valleys of performance of our fleet and get a quick overview of the groups of scoring ranges. This is to be expected as computers get used differently throughout the weeks and months by the humans that use them. We could also guess that near end of a quarter compute usage might go up as everyone is trying to close their end of quarter projects up, so they are ready to tackle next quarters. If that guess is true, we should be able to observe this in the data. The key is to let the data guide you, and while you can make assumptions ensure that you are also in a position to accept if your predictions are wrong. The data will tell you the truth! We can also look at individual devices and get a feel for their current state of performance: This is my laptop, and we can see it displays some metadata about the device which allows us to quickly view it to get an idea of overall hardware spec. We can also use this data to do device to device comparisons at a cost center or at a department level to compare a laptop to its peer laptops. The above image gives us the daily snapshot of data along with the last 30 days mean. This allows a user of our Streamlit app to quickly view the latest data of any given laptop in our fleet. If we scroll down the page, next we can view the CPU intensive apps. Admittedly this is where my device shows that I have transitioned from an IC to a Manager, as a lot of my time is spent in Zoom meetings. However, this is very useful when looking at an end user’s computer. While we typically see processes like Chrome often the leader of compute consumption, we can also observe individual apps and processes that may also chomp at a computer’s CPU. Think things like security agents, or IT agents we deploy to our fleet. We can now observe when they are in weird states and taking up more CPU than they normally do. Then if we keep scrolling down the app looking at a single device we can observe the processes that consume the most memory. This is where I can “prove” that I still do some IC engineering work as my most intensive memory process is actually my IDE. As I still write code from time to time, test code, and other software development and coding workflows. So, my IDE is eating more RAM that Chrome for once. Typically, when I look up my devices Chrome (or other Chromium browsers) oftentimes take up the most RAM and CPU of my devices. This is also common across our fleet as well. ","date":"2025-01-14","objectID":"/posts/building_endpoint_performance_metrics/:8:0","tags":["data","data apps","snowflake","performance metrics"],"title":"Building Endpoint Performance Metrics","uri":"/posts/building_endpoint_performance_metrics/"},{"categories":["osquery","snowflake","data","performance"],"content":"Conclusion, Benefits \u0026 Roadmap I cannot think of a single job I have ever had where we had this level of data telemetry and so many options to make that data actionable. I also can recall many times at past jobs working with leadership, procurement teams, people who volunteered to test hardware and provide feedback, and so forth all got together to plan for what hardware spec to buy and what refresh lifecycle we would use. To be honest, it was a lot of guess work, and with minimal amounts of data being used back then. The great thing about Snowflake, FleetDM, osquery, and apps like Streamlit is that they are available to the public. This means any org that wants to build this type of tech stack can. I do have friends at other big prestigious tech companies that have data pipelines and have built custom in-house solutions that likely accomplish similar things, but those solutions they have built will never be publicly available. The benefits are there as well. You can use data like this to project cost savings initiatives, do data driven hardware refresh, use the data as way to drill into devices that are preforming well versus those that aren’t. So, IT support can look at this as a way to be proactive on devices that are declining in UX score. For the future we will want to collect data, assess it, iterate over it and then improve it over time. This will allow us to expand the application’s features and enhance our data collections over time as well. This is definitely not a once and done project, but rather a once and sustain forever. When approaching these types of projects it is highly beneficial for you to really think long term what your roadmap is going to be. I always suggest to start with a minimal viable product and get that into the hands of your stakeholders. Then get feedback from them to iterate over. These type of projects and data applications are highly likely going to have multiple use cases to multiple teams, so it is not a crazy idea to think you should plan long term for stuff like this. I know this was likely my longest blog post ever, and when I tried trim it down to fewer words it just felt it was missing context. If you actually read this entire thing I thank you and am grateful for your time! ","date":"2025-01-14","objectID":"/posts/building_endpoint_performance_metrics/:9:0","tags":["data","data apps","snowflake","performance metrics"],"title":"Building Endpoint Performance Metrics","uri":"/posts/building_endpoint_performance_metrics/"},{"categories":["fleetdm","osquery","snowflake","data"],"content":"Getting Application Usage Data at Scale With Munki Munki is a series of tools and a popular application state management tool many Mac Admins across the globe use. Some out-of-box features of Munki solve problems many commercial MDMs still cannot solve to this day. It allows a Mac Admin to write some declarative data and have Munki takes care of the rest for you. We use it to manage and patch all of our third party apps in conjunction with AutoPKG. Many projects or ideas start with a simple statement, or declaring what you want to accomplish. I feel this specific topic is one that requires this. I feel this is required, because I know many organizations will likely abuse application usage data for things it is not meant for. Abstract: We want to enable humans to have the best technology available to them whenever they need it. However, we also want to be financially responsible and invoke cost savings whenever we can. Application usage data can help bridge this gap. It is my personal belief that the best UX with the best cost savings is to automatically provision licensed software upon request with little questions asked. Then revoke the license if the app is not used within an arbitrary amount of time. This allows humans you work with to get the software they need automatically and the organization to save money if that licensed software is no longer in use by that human. One of the neat features of Munki, is that it can track application usage and Munki can take action from that data. You can read about it here in the project’s wiki page. There is a local SQLite database located in /Library/Managed Installs/application_usage.sqlite and it contains the data we are looking to ingest at scale into Snowflake! The schema for this db is: CREATE TABLE application_usage ( event TEXT, bundle_id TEXT, app_version TEXT, app_path TEXT, last_time INTEGER DEFAULT 0, number_times INTEGER DEFAULT 0, PRIMARY KEY (event, bundle_id) ) So, you can just shell into that db on any Mac that has Munki installed and run some basic SQLite commands to look at what it collects. However, no one is going to shell into each individual end user computer and collect this data. It is worth mentioning that Munki Reports is another tool Mac Admins can use if they choose to, that can help centralize data and management of Munki. When you work with data at scale centralizing your data though is extremely valuable for many reasons. Here are some top reasons why an organization would want to do this: Data Governance: implement RBAC controls on who can access what data, and protect crown jewels data Analytics and Intelligence: Build dashboards and data workflows to help automate and gain insights Data Sharing: Share your data across teams Reduce noise: Ability to join your data to multiple data sources and apply laser focus context to your data I am sure folks have written books about this subject, so I will leave it with those 4 points. ","date":"2024-03-26","objectID":"/posts/app-usage-data-munki-fleet-snowflake/:0:1","tags":["application usage","fleetdm","osquery","data","snowflake","munki"],"title":"Leveraging Application Usage Data From Munki","uri":"/posts/app-usage-data-munki-fleet-snowflake/"},{"categories":["fleetdm","osquery","snowflake","data"],"content":"Use FleetDM to Configure ATC in osquery Osquery has a really neat feature called Automatic Table Creation (or ATC for short), and a good introductory blog post about it can be read here. It can essentially query something like a SQLite database and return the results into an osquery table. This is the perfect feature for a setup like ours. The first SQLite database I was targeting was of course, the Munki application usage database. FleetDM does this a bit different from the Kolide blog post, but conceptually it is the same in the end. Let’s start with our requirements: FleetDM Stack Firehose Kinesis Stream to Snowflake A Munki deployment More Info: With anything you roll out, you should look at a phased approach. We have setup Teams in FleetDM. We have a team set to just my team, a team set to our canary groups, and then there is the concept of “all teams” which we just consider production. We make changes to the CPE team first, then if nothing breaks we move it to the canary groups and monitor it for some arbitrary range of time. Finally, we move it to production, and I highly recommend every IT shop have this sort of setup. FleetDM has the ability to modify the osquery config files per a team, this allowed me to set up ATC in a controlled and phased fashion. There are also some caveats, as I found out the hard way that if your YAML file in FleetDM is not quite in proper order osquery will skip over your decorators. See this issue for more details. Decorators are important, especially if you ship osquery data from FleetDM downstream to a product like Snowflake. They are included in every query result, thus it is a smart idea to put unique strings or IDs as a decorator, so now you have a primary key in your data you can use to join to other data sets. So, again please test everything out against a small subset of your systems and use a phased approach before you go all in on any osquery configuration change. Here is a sample of a YAML config you can use to accomplish ATC: config: options: pack_delimiter: / logger_tls_period: 10 distributed_plugin: tls disable_distributed: false logger_tls_endpoint: /api/osquery/log distributed_interval: 10 distributed_tls_max_attempts: 3 decorators: load: - SELECT uuid AS host_uuid FROM system_info; - SELECT hostname AS hostname FROM system_info; - SELECT hardware_serial from system_info as serial_number; auto_table_construction: munki_app_usage: path: /Library/Managed Installs/application_usage.sqlite query: \u003e- select event, bundle_id, app_version, app_path, last_time, number_times from application_usage; columns: - event - bundle_id - app_version - app_path - last_time - number_times platform: darwin Be Specific on Selects When working with data, you might want to do things like SELECT * FROM FOO_TABLE; and this might get the exact results you want. This may work for a very long time as well. However, it is more of a best practice to be very specific in the tables you select to avoid breakage downstream. If Munki were to add a new column to their database for any reason that SELECT * would also include that new table. Now downstream any data models or preprocessing will have a new column to deal with that isn’t defined. This could break things, and it is likely when it does break this is not the first thing you will think to check. So, just start off by sanitizing your inputs, and only selecting the exact data you intend to get downstream. In semi-structured data like JSON this is much less of a risk as you will likely just be missing a new key. When you use ATC to create a table FleetDM won’t recognize it as a valid table, and you might see this warning message when trying to run a live query: If you just ignore this message and YOLO it anyway and hit that query button, you will see you actually do get results: You might notice that there are several event types the Munki collects about an application. They are: Activate - This monitors when the application is the foreground app Quit - Thi","date":"2024-03-26","objectID":"/posts/app-usage-data-munki-fleet-snowflake/:0:2","tags":["application usage","fleetdm","osquery","data","snowflake","munki"],"title":"Leveraging Application Usage Data From Munki","uri":"/posts/app-usage-data-munki-fleet-snowflake/"},{"categories":["fleetdm","osquery","snowflake","data"],"content":"Modeling the Data in Snowflake For this and most of our other osquery data ingests from FleetDM we typically do a 1:1 data model. Meaning that we write a single query, that typically does a single purpose in FleetDM and ship the results as a snapshot in time to Snowflake. We worry about transforming the data downstream within Snowflake itself. This allows us to quickly get data pipelines going and then transform downstream when it is already in the product. Some of our JSON files can be over 20,000 lines of query results, and this sample I am going to share is about 700 lines of JSON. An example of query results: { \"action\": \"snapshot\", \"calendarTime\": \"Tue Mar 26 19:13:20 2024 UTC\", \"counter\": 0, \"decorations\": { \"hardware_serial\": \"device_serial\", \"host_uuid\": \"device_uuid\", \"hostname\": \"host_name\" }, \"epoch\": 0, \"hostIdentifier\": \"host-identifier\", \"name\": \"pack/Global/ingest-munki-app-usage\", \"numerics\": false, \"snapshot\": [ { \"app_path\": \"/Applications/Slack.app\", \"app_version\": \"4.36.140\", \"bundle_id\": \"com.tinyspeck.slackmacgap\", \"event\": \"activate\", \"last_time\": \"1710526854\", \"number_times\": \"144\", \"path\": \"/Library/Managed Installs/application_usage.sqlite\" }, { \"app_path\": \"/Applications/Slack.app\", \"app_version\": \"4.36.140\", \"bundle_id\": \"com.tinyspeck.slackmacgap\", \"event\": \"launch\", \"last_time\": \"1710169315\", \"number_times\": \"1\", \"path\": \"/Library/Managed Installs/application_usage.sqlite\" }, I’ve truncated the results above, but you might notice I just grabbed 2 events for the Slack.app. This shows the activate event and the launch event. Now just imagine an entry like this for every application used on an end user’s device. In a previous blog post I covered some data modeling in Snowflake with FleetDM data at a high level. We will still use those same high level concepts here. We can create a view by running the following SQL: create or replace view DB.SCHEMA.OSQR_MUNKI_APP_USAGE_V( SERIAL_NUMBER, HOST_NAME, EVENT_TIMESTAMP, HOST_UUID, APP_PATH, APP_VERSION, BUNDLE_ID, EVENT_TYPE, LAST_TIME, NUMBER_TIMES, SOURCE_PATH ) copy grants as ( select serial_number , host_name , event_timestamp , host_uuid , f.value['app_path']::string as app_path , f.value['app_version']::string as app_version , f.value['bundle_id']::string as bundle_id , f.value['event']::string as event_type , f.value['last_time']::number as last_time , f.value['number_times']::number as number_times , f.value['path']::string as source_path from DB.SCHEMA.FLEET_OSQUERY_RESULTS , lateral flatten(input =\u003e snapshot) as f where name = 'pack/Global/ingest-munki-app-usage' ); Our data is structured in a way that our decorators that we put in the FleetDM configs for osquery are always split out into their own columns from the raw JSON data, and some other commonly used keys we use. Like the query name as this is something we will filter for quite often. I am also keeping the schema intact from the original source, the SQLite database that Munki generates. This is really for data fidelity reasons, and if anyone ever has to reverse all the work I have done it just makes the most sense to copy the schema data types from the source in the model itself. So, I keep that 1:1 relationship not only with the query results, but the data types defined by the schema as well. Another big bonus of mimicking the schema from the source is that I can produce the same results from the base data model as I could locally use the SQLite shell on a macOS device running Munki. ","date":"2024-03-26","objectID":"/posts/app-usage-data-munki-fleet-snowflake/:0:3","tags":["application usage","fleetdm","osquery","data","snowflake","munki"],"title":"Leveraging Application Usage Data From Munki","uri":"/posts/app-usage-data-munki-fleet-snowflake/"},{"categories":["fleetdm","osquery","snowflake","data"],"content":"Lets Test Drive the Data Model! Now we just need to test our data model in Snowflake, and validate it is good to data share and use in dashboards and data applications. First lets just run a basic query against our data model and see what we get. My favorite app in the world, Excel! Now with a bit more filtering and more specific results for Photoshop! ","date":"2024-03-26","objectID":"/posts/app-usage-data-munki-fleet-snowflake/:0:4","tags":["application usage","fleetdm","osquery","data","snowflake","munki"],"title":"Leveraging Application Usage Data From Munki","uri":"/posts/app-usage-data-munki-fleet-snowflake/"},{"categories":["fleetdm","osquery","snowflake","data"],"content":"Conclusion Application usage data is a fanstatic way to track when expensive apps are getting used (or not getting used), track active license usage, help procurement with true up renewals, and just have a good general cost savings model. IT is sometimes viewed solely as an operational cost center, but now you can turn IT into a service providing cost savings by leverage good IT and automation tools and shipping your data to a platform like Snowflake. I bet we will use this model for true up renewals moving forward. This also is a good way for IT to capture what applications they likely need to offer in their self-service flows. If you have a substantial number of end users constantly using an app, perhaps this is an indicator it should be packaged up, deployed and patched/updated by IT. There are many good use cases for this and those use cases can be tied to cost savings for the org and quality of life upgrades for your end users. Just remember application usage data is not meant to track productivity, or anything adjacent or even near that. You should not be shocked the majority of your users will spend the majority of their time in the web browser. I highly discourage any use of application usage tracking for anything other that quality of life improvements and cost savings initiatives (or things closely adjacent to these). ","date":"2024-03-26","objectID":"/posts/app-usage-data-munki-fleet-snowflake/:0:5","tags":["application usage","fleetdm","osquery","data","snowflake","munki"],"title":"Leveraging Application Usage Data From Munki","uri":"/posts/app-usage-data-munki-fleet-snowflake/"},{"categories":["fleetdm","osquery","mdm"],"content":"Programmatically Apply FleetDM Labels from Identity Data For a long time now it has been considered a “bad practice,” to join a macOS computer to any sort of directory service. Long have past the days of Binding to Active Directory, and the ancient lore of the golden (or magic) triangle is nearly lost in time, like tears in the rain. The one thing that we could consider missing from these days was the ability to locally query user and identity data through native tools like dscl in macOS. I have seen very clever replacement solutions over the years to get this data down to a macOS end user devices across various enterprises, but one thing has always bothered me about every method I have seen or sometimes used. The common theme was that they were all not very security focused solutions. At a previous job, LDAP lookups were just straight up open to anyone on network, like it did not have any authentication at all. When I was in vendor space I witnessed customers having various curl solutions to grab that data from some system they could cache locally. Typically, these also were not very secure either. The main problem being that if the service required authentication, the admin building the workflow still had to pass credentials in a script, and that script will always end up on disk in clear text at some point in time. This solution was of course better than the ones where no authentication was ever even required. So, a while ago I started playing around with custom payload variables in Jamf Pro for several years now, and immediately thought they could fill a gap we had with getting user and identity data and not wanting to do it in a non-security focused way. So, I used this method to help me apply labels to devices in FleetDM. ","date":"2024-03-21","objectID":"/posts/using-custom-mdm-payloads-for-fleet-labels/:0:0","tags":["labels","fleetdm","osquery","data","identity"],"title":"Using Custom MDM Payloads for FleetDM Labels","uri":"/posts/using-custom-mdm-payloads-for-fleet-labels/"},{"categories":["fleetdm","osquery","mdm"],"content":"The Setup We are an Okta shop, and we have enabled a feature in Okta called Okta UD, which the TL;DR take here is that it can act like an LDAP connector for Jamf Pro. So, I worked with my CloudOps folks and Identity folks to get that setup pretty much back when I started here years ago. This is how we map User and Location data to device records in our Jamf Pro MDM. Once setup, you can create a Directory Service EA that will map the LDAP attribute from Okta UD to your device record in Jamf Pro. Then you can craft a simple custom Configuration Profile payload like so: \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"\u003e \u003cplist version=\"1.0\"\u003e \u003cdict\u003e \u003ckey\u003eemail\u003c/key\u003e \u003cstring\u003e$EMAIL\u003c/string\u003e \u003ckey\u003elocal_user\u003c/key\u003e \u003cstring\u003e$USERNAME\u003c/string\u003e \u003ckey\u003efull_name\u003c/key\u003e \u003cstring\u003e$FULLNAME\u003c/string\u003e \u003ckey\u003eposition\u003c/key\u003e \u003cstring\u003e$POSITION\u003c/string\u003e \u003ckey\u003edepartment\u003c/key\u003e \u003cstring\u003e$DEPARTMENTNAME\u003c/string\u003e \u003ckey\u003ecost_center\u003c/key\u003e \u003cstring\u003e$EXTENSIONATTRIBUTE_108\u003c/string\u003e \u003ckey\u003ecountry\u003c/key\u003e \u003cstring\u003e$EXTENSIONATTRIBUTE_60\u003c/string\u003e \u003ckey\u003ecity\u003c/key\u003e \u003cstring\u003e$EXTENSIONATTRIBUTE_63\u003c/string\u003e \u003ckey\u003eldap_shortname\u003c/key\u003e \u003cstring\u003e$EXTENSIONATTRIBUTE_86\u003c/string\u003e \u003c/dict\u003e \u003c/plist\u003e When looking at that XML payload you will notice $EXTENSIONATTRIBUTE_\u003cID\u003e where the ID is the actual ID the EA has in Jamf Pro. You can get the ID by clicking on the object in the admin console and looking at the URL bar in your browser, you should see the integer ID that object is mapped to. So, I just filled them in according to how we have the data organized. Then when you payload the profile to disk, you can query it with simple binary tools like defaults. % defaults read /Library/Managed\\ Preferences/com.mycustom.info.plist { city = \"my office city location\"; \"cost_center\" = \"my cost center\"; country = US; department = \"Information Technology\"; email = \"tlark@acme.com\"; \"full_name\" = \"tlark\"; \"ldap_shortname\" = \"my ldap shortname\"; \"local_user\" = \"local user name\"; position = \"Job Title\"; } Pretty cool, huh? We have now successfully cached some very basic identity data without having to query LDAP, without having to curl script with API credentials, or have something with no authentication on it so others can consume identity data. Also, the MDM protocol by default encrypts this traffic until it is on disk. So, zero need for AD/LDAP joining, definitely do not need a golden triangle setup, and we don’t need to really do any bad security practices here to obtain this data. More Info: You will need to set up some sort of directory service into your MDM, and ensure your MDM supports custom variable payloads that can leverage those LDAP (or directory) mappings. Warning: Also be warned that in Jamf Pro a new profile will not be pushed on data change server side. So, if a computer swaps hands to a new user you will have to have some sort of workflow account for that. In short, something like, but not limited to: touch a file, scope to an EA and use as an exclusion via Jamf Smart Groups, then once that data clears, remove the file so scope recalculates and pushes a fresh profile. If you do not like this, please file a feature request with jamf. ","date":"2024-03-21","objectID":"/posts/using-custom-mdm-payloads-for-fleet-labels/:0:1","tags":["labels","fleetdm","osquery","data","identity"],"title":"Using Custom MDM Payloads for FleetDM Labels","uri":"/posts/using-custom-mdm-payloads-for-fleet-labels/"},{"categories":["fleetdm","osquery","mdm"],"content":"Setting up the Label Assuming all is well, and the MDM is pushing down a profile to your endpoints with valid data we may proceed. If you are familiar with fleetctl you can use it to get the current labels to get an idea of how the product applies the default labels that are built-in, like so: % fleetctl get labels +--------------------------------+----------+--------------------------------+---------------------------------------+ | NAME | PLATFORM | DESCRIPTION | QUERY | +--------------------------------+----------+--------------------------------+---------------------------------------+ | All Hosts | | All hosts which have enrolled | select 1; | | | | in Fleet | | +--------------------------------+----------+--------------------------------+---------------------------------------+ | macOS | | All macOS hosts | select 1 from os_version where | | | | | platform = 'darwin'; | +--------------------------------+----------+--------------------------------+---------------------------------------+ | Ubuntu Linux | | All Ubuntu hosts | select 1 from os_version where | | | | | platform = 'ubuntu'; | +--------------------------------+----------+--------------------------------+---------------------------------------+ | CentOS Linux | | All CentOS hosts | select 1 from os_version where | | | | | platform = 'centos' or name | | | | | like '%centos%' | +--------------------------------+----------+--------------------------------+---------------------------------------+ | MS Windows | | All Windows hosts | select 1 from os_version where | | | | | platform = 'windows'; | +--------------------------------+----------+--------------------------------+---------------------------------------+ | Red Hat Linux | | All Red Hat Enterprise Linux | SELECT 1 FROM os_version WHERE | | | | hosts | name LIKE '%red hat%' | +--------------------------------+----------+--------------------------------+---------------------------------------+ | All Linux | | All Linux distributions | SELECT 1 FROM osquery_info | | | | | WHERE build_platform LIKE | | | | | '%ubuntu%' OR build_distro | | | | | LIKE '%centos%'; | +--------------------------------+----------+--------------------------------+---------------------------------------+ | chrome | | All Chrome hosts | select 1 from os_version where | | | | | platform = 'chrome'; | +--------------------------------+----------+--------------------------------+---------------------------------------+ Seems simple enough, just looks like some logic where if the value you are querying for exists, then return 1 and we can craft a custom label like so: select 1 from plist where path = '/Library/Managed Preferences/com.mycustom.info.plist' and key = 'department' and value = 'Information Technology' ; Pro-tip, if you navigate the /labels URL in your FleetDM admin console, there is a GUI where you can drop a query in place and then name the label. I am not sure if FleetDM is going to add more UI features around this or go more into their GitOps approach. I personally really like both approaches, and I feel that if a product can pull off where everything in the UI you can also do through the API (and vice versa) that is the absolute best of both worlds. Now to finally check our work, and we can see that when we filter by label we now can use a label for the IT department! I now see a non-zero amount of devices in FleetDM with the Information Technology Department label applied to them. You should be able to now replicate this for all departments, cost centers, and any other LDAP attribute you can map into Jamf Pro and payload to disk via custom Configuration Profile payload. ","date":"2024-03-21","objectID":"/posts/using-custom-mdm-payloads-for-fleet-labels/:0:2","tags":["labels","fleetdm","osquery","data","identity"],"title":"Using Custom MDM Payloads for FleetDM Labels","uri":"/posts/using-custom-mdm-payloads-for-fleet-labels/"},{"categories":["leadership","development","mentoring","growth"],"content":"Success Isn’t Linear, Nor is it Transitive With the climate of tech workers in a weird state post Covid-19 pandemic, and with the rise of the tech influencer I feel this topic is not only relevant, but also important. There are people who really want to break into a tech career from their current path, and there are people who want to change careers within tech itself. Whether you are looking to level up your existing career, make a move into a different section of tech, or just trying to break into the tech industry any way you can, you have likely heard many other people tell their success stories. While there is value in other’s path to success, it is by no means a guaranteed repeatable process. In fact, it can be quite the opposite. Technology, business, and the economy (or things adjacent to these subjects) are a living and evolving thing. Someone who came up through the ranks of success in the 1980s is likely not going to have a success story that is repeatable in the 1990s or 2000s, just simply because the times are different. Things change, not only from a technology perspective but also from every other external factor that influences our lives one way or another. It is also worth stating that success is not a linear journey, as you will have setbacks, and then you will need to make a bit more forward progress to get back to where you were, or just ahead of where you last were. There aren’t set levels you can just level up on a single track with. You must work with what you have, and where you are currently at. This means that everyone has a different starting position when trying to make change in their professional lives. Just like some folks may have more privileges than other folks when starting out. ","date":"2024-03-18","objectID":"/posts/success-is-not-transitive/:0:0","tags":["leadership","IT","team building","leveling up","professional development"],"title":"Success Is Not Transitive","uri":"/posts/success-is-not-transitive/"},{"categories":["leadership","development","mentoring","growth"],"content":"My Origin Story is non-Transitive I started out in the late 90s working in tech. Back then if you could swap a hard drive and install an OS you pretty much qualified for an entry level computer tech or desktop support gig. I didn’t have any spectacular set of skills by today’s standards in the 90s, but I am a product of the 90s as that is when I entered the work force. I did warranty repairs, desktop support, OEM systems building, and many similar things. Later on I went to work IT for 2 different school systems. One of them I was a hybrid support and system administrator, and at the second one I was a Sr. System Administrator and started to delve into the engineering side. From that point on I have held various engineering focused jobs that were more geared toward automating with code. The big thing here about my career path progression, is that I had this huge middle step to support my path to engineer in system administration. Timing was everything here, because I quickly migrated toward an engineering first perspective, and now my team automates a large majority of system administrator tasks. So, the demand for folks doing System Administration work in the tech industry, has been going down over time, and the demand for engineering focused jobs has gone up. Things like DevOps has become a series of concepts many IT Engineering teams have adopted. We have seen other jobs pop up as well that have sort of taken over or been involved in device management like CPE and SRE While not every Org out there has taken this approach, we can also look at things like the diffusion of innovation which is something we have observed over time with technology and the adoption of said technology. Is the System Administrator dead? Oh no, it is not, and in fact in many industries it may be thriving. My career path was always moving toward working in the tech industry. Not everyone wants to work in the tech industry, and there are plenty of other industries that offer great jobs. However, for my specific success path, I think it would be much more difficult for someone to get onboard at a tech company and then move their way up from say help desk, desktop support, and then eventually some sort of IT engineering job. This is my opinion, and not fact, but it comes from the idea that the mid-career progression job I had as a Sr. Sys Admin, is just less common at high-tech orgs these days. That job allowed me to do all my work as a tech worker without having a hard requirement to write code. So I was able to write code as I progressed, and I was able to do it a pace that worked really well for me. I don’t feel this opportunity is no longer in existence, but rather it is just much harder to find in some industries. My Sr. Sys Admin job is really the one that allowed me to transition into engineering. If I had not had that opportunity my path could have ended up much differently. So, I typically don’t tell other people to follow my footsteps. I don’t think my path is as relevant as it was back in the late 1990s and early 2000s. I tell folks to focus on core fundamental skills, and I feel those are the most transferable today. ","date":"2024-03-18","objectID":"/posts/success-is-not-transitive/:0:1","tags":["leadership","IT","team building","leveling up","professional development"],"title":"Success Is Not Transitive","uri":"/posts/success-is-not-transitive/"},{"categories":["leadership","development","mentoring","growth"],"content":"Other’s Success isn’t Transitive Either One thing we often do in our communities, is share what works and what doesn’t work for us. Add in things like scale and requirements from our employers, and you will get a plethora of answers and a lot of them may not ever work for you in your current work environment. I stopped assuming what I could accomplish is also what others could. There are always factors why a good idea, is actually not a good idea and being able to tell the difference can be quite difficult at times. So, even in modern times of today, if someone builds something that works and scales at their Organization, it may never work anywhere else just due to the nature of the Org and the job. This is one factor why writing and building open source tools is so difficult. It is so hard to balance an abstracted high level tool that has many use cases, without tons of people wanting to file issues to meet their specific niche needs. I honestly have so much respect for open source maintainers, as they have a very difficult position to manage. Since it is very difficult for an open source maintainer to account for every person’s problem at every potential job site, they have to abstract their tools into something that just gives the end user of that tool the options to use it how they best see fit. It ends up being more like a framework or similar. In the commercial software space, you can just hire someone to build something to your specification. The tradeoff is that cost you money, but you can get what you want (and what you pay for). So, what you may come to learn is, even though you can communicate with your peers across the all the tech communities, their successes on what they do may never work for you or your Org. That is also okay, and sharing our successes is still a great thing to do. It opens up ideas, squashes echo chambers, and diversifies your thoughts even if their successful solution won’t work for you. ","date":"2024-03-18","objectID":"/posts/success-is-not-transitive/:0:2","tags":["leadership","IT","team building","leveling up","professional development"],"title":"Success Is Not Transitive","uri":"/posts/success-is-not-transitive/"},{"categories":["leadership","development","mentoring","growth"],"content":"Tech Influencers and Content Creators Is an Influencer and a Content Creator synonymous? I honestly do not even know the answer to this. What I do think is that they are likely going to be around for a while. That is okay too, there is nothing really inherently wrong with either of these things, because if you don’t like it, you can simply ignore them. What I would like to point out though is that many of these folks have content that can boost your career in tech. Oftentimes an ex tech worker turned content creator will use their platform to replace their job. The reason I am mentioning this specifically is I have noticed a few trends around Influencers and Content Creators, and some of them seem to be selling their own personal success stories. I am not very well versed in this subject, but I definitely follow several of them on LinkedIn and other socials. Since most things are relative, or at least have some relativity when it comes to a human’s personal journey through life, I would suggest taking precautions with any Content Creator or Influencer that wants to bootcamp style train you for a job in tech, basing it off their own personal success. I won’t name any names, but I have definitely found some real genuine and great content creators out there that do in fact post meaningful content which I think is mostly good. A common occurrence among the ones I like, is they focus more on core fundamental skill building over selling you a success story. Most of the ones I really like, rarely (or never) sell their own personal success as direction on how to succeed. ","date":"2024-03-18","objectID":"/posts/success-is-not-transitive/:0:3","tags":["leadership","IT","team building","leveling up","professional development"],"title":"Success Is Not Transitive","uri":"/posts/success-is-not-transitive/"},{"categories":["leadership","development","mentoring","growth"],"content":"Fundamentals and Core Skills are Successful! When it comes to building a path to success, I do believe that developing core fundamental skills are a great place to start. They can be somewhat transitive, and this is a space where others success may be more relative to your own. Fundamental skills aren’t a guaranteed path to success in your career, but they are something that can help you be a life-long student, and they are something you can always fall back on. When an athlete plateaus or recovers from an injury, or to strengthen their skills, they do tons and tons of core fundamental drills and exercises. A musician will do repetitive drills, scales, sick riffs, produce sweet beats, music theory, and many other core fundamental skills that help them hone in on their craft. No one is born with innate skills or knowledge of anything, everyone must build them over time. There are a set of core fundamentals for many things humans do, and tech work is honestly no different. So, if you are trying to level up, or stand out in the crowds in this job market, remember to fall back on your fundamentals and focus on your strengths. If you are trying to get into tech, keep focusing on building strong fundamental skills over time. Remember, no one is born with this, everyone has to put in time to build these skills. So, don’t think you can shortcut it. It is okay to take time, it is okay to build them at your pace. What matters is mostly if you are progressing. ","date":"2024-03-18","objectID":"/posts/success-is-not-transitive/:0:4","tags":["leadership","IT","team building","leveling up","professional development"],"title":"Success Is Not Transitive","uri":"/posts/success-is-not-transitive/"},{"categories":["leadership","team","management","collaboration"],"content":"I started working in tech back in 1999. My first tech job was a repair technician at a computer store chain. I did hardware and software repairs, warranty repairs, OEM systems building, and similar support type work. Over the years I’ve likely had a similar experience to most other workers in tech. I have had bad bosses, and good bosses. I also have worked with great people, and not-so-great people as well. I will say that I am lucky in the sense that most folks I have worked with have generally been great, and the not-so-great experiences were very minimal. Throughout my career I have done technician work, IT support roles, System Administration, System Engineering, and I have done so in both public and private sectors. As of last year I finally took the plunge and jumped from IC (individual contributor) to M (manager). I also worked in vendor space and traveled for work internationally and got to peek behind many curtains. I ended up in Silicon Valley, and then was the main onsite engineer for many Silicon Valley companies. I have been exposed to many philosophies, methodologies, and have interacted with tons and tons of really smart and amazing people. Over time, you start to take mental note of things. Things that work, that don’t work. That scale and don’t scale. These experiences are oftentimes also big teaching moments for you and your professional growth. So, all things considered this is how I have come to where I am currently at in regard to building an IT engineering team. I started at my current job in 2019 with the agreement that when the team expanded, I could build the team how I saw fit. My boss at the time agreed, and over the next 5 years I built the team out, designed the roadmaps, and eventually became the team manager. ","date":"2024-03-05","objectID":"/posts/building-it-eng-team/:0:0","tags":["leadership","IT","team building","leveling up"],"title":"Building an IT Engineering Team","uri":"/posts/building-it-eng-team/"},{"categories":["leadership","team","management","collaboration"],"content":"Get Rid of What You Don’t Like One of the first things I established when I started at my current place of employment, was to get rid of everything I did not like from my previous jobs. If I was going to get top talent to join me, I should have a great environment to work in, and I should give the best potential opportunities to folks wanting to join the team here. So, I decided that everything I did not like about IT or Engineering I would throw out, unless my boss overrides my decision. Which I was 100% willing to let that happen if or when that came up. I first tackled packaging and third party app patching. I set up and open sourced automatic patching and a DEP Notify enrollment starter scripts. I did not want to waste any time building packages or dealing with device enrollment. So, I cranked out these two things in my first 3-4 weeks. Later on I retired the AutoPKG + JSS Importer workflow for GitOps Munki. I started making a list of things I wanted to accomplish as the sole CPE and eventual team lead as we expanded here. I work for a data company, so the data skills were already in the back of my mind, but trying to find a CPE that is good at data stuff could be over complicating my search for team members. So, it was a nice to have. Here goes the wish list I started with when building out the team here: No overcomplicated processes or workflows that we control No scrum or project management that doesn’t have a positive return on investment (for our work) No manual labor when we can help it Diversify Skills Diversify Tools Be able to pivot Avoid vendor lock-in as much as possible Build with cloud, security, and engineering first philosophies Always leverage server-less where we can To provide some context of how I approached these ideas, here are some things I have built or established in the past 5+ years. There will be no dedicated QA, as I feel QA is largely misunderstood in general in tech, but I think even more so in the IT/Ops side of the world. Instead, we would adopt a peer review model. You are never allowed to self-publish your own work into production, unless another human signs off on your work. Meaning QA is now everyone’s job, and it meant I can now focus on hiring engineers that can code and automate. Furthermore, I had the team build a CI/CD automation pipeline, and with the features of git and things like branch protection. Now I could force another human looking at someone’s work before it gets merged back to a main branch. I also had submitting pull requests to the prod branch disabled, and the only way the prod branch gets updated is through promoting our stage branch to production. This also means that the entire team will be collaborating within our build tools and our development environment, which helps with consistency across individual’s work. I have just seen too many bad practices from IT engineers over my time that I know even a human with the best intentions can take shortcuts when they feel pressured to do so. I also have committed bad practices myself once or twice. Setting up a GitOps workflow, with branch protection and forcing at least one peer review before you can merge solves a lot of problems I have seen IT shops have in my lifetime. It also vastly improves delivery as now things are automated and the basic day-to-day system administrator tasks are now done by a single computer in the cloud running our CI pipeline. Servers are a pain point. No matter how you look at it, they are a pain point for every team. They require labor to stand up, maintain, patch, configure, and let’s not forget all the compliance scans you are now subjected to for every server you own. To mitigate this pain point I did not like, I have set a precedent that we will always try to leverage server-less compute first and foremost. Things like AWS Fargate, Lambda functions, cloud storage, CDNs, PaaS, API Gateways, so on and so forth. You cannot install agents, vulnerability scans, patch, or even log into any of t","date":"2024-03-05","objectID":"/posts/building-it-eng-team/:0:1","tags":["leadership","IT","team building","leveling up"],"title":"Building an IT Engineering Team","uri":"/posts/building-it-eng-team/"},{"categories":["leadership","team","management","collaboration"],"content":"Diversify Everything You Can One mistake I have witnessed over and over again in my career, is that Orgs keep hiring for specific tools, or they hire a team of people with the same skills. I feel both of these ideas can work, but are ultimately a suboptimal way to build an IT engineering team. If I hired a bunch of Mac Admins, that is what I would have, and it would not expand much beyond that. I wanted to hire for specific skills and experience, and I wanted to diversify those skills and experiences across the team. So, I hired Windows Engineers, macOS engineers, DevOps engineers, Sys Admins, Mobile Engineers, VDI experts, etc. A diversified set of skills and experiences will enable your team to build so much more, and do so from a broader point of view. I feel that this avoids things like echo chambers, and poor decisions that build exponential more tech debt over time which could be mitigated if you had pivoted to something else instead. So, the first hires we did for our team I went out and got a Sys Admin to help out with the day-to-day work, DevOps engineers and a Windows Engineer to be the SME for our Windows deployment. I feel we have positioned ourselves to be more agile and able to pivot across many tech stacks if the business needs us to. It also offers opportunity for cross-training, mentorship, and the ability to actually try something new and not be siloed into one aspect of tech work. I have been siloed before, and it was always a driver for me to start looking for a new job when it happened. Having a diversified team positions you to be able to accomplish so much more than just a bunch of people with the same overlapping skills and experience. Diversify your tools stack as well. Another massive problem I have seen IT teams do, is they will pile so much tech debt on top of an existing tool because they can, and perhaps because it is the easiest way to do that. I have highly avoided using any of our MDM tools for anything that is business critical, or integrate our MDMs with anything that would lock us into that MDM vendor. I don’t want to be in a position where we are unhappy with our software vendors, but due to a giant mess of a tech stack we cannot migrate from the tools we have all because we made bad design decisions early on. Then over time, piled so many things on top of those bad design decisions. Bad decisions happen, we do it all the time. I have done it plenty of times myself. What makes a bad decision worse, is when you pile it onto something that locks you into a bad decision for life. I don’t want to be the team that cannot pivot or modernize because we decided to build a monolith and pile onto it for years versus diversifying our tech stacks to use tools that make the most sense. ","date":"2024-03-05","objectID":"/posts/building-it-eng-team/:0:2","tags":["leadership","IT","team building","leveling up"],"title":"Building an IT Engineering Team","uri":"/posts/building-it-eng-team/"},{"categories":["leadership","team","management","collaboration"],"content":"Build SMEs and Cross Train This is something that is very tough to do. I am still trying to figure this out to be honest. Everyone has to work and deliver in a tech job. Employers aren’t going to pay you to just learn things all day, they pay you to do work. So, we started assigning folks a subtitle of SME in their specific area. We have SMEs in Windows, macOS, Linux, Cloud, CI/CD, VDI, Mobile, DevOps, so on and so forth. A person can be SME in more than one subject as well, there aren’t any real hard rules here. I also expect all my Sr. Engineers and above to be mentors in some capacity, as well as mentees. No one can know everything, so everyone has an opportunity to learn. So, I drive collaboration to our team channels in Slack, and I will identify things I feel are IC 1 level work in a specific subject and then farm it out to the team for anyone who is new to that specific tech subject and wants some experience. For example, I might identify something that is a nice to have, and something I would consider IC1 or IC2 engineering level work I simply create a Jira task for it and then ask the team who needs practice in this subject? The struggle here is you must train everyone to not just fix things in a matter of minutes. A Senior level engineer or higher should be able to do these type of things in minutes, but to a beginner that could be a slight challenge, and we want to provide hands-on experience to grow the team’s collective skills. So, now I am having all my engineers backlog anything that could be considered IC1 or IC2 level work, and it is not a high priority. Then anyone who doesn’t have experience in that subject they can grab that IC1/IC2 level work from the backlog and attempt to do it. The big problem here is training your SMEs to not just instantly fix the problem, or implement the nice to have immediately. I feel this is a good method for cross-training. ","date":"2024-03-05","objectID":"/posts/building-it-eng-team/:0:3","tags":["leadership","IT","team building","leveling up"],"title":"Building an IT Engineering Team","uri":"/posts/building-it-eng-team/"},{"categories":["leadership","team","management","collaboration"],"content":"Optimize From Past Experiences I have seen scrum done many ways across multiple jobs. I have had jobs where every team just gets to run their own concept of scrum. I have seen upper management force a top-down model of scrum across an entire org. I have seen ClickOps workflows in Jira that would just annoy you because it is manual labor you must click on every single step for each item. I have also seen things that should never be in scrum, that were all over it. Things like updating packages, this is a sustaining task that should be automated, not wasting hours of time doing paper work on it. I am not saying scrum is bad, because it is really how you implement it. I am not saying an org should or should not use scrum. I am honestly indifferent to it as a framework/tool, but I have seen so many iterations of it that make no sense. Even when I look at story points, which in my view, are supposed to only measure capacity and priority. Yet, so many orgs use it as a measurement of performance. Which it isn’t meant to be. I have even heard things like story points will justify more headcount if a team demonstrates they need more headcount by the number of story points they can deliver. I have never in my life seen a headcount get created due to story point data. I will wait until I see that happen! So, I tossed out everything about project management and scrum and just started with agile and then just decided we would build this out over time. We ended up on something like this, for our work: Simple kanban board no story points no estimations focus on quarterly deliverables vs managing sprints status updates in a specific status updates Slack channel meet when we need to by going direct 1 team meeting per a week Making these changes from every other job I have had in the past 10-15 years has made life so much easier for us. There is no time-wasting or pressure to estimate everything you do in a scrum board. We don’t have daily stand-ups, you just type your status update into a Slack channel. I try to give as much time back to my ICs as I can, so they can spend that time developing solutions, peer reviewing other team member’s work, collaborating and cross-training, and mitigating burnout. Now, I would like to also say, the things I got rid of could easily come back. I want to position our team where we can decide what works best for us, and what makes us most productive. If going back to scrum + sprints does this, then we shall adopt that. If it doesn’t have a ton of value, then we will not. We want to remain agile and be able to pivot whenever we must, and do so with the least amount of friction. Also, this is for our work. In cross team collaboration scenarios there is likely a PM and the rules change. We are agile and will adapt to how a PM wants to manage a project. We will give estimations when we can get the work done, and we try to be honest about timelines. ","date":"2024-03-05","objectID":"/posts/building-it-eng-team/:0:4","tags":["leadership","IT","team building","leveling up"],"title":"Building an IT Engineering Team","uri":"/posts/building-it-eng-team/"},{"categories":["leadership","team","management","collaboration"],"content":"Don’t Worry About Failure If there is one thing I have learned over the years, and I try to establish it as a philosophy for our team, is that failure is just a part of the process. You just need to keep going, keep progressing, and don’t let failure block you from expanding things. Failure is also an opportunity to take mental notes of what doesn’t work. Remember, you may not always know what to do, but you likely have a list of things not to do that you know from previous experience. If your team members are afraid to fail, then they may not take risks, or try things that in their mind may be perceived as failure. Encourage taking risks. If I played it safe my entire career I would have never grown skills or experience wise. Many years ago I was afraid to swap to a new programming language. I knew the languages I already used pretty well, or well enough to do most of the things I needed to do in my job. I was afraid that it would just cause complexity, and I had definite hurdles in my way. However, one day I just decided, no more code in the languages I know! I only wrote code in the new language I was trying to learn, and I grew so much and faster than I ever had because I learn best by doing actual work. I often highly encourage folks in the Mac Admin Slack to expand into new languages and new tech. What if you were a Mac admin that also knew Linux? Perhaps you are a Mac Admin that can code in Swift, Python, and Go? Let’s not forget everyone’s favorite in the Mac Admin Slack, what if you also knew how to manage Windows devices? No one is born with any knowledge or skills, and humans must train or practice over time to build those things. This makes you a better engineer, and it makes you more desirable by employers the more diversified your skill sets are. Also, I have only been a manager for a year now. I have so much to learn in this role, and I oftentimes have no idea what I am doing. I ask for help and guidance from my manager and my peers. I go to LinkedIn to read some leadership articles. I try to find out what other managers in tech are doing with their teams. I will likely fail a lot while I learn how to transition from an IC to a Manager and I will likely learn a bunch in that process. This is okay, this is to be expected. I have hopefully learned enough to mitigate all my failures to be non-impactful to anyone though. The point is though, I am no longer letting fear of failure stop me from trying to grow and learn in my life. ","date":"2024-03-05","objectID":"/posts/building-it-eng-team/:0:5","tags":["leadership","IT","team building","leveling up"],"title":"Building an IT Engineering Team","uri":"/posts/building-it-eng-team/"},{"categories":["security","threat models","IT","collaboration"],"content":"IT Teams are also Security Teams If you have worked in some tech related job odds are you have also worked with various security teams throughout your career. There has definitely been a divide between IT teams and Security teams I have observed in my near 25 years working in tech, but that doesn’t have to be the case. During my career I have definitely observed security “punting labor” over the fence to IT teams, and I have seen IT teams dig their feet into the ground when security wants to change things. This is just the wrong way to collaborate with IT and Security teams all together. I am very lucky and privileged to have at least been able to work with some passionate and collaborative security teams in recent years. When you approach the relationship of IT and security this way you get such a better return from every team’s work, and you find yourself working with security teams on a constant basis to deliver solutions to your organization. I have also been lucky enough to be mentored by some of our security leaders and engineers over the past few years in my current job. So, here is a good old blog post about how IT and Security should be best friends! ","date":"2024-02-27","objectID":"/posts/cpe-is-a-security-team/:0:0","tags":["security","IT","threat modeling","tech"],"title":"IT Teams are also Security Teams","uri":"/posts/cpe-is-a-security-team/"},{"categories":["security","threat models","IT","collaboration"],"content":"Engage as Partners We have a security partner program here at my current place of work. I am also a security partner, as I possess some expertise and skills in some specific tech domains. This allows me to apply that cross functionally, and help enable other IT staff to be better at security, and also help security bring domain experts into their consultation. It is unrealistic to expect security staff to be domain experts on your tech stacks. Likewise, it is unrealistic for security to tell IT how to design and scale systems. This is a generalization, so don’t think this is a binary and absolute thing, as it is not. However, the generalization is the conceptual point I want to make here. Let the experts be experts, and work with each other from a partnership point of view. This is easier said than done, but one pro-tip I can give everyone who reads this post, breaking bread with other humans is a great way to start. One of our security leaders started bringing food to our meetings, and he would ensure we all had our snacks, and we chatted over food. As cheesy as this sounds, it worked! We were more relaxed, we got to snack on some food, we started chatting more casually about things, and we agreed to work toward common end goals. So, make a security partner program, train the security partners on how to be more security focused, because security is not just one department’s job, it is everyone’s job. It is very unrealistic to think that one department can cover every single risk an organization encounters on a daily basis, thus we do things like end user security training, partnerships, sync meetings, and so forth. I volunteered to be a security partner about 4-ish years ago, and have helped many folks across our org write threat models. I have been able to apply my work experience, my expertise, and my all around systems’ knowledge to this security partnership. ","date":"2024-02-27","objectID":"/posts/cpe-is-a-security-team/:0:1","tags":["security","IT","threat modeling","tech"],"title":"IT Teams are also Security Teams","uri":"/posts/cpe-is-a-security-team/"},{"categories":["security","threat models","IT","collaboration"],"content":"Threat Model Your Tech Stacks This one can be controversial, but I have personally come to terms with it a few years ago. When I was a Principal Engineer designing systems at scale, I was the expert designing them. I knew the tech’s features and abilities, I was best qualified to describe how the system worked, and how data flowed. Which is the bulk of a threat model. I also think that if you are in a position to design a system, the way you qualify yourself to build it, is if you can threat model it. I know that sounds a bit harsh or maybe absurd to some folks, but hear me out please. The designer and implementer will have the most intimate knowledge of the system, they should be able to describe how their tech stacks work together. If they cannot, maybe they should learn how they operate before building it in production. That is why we have POC and test environments, and why we run pilots, so we can get to know our tech stacks well before we ship them to production! I have learned how to use STRIDE, which is a threat modeling framework. It is honestly not too difficult to understand the bits you need to once it finally clicks! While that is true of most things, at least this is a framework, so it is very repeatable once you learn it. The table below abstracts what S.T.R.I.D.E. stands for, and defines each threat to model in your tech stack. Threat Desired property Threat Definition Spoofing Authenticity Pretending to be something or someone other than yourself Tampering Integrity Modifying something on disk, network, memory, or elsewhere Repudiation Non-repudiability Claiming that you didn’t do something or were not responsible; can be honest or false Information disclosure Confidentiality Providing information to someone not authorized to access it Denial of service Availability Exhausting resources needed to provide service Elevation of privilege Authorization Allowing someone to do something they are not authorized to do As a systems designer, you should be able to data flow diagram (commonly referred to as DFD) your tech stacks and define each threat throughout the DFD. Each threat should have a mitigation or compensating control. IT and Security teams should walk through the threat model together so they both better understand what is being published to production. ","date":"2024-02-27","objectID":"/posts/cpe-is-a-security-team/:0:2","tags":["security","IT","threat modeling","tech"],"title":"IT Teams are also Security Teams","uri":"/posts/cpe-is-a-security-team/"},{"categories":["security","threat models","IT","collaboration"],"content":"Building a Threat Model Let’s assume we have a simple integration we are building from a SaaS application into cloud storage, where our in-house application we have built will access the cloud storage data to do the thing it was designed to do. A simple DFD: Quick notes on the DFD: Entities are defined by the square like shapes (may not be exact squares, could be rectangles haha!) Processes are defined by the circles (this is to show what actions or workflows are taking place) Label each threat with the corresponding letter in S.T.R.I.D.E. Label the trust zones (will explain below) Take the DFD and some form of notes describing the diagram to consult with your security teams Trust zones were something that took me a moment to really get what they are meant for. They are arbitrary numbers you assign a value to, and it is more of an offset than it is a scale. Assume trust zone 0 is the absolute lowest trust zone, and trust zone 9 is the absolute highest and most trusted of zones. What you are wanting to represent in your DFD is that you are either going up in trust zones or you are going down in trust zones. Typically, security teams will have some baseline definitions of trust zones like the end user is always in trust zone 0 no matter what, and only systems we fully design and build are in trust zone 9 as an example. I used this example in the DFD above as well. SaaS apps you do not own nor do you have direct access to the host, so it is a bit of a mystery box to you and your org on what goes on in the app. However, you likely have some level of admin access and some level of logging, and they aren’t an end user so to remain relative to the flow I marked them as trust zone 1. As we can get some data, and we have some control out of the SaaS app. The private cloud I marked as a trust zone above the SaaS apps as we have more controls in this space. Things like, but not limited to: IAM, role assumption, RBAC, permissions, and other various forms of protections and limitations we can impose to follow the least privilege model. While the in-house app is something we can put in the highest trust zone, zone 9, as the company owns the code, employs the devs that built it, and essentially has full control over it. You can just as easily bumped the SaaS apps to Zone 3, the private cloud to zone 6 and the in-house app would remain a trust zone 9, because it is all relative to the model. It still conceptually makes sense. This was a big hang up for me in the beginning of learning how to threat model. Now that I know these are arbitrary values to show a relative movement in trust zones (going up or going down) it is much easier for me to grok. When changing a trust zone, an information disclosure threat should be automatically generated. Which is the I in S.T.R.I.D.E. This is because traffic of your data is transversing through various systems you have varied control over. So, some standard concepts should be defined by your security team you can follow, and then you just need to apply the relativity of your tech stacks through the DFD. ","date":"2024-02-27","objectID":"/posts/cpe-is-a-security-team/:0:3","tags":["security","IT","threat modeling","tech"],"title":"IT Teams are also Security Teams","uri":"/posts/cpe-is-a-security-team/"},{"categories":["security","threat models","IT","collaboration"],"content":"Use Your Systems Knowledge to Describe Threats One of the ways we have described the threats is a syntax called Gherkin, which uses some specific words to describe a threat to each scenario or process. It goes something like this: Scenario: User Authentication Threats: Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege When a user authenticates to the sytsem, that user must prove to the authority federated to it that it is a valid user and authenciates to the IdP Then that user is validated through our IdP and must provide MFA to proceed AND all traffic is ecnrypted over HTTPS AND all login actions are logged to validate the user is a valid user Given a valid authentiation session, the user will assume the role they are assigned using least privilege models Given the app suffers too many login attempts, it will temporarily block IP addresses to mitigate DDoS attempts I will admit, that is not my best work above when writing out the threat and mitigations, but I tried to keep it generic enough that it makes sense. So, this would represent the Authentication process in the DFD. You would then go through each process in your DFD and use the above syntax to describe your threat. You don’t have to use Gherkin to do this and Gherkin is more of a framework than an authoritative language. You can bend it to your preference to use it in a way that it makes the most sense for your Org. Also, the description covers each letter in S.T.R.I.D.E. used. Another pro-tip to fast track this, is that some common threats like information disclosure can oftentimes be mitigated by enforcing TLS/HTTPS traffic. Which really almost everything nowadays does this. So, you don’t need to overthink each threat. I also find Google Docs to be a great way to actually describe the threat model as well. Simply because a Google Doc is a living and collaborative document. You can tag folks in them, you can assign tasks, and so forth. So, something like Gherkin isn’t even required, but it is something I learned, and I feel it has value if you are looking to have a bit more strict standards in your threat modeling. ","date":"2024-02-27","objectID":"/posts/cpe-is-a-security-team/:0:4","tags":["security","IT","threat modeling","tech"],"title":"IT Teams are also Security Teams","uri":"/posts/cpe-is-a-security-team/"},{"categories":["security","threat models","IT","collaboration"],"content":"Using the 80/20 Model Threat modeling should follow something around the 80/20 model give or take. Where you are expected to cover about 80% of the data flows, processes, and threats. If your security wanted to force 100% coverage on a threat model, then nothing would get done in a timely fashion, and you could spend weeks or longer chasing down every edge case. You can focus on getting about 80% of the threats covered up front and the last 20% as you go through the process, or after you go to production. No one wants to block folks from shipping product, or block them from doing anything that impacts the business in a negative way. So, both IT and security teams should work together to streamline this process. ","date":"2024-02-27","objectID":"/posts/cpe-is-a-security-team/:0:5","tags":["security","IT","threat modeling","tech"],"title":"IT Teams are also Security Teams","uri":"/posts/cpe-is-a-security-team/"},{"categories":["security","threat models","IT","collaboration"],"content":"Building Security Minded Development The real end goal of something like this, is to get every individual to think about security when they are designing systems. Doing your own threat models helps this cause, and this is probably the biggest reason why I support the concept of having security partners, having IT involved in threat modeling, and having implementers threat model their own tech stacks. In the end, if done right, this can transform an entire organization into a security driven one, and everyone benefits from this. I do honestly believe that this process has made me a better engineer. It has made me think more and more about secure design, and honestly about convoluted workflows. If something is way too complex to threat model, then perhaps it is just too complex to even do. That is a sign for me personally to pivot, and look at accomplishing my goals another way. Not because it is too difficult to do, and I want it easier, but rather with complexity comes a ton of other problems. The threat model process sorta surfaces things like that. ","date":"2024-02-27","objectID":"/posts/cpe-is-a-security-team/:0:6","tags":["security","IT","threat modeling","tech"],"title":"IT Teams are also Security Teams","uri":"/posts/cpe-is-a-security-team/"},{"categories":["security","threat models","IT","collaboration"],"content":"CPE Teams are Security Teams If you work in the CPE space (Client Platform Engineering), I would argue you are already working in security. A lot of what a CPE does is around securing the end user devices, reducing risks with the end user devices, and acting as a front line defense on end user devices. I believe CPEs are also more aligned with software engineers and DevOps engineers than compared to the classic Sys Admin. As a CPE team member, you can leverage your coding skills, GitOps, and automation pipelines to really help secure an end user device at an organization. So, CPE teams are already doing security work, building threat models for their automation systems and infrastructure should not be too difficult given their expertise on building and designing said systems. ","date":"2024-02-27","objectID":"/posts/cpe-is-a-security-team/:0:7","tags":["security","IT","threat modeling","tech"],"title":"IT Teams are also Security Teams","uri":"/posts/cpe-is-a-security-team/"},{"categories":["security","threat models","IT","collaboration"],"content":"It Truly is a Community Effort That age-old saying, “It takes a village,” is very applicable here. Security will need to work on building an internal community around enabling teams to be more security driven. Security will also need to listen and take feedback from those teams to ensure there is actual community and collaboration happening. Just like IT needs to also put forth their effort in security as well. Once you build a culture of collaboration and community, it makes the debates easier to have. Bonus if your org has lots of data, because you can use data to help drive these conversations and help make decisions. So, in the end we all work together and are on the same team, and we need to act like it. ","date":"2024-02-27","objectID":"/posts/cpe-is-a-security-team/:0:8","tags":["security","IT","threat modeling","tech"],"title":"IT Teams are also Security Teams","uri":"/posts/cpe-is-a-security-team/"},{"categories":["data","osquery","snowflake","data modeling"],"content":"Data Modeling osquery Data in Snowflake We have been working on many projects the past few months, and osquery data pipelines with the help of FleetDM has been one of our constant works in progress. Before I dive into the data modeling part, I wanted to share how we scaled this data out. We ended up going forward with FleetDM’s SaaS offering over self-hosting the app. However, we are still streaming query results from FleetDM cloud to Snowflake over AWS Kinesis Firehose and we now have data streaming from FleetDM into Snowflake at a very fast rate and high volume. We are typically collecting data from osquery every hour on most things, and we have a lesser number of queries that run every 12 hours or once a day. ","date":"2024-01-25","objectID":"/posts/more-osquery-data-modeling-snowflake/:0:0","tags":["macOS","IT Tools","osquery","snowflake"],"title":"More Osquery Data Modeling in Snowflake","uri":"/posts/more-osquery-data-modeling-snowflake/"},{"categories":["data","osquery","snowflake","data modeling"],"content":"Scaling the Data When a query comes into Snowflake, it is just a JSON document with lots of keys and values, with some of those key value pairs being nested. Snowflake treats this as a variant data type and if you are familiar with working with dictionary data in say Python or Ruby this might click pretty quickly how one can simply parse native JSON in Snowflake. select column['key']['nested-key']::type-cast In the above pseudocode is a very quick and dirty example of how one could get a value from a nested key using SQL in Snowflake. This assumes you have a column with just raw JSON data inside it. However, when you are streaming data from thousands of endpoints to cloud storage and then rapidly ingesting them into Snowflake you end up with many JSON files to parse. Parsing the JSON can be a much heavier compute process at scale. So the first thing I did was work with my data engineering team here and setup what is called key clustering This pre-processes the JSON files as they come into Snowflake and separates specific keys into their own columns, and you can then model the data further after that. The big benefit of this is that you get all that indexing and caching features where you don’t really get that by parsing raw JSON variant data. So, we had already made some design decisions in our config files around this before, and we were now going to leverage those additions. We split up the following key/value data sets into their own columns from FleetDM osquery results: Serial number (unique identifier, can be used as primary key across many data sets) Hostname (not unique, but some tools only collect hostnames from other data ingests) Hardware UUID (unique and MDM tools collect this, so another primary key) Date stamp (we are going to be filtering by date ranges a bunch) Name (the query pack name, which again would be a very common filter) The main idea here is that we were splitting out values of data that we knew we would use heavily for filtering and joining other data sources to it, and this helped us scale immensely. From our design decisions, we also added decorators for the serial number, hardware UUID, and the hostname. These are included in all query results. These can be useful primary keys to join to many other data sources in Snowflake downstream. Remember to always think about making data quality decisions upstream, so you can benefit from them downstream. To give a quick summary of performance increase is I was querying 300 million rows of raw JSON data at first, and it would take 20 to 30 minutes to complete my query pre-setting up the key clustering. Post key clustering that same query could complete in around 30 seconds using the exact same compute. That is a pretty amazing increase and a vast UX improvement for anyone working with this data. So, if you go down the journey of streaming data to your Snowflake instance and want to really see performance increases definitely consider key clustering. Also, please talk to an expert on this subject. I am not an expert, but am lucky enough to work with a lot of experts on our data engineering team that works with us when we need their help. ","date":"2024-01-25","objectID":"/posts/more-osquery-data-modeling-snowflake/:0:1","tags":["macOS","IT Tools","osquery","snowflake"],"title":"More Osquery Data Modeling in Snowflake","uri":"/posts/more-osquery-data-modeling-snowflake/"},{"categories":["data","osquery","snowflake","data modeling"],"content":"Some Examples We Have Built The osquery log stream from Fleet DM has 3 different data sets you can ingest. It has the query results, which is the bread and butter of the data, the audit log, and the osquery status. The audit log is just what you probably think it is, a full on change log of what is happening in the application itself. I have to hand it to FleetDM the log is very thorough, and it is one of the better ones I have personally seen. The last bit of data is the osquery status logs. Audit Logs create or replace view DB.SCHEMA.OSQR_AUDIT( INGEST_DATE, FILE_NAME, USER_EMAIL, USER_NAME, FLEET_USER_ID, CREATION_DATE, QUERY_CODE, QUERY_NAME, TARGETS_COUNT, QUERY_ID, ACTION ) as ( select snapshot_date as ingest_date , file_name::string as file_name , raw['actor_email']::string as user_email , raw['actor_full_name']::string as user_name , raw['actor_id']::string as fleet_user_id , raw['created_at']::string::date as creation_date , raw['details']['query_sql']::string as query_code , raw['details']['query_name']::string as query_name , raw['details']['targets_count']::integer as targets_count , raw['id']::integer as query_id , raw['type']::string as action from DB.SCHEMA.FLEET_OSQUERY_AUDIT ) ; The above data model is a real fantastic way to keep track of what actions users of FleetDM are performing and what objects in the application are being modified or accessed. This is the type of model you can hand off to your Threat Detection and Incident Response teams to monitor for anomalies. select * from DB.SCHEMA.OSQR_AUDIT where contains(query_code, 'curl') The above query against the audit data model will show results of any person who has used the curl table in the FleetDM product. This is really thorough, and if a bad actor were to gain access to an IT tool, they might try to start using tools like curl to deploy malicious software across your fleet. Most IT/Ops tools I have personally experienced in my career, do not have this level of auditing. Also, the power of Snowflake makes this such an easy query to write. There are many actions you can track like, but not limited to: User’s logging in User’s being created User’s editing or executing queries Tables being accessed for live queries User’s fail to log in User’s role gets changed What the actual query code being ran is So, this is really great for monitoring and auditing access, editing all objects in FleetDM, tracking who ran what live queries, and much more. Other vendors should really take note and build auditing trails this good. ","date":"2024-01-25","objectID":"/posts/more-osquery-data-modeling-snowflake/:0:2","tags":["macOS","IT Tools","osquery","snowflake"],"title":"More Osquery Data Modeling in Snowflake","uri":"/posts/more-osquery-data-modeling-snowflake/"},{"categories":["data","osquery","snowflake","data modeling"],"content":"Process Events Table The es_process_events table contains process events from the EndpointSecurity API in macOS. This table requires additional configurations to be set in the YAML config file for osquery. This is a table that also can sometimes take up more compute if you get heavy-handed with it. You can use this table for lots of different reasons like but not limited to: What processes are running, tracking how long processes run, and extended metadata about processes that are running on your macOS endpoint. This can be very useful for both IT/Ops and Security Teams on what processes are running to later join to resource usage query results to get compute costs. Security teams can also use this for a plethora of security monitoring across your fleet. This table is a bit “crunchy” to model in Snowflake due to the JSON data passing blank strings for things that should be Null and also an integer data type. Here is a sample of one of my query results, which is actually a 19,908 line JSON doc. Yes, each of the query results for each endpoint can be about 20,000 lines of JSON. This may seem like a lot, but it is really not much in terms of what Snowflake can handle. { \"cdhash\": \"1c06443abb6d077d15600fe3e5b8856345228707\", \"child_pid\": \"73831\", \"cmdline\": \"\", \"cmdline_count\": \"0\", \"codesigning_flags\": \"\", \"cwd\": \"/\", \"egid\": \"20\", \"env\": \"\", \"env_count\": \"0\", \"euid\": \"20\", \"event_type\": \"fork\", \"exit_code\": \"\", \"gid\": \"20\", \"global_seq_num\": \"17966\", \"original_parent\": \"1\", \"parent\": \"1\", \"path\": \"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome\", \"pid\": \"11559\", \"platform_binary\": \"0\", \"seq_num\": \"3887\", \"signing_id\": \"com.google.Chrome\", \"team_id\": \"EQHXZ8M8AV\", \"time\": \"1699986421\", \"uid\": \"501\", \"username\": \"batman\", \"version\": \"7\" }, In the above example you can see a key for exit_code, which according to the schema and docs should be an integer, but in the actual JSON results it is a blank string. This can be confusing and Snowflake has perfect ways to work around data that is imperfect. I would expect this to return a Null and not a blank string of \"\" since the schema states this should be an integer data type. create or replace view DB.SCHEMA.OSQR_MACOS_PROCESS_EVENTS_V( HOSTNAME, HOST_UUID, SERIAL_NUMBER, CDHASH, CHILD_PID, CMDLINE, CMDLINE_COUNT, CODESIGNING_FLAGS, CWD, EGID, ENV, ENV_COUNT, EUID, EVENT_TYPE, EXIT_CODE, GID, GLOBAL_SEQ_NUM, ORIGINAL_PARENT, PARENT, PATH, PID, PLATFORM_BINARY, SEQ_NUM, SIGNING_ID, TEAM_ID, TIME, UID, USERNAME, VERSION ) as ( select host_name as hostname , host_uuid as host_uuid , serial_number as serial_number , f.value['cdhash']::string as cdhash , NULLIF(f.value['child_pid']::string, '')::number as child_pid , f.value['cmdline']::string as cmdline , NULLIF(f.value['cmdline_count']::string, '')::number as cmdline_count , f.value['codesigning_flags']::string as codesigning_flags , f.value['cwd']::string as cwd , NULLIF(f.value['egid']::string, '')::number as egid , f.value['env']::string as env , NULLIF(f.value['env_count']::string, '')::number as env_count , NULLIF(f.value['euid']::string, '')::number as euid , f.value['event_type']::string as event_type , NULLIF(f.value['exit_code']::string, '')::number as exit_code , NULLIF(f.value['gid']::string, '')::number as gid , NULLIF(f.value['global_seq_num']::string, '')::number as global_seq_num , NULLIF(f.value['original_parent']::string, '')::number as original_parent , NULLIF(f.value['parent']::string, '')::number as parent , f.value['path']::string as path , NULLIF(f.value['pid']::string, '')::number as pid , NULLIF(f.value['platform_binary']::string, '')::number as platform_binary , NULLIF(f.value['seq_num']::string, '')::number as seq_num , f.value['signing_id']::string as signing_id , f.value['team_id']::string as team_id , NULLIF(f.value['time']::string, '')::number as time , NULLIF(f.value['uid']::string, '')::number as uid , f.value['username']::string as username , f.value['version']::string as version from DB.SCHEMA.F","date":"2024-01-25","objectID":"/posts/more-osquery-data-modeling-snowflake/:0:3","tags":["macOS","IT Tools","osquery","snowflake"],"title":"More Osquery Data Modeling in Snowflake","uri":"/posts/more-osquery-data-modeling-snowflake/"},{"categories":["data","osquery","snowflake","data modeling"],"content":"Third Party Application Data A very common data collection from osquery is the installed 3rd party apps on an end user device. This of course has many functions data wise. It is good for observing how fast you patch apps, what versions of apps are out in your fleet, what type of apps are users installing (helps understand their needs), and then the big one, collecting all the out of date and vulnerable versions of apps. Patch quickly, Patch often is a phrase you hear nowadays, and it is due to the amount of vulnerabilities dev shops are finding. With the help of bug bounty programs some efforts are also now crowd-sourced to the public. Thus, you can patch a single app multiple times in a month to fix known security issues. Model code: create or replace view DB.SCHEMA.OSQR_MAC_INSTALLED_APPS_HISTORICAL_V( HOSTNAME, SERIAL_NUMBER, APP_NAME, INGEST_DATE_STAMP, EXECUTABLE_NAME, BUNDLE_ID, SHORT_VERSION, LONG_VERSION, LAST_TIME_USED_EPOCH, INSTALLATION_PATH, ARCH ) as ( select host_name as hostname , serial_number as serial_number , f.value['name']::string as app_name , event_timestamp as ingest_date_stamp , f.value['bundle_executable']::string as executable_name , f.value['bundle_identifier']::string as bundle_id , f.value['bundle_short_version']::string as short_version , f.value['bundle_version']::string as long_version , f.value['last_opened_time']::string as last_time_used_epoch , f.value['path']::string as installation_path , f.value['bundle_package_type']::string as arch from DB.SCHEMA.FLEET_OSQUERY_RESULTS , lateral flatten(input =\u003e snapshot) as f where name = 'pack/Global/ingest-get-all-macos-apps' ) ; This is a pretty standard data model and nothing really out of the ordinary with it. I am using one of my favorite features of Snowflake, which is flatten and I use this feature so much. When we set up key clustering on our data ingest, we also put the snapshot key into its own column. This contains the actual query results and is a series of nested data sets under the snapshot key. This makes parsing dictionary-like-data easy-peasy. For context, the snapshot column is still variant data, and it is the remaining JSON of the query results. So, when you are modeling data during your ingest process to add things like key clustering, you can still just dump parts of the JSON file into its own column. This is the output of select * from apps; in osquery, so we have basically a giant historical data set of every item in the osquery apps table. This is very useful for many reasons, but really trending the data over time to observe how often certain apps patch or do not patch, how quickly they patch, what apps are constantly showing up but not in your automation pipeline yet? There are also some findings in here for security for things like a PUP for example. It also can tell you how often apps are opened. This is super useful for tracking usage of licensed software. If you have licensed software that can cost $100s of dollars or more, and it is not being actively used, and historically not used, you can now trend that data to provide tons of cost savings for the organization. Consider this query: select * from DB.SCHEMA.OSQR_MAC_INSTALLED_APPS_HISTORICAL_V qualify 1 = row_number() over (partition by serial_number, app_name order by ingest_date_stamp desc) ; I am now using my other favorite feature of Snowflake, the qualify window function filter. This will return the latest data and only the latest data per an app per each serial number of devices in your fleet. This query was a bigger one, and it took about 1 minute to run and returned over half a million rows of data. I don’t find it shocking and in fact I would have sort of assumed we would have closer to a million unique apps across the fleet, but we are big SaaS users, so it tracks. I took the opportunity to look at an app we all know and love, Adobe Photoshop. As you can see from the screenshot above we are able to get the last used timestamp and can do all sorts of things wi","date":"2024-01-25","objectID":"/posts/more-osquery-data-modeling-snowflake/:0:4","tags":["macOS","IT Tools","osquery","snowflake"],"title":"More Osquery Data Modeling in Snowflake","uri":"/posts/more-osquery-data-modeling-snowflake/"},{"categories":["data","osquery","snowflake","data modeling"],"content":"Until Next Time Tools like osquery are not always straight forward to use and get returns off of. The Snowflake Data Platform really solves a big problem with getting the biggest returns from your osquery data. FleetDM makes managing osquery a breeze and my experiences with them have been great. Data modeling isn’t some mystic art and IT/Ops shops should really start thinking their data strategy if they don’t already have one. This type of data is paradigm shifting, and words really cannot describe how great it is. ","date":"2024-01-25","objectID":"/posts/more-osquery-data-modeling-snowflake/:0:5","tags":["macOS","IT Tools","osquery","snowflake"],"title":"More Osquery Data Modeling in Snowflake","uri":"/posts/more-osquery-data-modeling-snowflake/"},{"categories":["data","osquery","snowflake","data modeling"],"content":"I recently told my dog, Booms, all about how you can use osquery data in Snowflake. Needless to say, he was quite impressed with all the use cases and in-depth data you can model about your fleet of laptops using osquery data in Snowflake. Booms came to the office with me, and he even got a bit of company swag! As we spent the day working I told him all about all the work my team was doing with osquery. Anyone can see in this pic of him, he is very impressed with the speed and scale one can leverage data from osquery in Snowflake ","date":"2023-08-31","objectID":"/posts/osquery-data-in-snowflake/:0:0","tags":["macOS","IT Tools","osquery","snowflake"],"title":"Working with osquery Data in Snowflake","uri":"/posts/osquery-data-in-snowflake/"},{"categories":["data","osquery","snowflake","data modeling"],"content":"Quick Recap A while ago I posted a blog about osquery, FleetDM, and Snowflake. You can find my old blog post, here We are now using this tech stack even more today, and while we have been working on many other different projects we have made progress with our osquery story. If you are unfamiliar with FleetDM, osquery or Snowflake I suggest you read the previous blog post. The basics are covered there. Our setup is FleetDM managing osquery, integrated with Snowflake. Leveraging AWS cloud technologies like S3 storage, and Kinesis Firehose to stream data in near-real-time. With Snowflake’s Snow Pipe to rapidly ingest data, so we can get data as fast as we choose to. ","date":"2023-08-31","objectID":"/posts/osquery-data-in-snowflake/:0:1","tags":["macOS","IT Tools","osquery","snowflake"],"title":"Working with osquery Data in Snowflake","uri":"/posts/osquery-data-in-snowflake/"},{"categories":["data","osquery","snowflake","data modeling"],"content":"Some Problem Statements People may be curious as to why we want to deploy more tools to collect data, when there are things like MDM that most IT Operations shops already have. The simple answer is that osquery is just way better at collecting data, it is cross-platform, and it gives us much better control over our data pipelines. So, here are some things we are initially trying to solve: Proactive monitoring Performance monitoring Security and Compliance data Extended software gathering Bridging gaps in current data collection features our or tech stack We have many more use cases, but we will continue with the above. One of the problems IT has always faced, is the ability to predict when a device is going to have problems and proactively help or fix/replace an end user device. Security will want all the sweet data that osquery can provide, you just have to inform them you are able to leverage osquery, and they will have asks. Performance data is tough to track on end user devices. Unlike server infrastructure, end users can do, well anything they desire to do. Like run tons of apps, configure their technology how they see fit, and develop workflows that you have zero insights to. Your end user’s workflows may be very foreign to you as well. IT folks may have no idea what a marketing or sales staff may do. Professional services folks are a great x-factor to mention as well. They do all sorts of things customers ask them to do. So, tracking an end user device’s performance can be a great way to tell if you are over/under spec’ing the device, or if certain applications on the device are potentially not optimally configured, etc. However, none of this should stop you from trying to understand what your coworkers do day to day, and how you could potentially help with data. Booms felt skeptical that we could pull this off at scale. I told him we would never know if we didn’t try ","date":"2023-08-31","objectID":"/posts/osquery-data-in-snowflake/:0:2","tags":["macOS","IT Tools","osquery","snowflake"],"title":"Working with osquery Data in Snowflake","uri":"/posts/osquery-data-in-snowflake/"},{"categories":["data","osquery","snowflake","data modeling"],"content":"A Decent Start Before I dive into what we have done, and how we have leveraged the data, I would like to mention that we have not turned on any events based tables in osquery yet. So, the performance data we have is a snapshot in time, of the most expensive compute processes since boot time. The caveat with this data is that it doesn’t tell us how long these processes ran. If a process runs at 900% cpu usage from a 10core CPU system for 5 seconds, is likely a different story than a process running at 500% CPU usage for 25 minutes on that same system. As well as a process taking up 5 cores on a 6 CPU core system, versus 5 cores on a 10 CPU core system. While we do not have the full on events’ data yet in osquery, we do have something to start with. Event tables in osquery are a bigger compute cost to leverage. We plan on exploring these tables, but we take UX of our laptops as a high priority. So, the enabling of the events’ data in osquery will be a slow roll-out while we monitor any potential impacts we could potentially encounter. I would also like to point out, tons and tons of effort has gone into making osquery much more performant over the years. So, if you have not tried a modern version of osquery I would recommend that you give it another go! ","date":"2023-08-31","objectID":"/posts/osquery-data-in-snowflake/:0:3","tags":["macOS","IT Tools","osquery","snowflake"],"title":"Working with osquery Data in Snowflake","uri":"/posts/osquery-data-in-snowflake/"},{"categories":["data","osquery","snowflake","data modeling"],"content":"Let’s look at some data! Consider this query in osquery: SELECT pid , uid , name , ci.model , ci.number_of_cores , ci.logical_processors , ci.max_clock_speed , ci.number_of_efficiency_cores , ci.number_of_performance_cores , ROUND(( (user_time + system_time) / (cpu_time.tsb - cpu_time.itsb) ) * 100, 2) AS percentage FROM processes,( SELECT ( SUM(user) + SUM(nice) + SUM(system) + SUM(idle) * 1.0) AS tsb, SUM(COALESCE(idle, 0)) + SUM(COALESCE(iowait, 0)) AS itsb FROM cpu_time ) AS cpu_time cross join cpu_info as ci ORDER BY user_time+system_time DESC LIMIT 20; The above query cross joins the processes table and the cpu_info table to get some basic CPU usage and context of the CPU data. I found this query from a blog found here. I also found that blog post pretty informative around some performance monitoring concepts with osquery. So, we loaded this query up in FleetDM and let it run to see what we got back. { \"action\": \"snapshot\", \"calendarTime\": \"Wed Aug 30 17:31:16 2023 UTC\", \"counter\": 0, \"decorations\": { \"host_uuid\": \"UUID string\", \"hostname\": \"the-host-name\", \"serial_number\": \"the-serial-number\" }, \"epoch\": 0, \"hostIdentifier\": \"UUID-string\", \"name\": \"pack/Global/ingest-macos-top-20-processes\", \"numerics\": false, \"snapshot\": [ { \"logical_processors\": \"10\", \"max_clock_speed\": \"3228\", \"model\": \"Apple M1 Max\", \"name\": \"WindowServer\", \"number_of_cores\": \"10\", \"number_of_efficiency_cores\": \"2\", \"number_of_performance_cores\": \"8\", \"percentage\": \"204.99000000000001\", \"pid\": \"565\", \"uid\": \"0\" }, { \"logical_processors\": \"10\", \"max_clock_speed\": \"3228\", \"model\": \"Apple M1 Max\", \"name\": \"kernel_task\", \"number_of_cores\": \"10\", \"number_of_efficiency_cores\": \"2\", \"number_of_performance_cores\": \"8\", \"percentage\": \"123.26000000000001\", \"pid\": \"0\", \"uid\": \"0\" }, { \"logical_processors\": \"10\", \"max_clock_speed\": \"3228\", \"model\": \"Apple M1 Max\", \"name\": \"\u003credacted\u003e\", \"number_of_cores\": \"10\", \"number_of_efficiency_cores\": \"2\", \"number_of_performance_cores\": \"8\", \"percentage\": \"62.219999999999999\", \"pid\": \"12800\", \"uid\": \"0\" }, { \"logical_processors\": \"10\", \"max_clock_speed\": \"3228\", \"model\": \"Apple M1 Max\", \"name\": \"AdobeReader\", \"number_of_cores\": \"10\", \"number_of_efficiency_cores\": \"2\", \"number_of_performance_cores\": \"8\", \"percentage\": \"48.670000000000002\", \"pid\": \"78967\", \"uid\": \"501\" }, { \"logical_processors\": \"10\", \"max_clock_speed\": \"3228\", \"model\": \"Apple M1 Max\", \"name\": \"Google Chrome\", \"number_of_cores\": \"10\", \"number_of_efficiency_cores\": \"2\", \"number_of_performance_cores\": \"8\", \"percentage\": \"32.32\", \"pid\": \"1667\", \"uid\": \"501\" }, { \"logical_processors\": \"10\", \"max_clock_speed\": \"3228\", \"model\": \"Apple M1 Max\", \"name\": \"Google Chrome Helper (GPU)\", \"number_of_cores\": \"10\", \"number_of_efficiency_cores\": \"2\", \"number_of_performance_cores\": \"8\", \"percentage\": \"28.050000000000001\", \"pid\": \"1767\", \"uid\": \"501\" }, { \"logical_processors\": \"10\", \"max_clock_speed\": \"3228\", \"model\": \"Apple M1 Max\", \"name\": \"redacted\", \"number_of_cores\": \"10\", \"number_of_efficiency_cores\": \"2\", \"number_of_performance_cores\": \"8\", \"percentage\": \"21.800000000000001\", \"pid\": \"70254\", \"uid\": \"0\" }, { \"logical_processors\": \"10\", \"max_clock_speed\": \"3228\", \"model\": \"Apple M1 Max\", \"name\": \"launchd\", \"number_of_cores\": \"10\", \"number_of_efficiency_cores\": \"2\", \"number_of_performance_cores\": \"8\", \"percentage\": \"19.34\", \"pid\": \"1\", \"uid\": \"0\" }, { \"logical_processors\": \"10\", \"max_clock_speed\": \"3228\", \"model\": \"Apple M1 Max\", \"name\": \"distnoted\", \"number_of_cores\": \"10\", \"number_of_efficiency_cores\": \"2\", \"number_of_performance_cores\": \"8\", \"percentage\": \"16.460000000000001\", \"pid\": \"867\", \"uid\": \"501\" }, { \"logical_processors\": \"10\", \"max_clock_speed\": \"3228\", \"model\": \"Apple M1 Max\", \"name\": \"Google Chrome Helper (Renderer)\", \"number_of_cores\": \"10\", \"number_of_efficiency_cores\": \"2\", \"number_of_performance_cores\": \"8\", \"percentage\": \"15.69\", \"pid\": \"40686\", \"uid\": \"501\" }, { \"logical_processors\": \"10\", \"max_clock_speed\": \"3228\", \"model\": \"Apple M1 Max\", \"name\": \"blu","date":"2023-08-31","objectID":"/posts/osquery-data-in-snowflake/:0:4","tags":["macOS","IT Tools","osquery","snowflake"],"title":"Working with osquery Data in Snowflake","uri":"/posts/osquery-data-in-snowflake/"},{"categories":["data","osquery","snowflake","data modeling"],"content":"Closing thoughts We have more data coming in I did not cover yet. Things like browser extensions, application versions, systems data, how much memory is being consumed by what process, and more. We aren’t as tenured as some Orgs with osquery, but better late than never as they say! This is the beginning of this journey, and I would bet more blog posts will come from this. If you got this far, thanks for reading! Here is a bonus pic of Booms when we took him to see some mountains along the Pacific Ocean. ","date":"2023-08-31","objectID":"/posts/osquery-data-in-snowflake/:0:5","tags":["macOS","IT Tools","osquery","snowflake"],"title":"Working with osquery Data in Snowflake","uri":"/posts/osquery-data-in-snowflake/"},{"categories":["data cloud","asset inventory","servicenow","snowflake"],"content":"Inventory Asset data is extremely important to any organization. It is also some of the most difficult data to keep accurate and up-to-date. We have been shipping ServiceNow data to Snowflake here for a good while. Over the years the asset data has proven time after time to be extremely valuable. It helps filter out tons of noise and allows one to focus on specific asset data at any given time. Just by creating some simple queries and joining ServiceNow data with other data sources. I recently created a new view in Snowflake to help us better filter data down to more specifics. ","date":"2023-03-28","objectID":"/posts/servicenow-asset-data-in-snowflake/:0:0","tags":["asset data","endpoints","data insights"],"title":"Servicenow Asset Data in Snowflake","uri":"/posts/servicenow-asset-data-in-snowflake/"},{"categories":["data cloud","asset inventory","servicenow","snowflake"],"content":"Benefits from Data Upstream ServiceNow is a platform, which can host many applications within its ecosystem. You can have all sorts of workflows for different teams, ticketing systems for various departments, knowledge base articles, and of course asset inventory. When the ServiceNow developers or admins create new features or integrations within the application upstream, you can also have that data downstream. For example, I engaged with our ServiceNow team not too long ago to integrate with our HR system so if anyone was on a leave of absence we could mark their asset as LOA in the asset tracker in ServiceNow. This allows teams downstream consuming this data in Snowflake to easily filter all leave of absence devices into their own bucket. As a general philosophy, I tend to never get rid of data, but rather filter it into contextual buckets where it makes the most sense. Another thing we also got to benefit from, is that when someone marks an asset in ServiceNow as a VIP asset, we know it belongs to a VIP end user. These are executives, department heads, VPs, and other important people that might need more of an executive IT support model vs a standard IT support model. It also allows us to track VIP computers specifically, so we can ensure they have the latest security patches. These type of enhancements that your ServiceNow developers and admins can build within ServiceNow allow many other teams to benefit from this data downstream. ","date":"2023-03-28","objectID":"/posts/servicenow-asset-data-in-snowflake/:0:1","tags":["asset data","endpoints","data insights"],"title":"Servicenow Asset Data in Snowflake","uri":"/posts/servicenow-asset-data-in-snowflake/"},{"categories":["data cloud","asset inventory","servicenow","snowflake"],"content":"Modeling the Data So, now that I have this new data added upstream by my ServiceNow team, I can easily just consume it downstream in Snowflake. Since we already have data pipelines coming in from the CMDB tables, when the person that developed these enhancements moved the changes to prod, the next time the data ingested it showed up in Snowflake. Now, I wanted to take advantage of this, so I created a view joining these tables together into a single spot within Snowflake we can consume this data from. example SQL code: create or replace view DB.SCHEMA.SERVICENOW_ASSETS_V( DISPLAY_NAME , SERIAL_NUMBER , ASSET_TAG , USER_NAME , EMAIL , INSTALL_STATUS , RETIRED , IS_DELETED , SUBSTATUS , MODEL_CATEGORY , VIP , ON_LEAVE ) as ( select a.display_name , SERIAL_NUMBER , asset_tag , user_name , email , install_status , Retired , IFF(documentkey is NULL, false, true) as is_deleted --marking deleted if it exists in the deleted table , substatus , d.name as model_category , b.vip as vip , e.u_loa as on_leave from (Select assigned_to, assigned_to_value, display_name, SERIAL_NUMBER, asset_tag, install_status, substatus, Retired, sys_id, model_category from DB.SCHEMA.ALM_ASSET)a --listing of assets left outer join (select sys_id, last_login, user_name, name, email, vip from DB.SCHEMA.SYS_USER)b on a.assigned_to_value = b.sys_id left outer join (select sys_id, u_loa from DB.SCHEMA.ALM_HARDWARE) as e on a.sys_id = e.sys_id left outer join DB.SCHEMA.SYS_AUDIT_DELETE c --joining the deleted table to check deletion status on c.documentkey = a.sys_id left outer join DB.SCHEMA.CMDB_MODEL_CATEGORY as d on a.model_category = d.sys_id where not is_deleted --filtering non deleted ); Things to Note CMDB Tables in ServiceNow have a unique ID referred to as a sys_id that can act as a primary key When data modeling, try to keep the view specific and avoid joining every data source into a single monolithic view When updating a view, you may want to use copy grants Always ask your local domain experts for guidance, don’t assume what data means ServiceNow is a massive product, with 1000s of database tables, and it takes a lot of time and effort to be an expert on it. I am very far from an expert, so I engaged with my internal experts to help understand and model this data. They were able to help me understand what the data means and how to best model it. ","date":"2023-03-28","objectID":"/posts/servicenow-asset-data-in-snowflake/:0:2","tags":["asset data","endpoints","data insights"],"title":"Servicenow Asset Data in Snowflake","uri":"/posts/servicenow-asset-data-in-snowflake/"},{"categories":["data cloud","asset inventory","servicenow","snowflake"],"content":"Leveraging thew New Asset Data We have an IT Remediation project, where we meet once a week and review our data and dashboards. A lot of things have surfaced from this meeting. We go through the data and dive into it, validating that is correct, and pointing out where we can enhance it. Our colleagues in the EU asked us to add geolocation to them, so they could easily identify assets in their region they supported that were out of compliance as an example. This shifted us into be proactive, but also we were validating the data was good. Before our support staff were manually filtering out device location to divide and conquer IT support projects. Now that data is joined into these dashboards. I feel this important because it will start a culture of looking at data, checking if the data is accurate, identifying gaps and enhancements, and it really helps with cross team collaboration. We recently heavily campaigned our fleet to all update to the latest version of Ventura. We combined the geo data, MDM data and all those upstream data enhancements we added to ServiceNow asset data. In the above dashboard we first started out with just webhook data from our MDM (jamf pro) and were tracking active macOS devices current OS. This was an okay place to start. However, once we were able to add the ServiceNow Asset data we were able to really gain much more insight much faster. Then over time we enhanced this dashboard with the following asset data from ServiceNow: Asset function: primary, secondary, testing, conference room, etc Asset status: in-use, out for repairs, in-stock, retired, etc Integration data: leave of absence, vip users Asset assignment: what human has what device Since ServiceNow is the source of truth for asset assignments, this allowed us to enrich the data even further and use the email address of the assigned human and join that to HR employee directory data. This is how we were able to look up where the employee worked out of, and thus gave us the ability to easily filter by geolocation so local employees could better support their region of users. Imagine being a manager or an individual contributor(IC) for an international company and be able to easily filter out the regions you support, and review those specific device’s data by just checking a dashboard? Managers can get high level overviews and ICs can proactively identify technical issues and set plans to address them. ","date":"2023-03-28","objectID":"/posts/servicenow-asset-data-in-snowflake/:0:3","tags":["asset data","endpoints","data insights"],"title":"Servicenow Asset Data in Snowflake","uri":"/posts/servicenow-asset-data-in-snowflake/"},{"categories":["data cloud","asset inventory","servicenow","snowflake"],"content":"Same Old Problems Many years ago I was scanning thousands of Macbook Air laptop serial numbers from a big laptop deployment my team and I were doing at a previous job of mine. We quickly noticed that the barcode we scanned would append the letter S as the beginning of the serial number. We programmed the scanners to ignore the first character and that fixed it. Well, this is one of those human error problems. Humans might forget to program their scanners sometimes. So, we can use data in Snowflake to audit and fix our asset data. Quick query to get serial numbers that start with S select * from DB.SCHEMA.SERVICENOW_ASSETS_V where startswith(serial_number, 'S') and display_name ilike '%apple%' ; Another classic problem is how do you monitor your asset data to ensure you did not make a human error and forget to add assets to your asset tracker. SELECT JI.WEBHOOK:eventTimestamp::varchar::TIMESTAMP_LTZ as last_contact , JI.EVENT:computer:serialNumber::varchar as j_serial , JI.EVENT:computer.emailAddress::varchar as EMAIL , JI.EVENT:computer.department::varchar as DEPARTMENT from \"DB\".\"SCHEMA\".\"JAMF_CHECKIN\" as JI WHERE TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(days, -30, current_timestamp()) QUALIFY 1 = ROW_NUMBER() OVER (PARTITION BY JI.EVENT:computer:serialNumber::varchar ORDER BY last_contact DESC) and not exists( select upper(SN.SERIAL_NUMBER)::varchar as sn_serial from DB.SCHEMA.SERVICENOW_ASSETS_V as SN where sn_serial = JI.EVENT:computer:serialNumber::varchar ) ; In efforts to ensure our asset data is good and accurate, we can simply use not exists in Snowflake to compare serial numbers of actively checking in devices that do not have valid asset records in ServiceNow. This example is a webhook event from our MDM, and we check if the serial number of that device exists. That is it, that is how simple this check is. ","date":"2023-03-28","objectID":"/posts/servicenow-asset-data-in-snowflake/:0:4","tags":["asset data","endpoints","data insights"],"title":"Servicenow Asset Data in Snowflake","uri":"/posts/servicenow-asset-data-in-snowflake/"},{"categories":["data cloud","asset inventory","servicenow","snowflake"],"content":"Asset Data is Important Asset data accuracy is something that is so tough to keep up with. Assets change states when they go out for repairs, a hardware refresh, get returned, as each one of those states follows a process that involves a human doing work. Asset data is also some very important data that really needs to be always accurate. Having your hardware asset data in a product like Snowflake gives you avenues to actually actively test and ensure your asset data is accurate and audited. We find mistakes all the time, and we find them the second the data is ingested into Snowflake. The data it provides can be joined to other data sources and build insights and metrics off of it all. This data is used across many teams for many purposes. Security will want to vary their priorities based on people and teams, and with asset data they can do that. Teams that have more privileged access generally yield higher priority alerts. The asset data being accurate is a must for these things. Asset data gives us the person who the asset is assigned to, and we join that across tons of different data sets. We also were looking at secondary device data a lot over the past few years. There are times when someone might need another computer to perform work on in addition to their primary device. So we issue them another device to help them do their work. However, how do you know when the secondary device is no longer needed? If a secondary device is not needed anymore, and it is not online, it is most likely not getting patched either. Well, that asset data tells us the serial number and the human tied to that device, and we can join it to MDM or agent data. From here we can simply deduct if the device is not checking into our MDM servers it is not coming online, and likely not being used. If we detect a secondary device is not online greater than an arbitrary timeframe (30 or 60 days, etc) we can trigger workflows that will start a process to see if we can get that secondary system back. These are just some reasons and use cases I feel outline the importance of asset data and how having that in a tool like Snowflake can really help boost accuracy of your asset data. Thanks for reading! ","date":"2023-03-28","objectID":"/posts/servicenow-asset-data-in-snowflake/:0:5","tags":["asset data","endpoints","data insights"],"title":"Servicenow Asset Data in Snowflake","uri":"/posts/servicenow-asset-data-in-snowflake/"},{"categories":["automation","patching","software deployment"],"content":"Hello Everyone, and it has been a while since my last blog post. I have been busy with work and with some other things outside of work as well. However, we did finally release Munki to our fleet last summer. This was a milestone project for us, and while we aren’t the first Org to do this, I wanted to share with the community how and why we did this. Everything I will write here has probably already been done by many other Orgs that currently use Munki, and we aren’t really doing much that is radically different. So, if you hae deployed a server-less Munki setup before, this is probably not going to be new to you. That being said, server-less is a pretty key word for our vision here. ","date":"2023-01-22","objectID":"/posts/diversifying-your-tools-munki/:0:0","tags":["munki","open source","IT Tools"],"title":"Diversifying Your IT Tools: Integrating Munki","uri":"/posts/diversifying-your-tools-munki/"},{"categories":["automation","patching","software deployment"],"content":"How it all started Over the years I have been privileged enough to peek behind many curtains around the globe when I worked as a professional services engineer in vendor space. This has given me a lot of perspective on how many different IT shops, all try to solve very similar problems. One of those common problems is patching third party applications. We all know vulnerabilities just keep coming and never stop, therefore an endless amount of patches and updates for apps also come out as well. When I swapped over to my current job in 2019 I built this as our solution. It leveraged autoPKG and another tool JSS Importer which is now sunset. Jamf is also changing their classic APIs which will break JSS Importer. With these changes on the horizon we had to make a decision, to either migrate everything to a new integration like jamf upload or look at using a new tool. One thing was definitely certain and that was that we would not be manually packaging up and deploying software updates. It had to be automated, no other options allowed. ","date":"2023-01-22","objectID":"/posts/diversifying-your-tools-munki/:0:1","tags":["munki","open source","IT Tools"],"title":"Diversifying Your IT Tools: Integrating Munki","uri":"/posts/diversifying-your-tools-munki/"},{"categories":["automation","patching","software deployment"],"content":"Building the Requirements When you sit down to design any system at all, it is always a good idea to start with a best-case-scenario. List the things you want to accomplish, and list the things you want to avoid. I have found that as my tech career has advanced I have observed many things that don’t work or scale. I have also done many things myself that don’t work or scale. This is fine, normal, and acceptable. Many times you will not know what to do or how to scale unless you start somewhere. Start trying things out and see what results you get. Trial and error is a part of the scientific process after all. Always bring your experiences with you, but also keep an open mind. Here are the things I came up with before I even looked at choosing the proper tools, or path we were going to take. Has to be server-less (content delivery model) Everything has to be in version control We need CI/CD Everything needs to be in code All development should be done locally Our dev environment should be the same for both local and CI/CD environments Any Mac should easily become the build server if need be Everything needs peer review Avoid large efforts in API integration The tools should do a lot of the basic work for us (versus us writing tons of scripts) Easily able to integrate this into any MDM ","date":"2023-01-22","objectID":"/posts/diversifying-your-tools-munki/:0:2","tags":["munki","open source","IT Tools"],"title":"Diversifying Your IT Tools: Integrating Munki","uri":"/posts/diversifying-your-tools-munki/"},{"categories":["automation","patching","software deployment"],"content":"Socializing the Requirements Why server-less? In previous jobs I have been a Linux/Server administrator. This meant I had to deal with vulnerability scans, patching, downtime, upgrades, monitoring, on-call rotations, and all the other things that no one really likes to deal with when it comes to server and on prem infrastructure. So, then to solve this problem, I just get rid of the servers. You cannot vuln scan or patch a S3 bucket or CloundFront CDN that is hosting your software packages. I wanted to just go to a content delivery system and serve up content on a CDN and have the client just fetch it when it needed to. I based this off experiences across many past jobs where I had these extra responsibilities of managing all of our server infrastructure. I wanted to shift us into an engineering focused and automation first approach, over some legacy IT Operations processes that are out there. So, I socialized these ideas with my leadership and peers. I wanted to adopt more DevOps practices, and steer away from things like manual QA or manual paper work to move something like deploying an app or an app update through an end-to-end process. So, when we expanded our team I went after DevOps engineers and not Mac Admins. I wanted a more diverse set of skills and experiences to help build a modern team. Luckily for me, my boss agreed. ","date":"2023-01-22","objectID":"/posts/diversifying-your-tools-munki/:0:3","tags":["munki","open source","IT Tools"],"title":"Diversifying Your IT Tools: Integrating Munki","uri":"/posts/diversifying-your-tools-munki/"},{"categories":["automation","patching","software deployment"],"content":"The Tools This is what we ended up with after some discussions and a few tests: Munki AutoPKG AWS Hosted Mac Mini S3 buckets CloudFront CDN GitHub actions and remote runners Build tools ","date":"2023-01-22","objectID":"/posts/diversifying-your-tools-munki/:0:4","tags":["munki","open source","IT Tools"],"title":"Diversifying Your IT Tools: Integrating Munki","uri":"/posts/diversifying-your-tools-munki/"},{"categories":["automation","patching","software deployment"],"content":"IT Development Cycle Here is a rough diagram of the flow we started with. Things can change too, and we are open to change, but I think we have a solid starting place to base us from. (starts left to right) Essentially, one does the following to deploy a new app and have Munki automatically update it: Clone the Munki + AutoPKG repo Run build tools Build tools will build the full local environment IT Engineer checks out a new branch and builds the AutoPKG recipe and tests it locally to ensure it works Build tools can build a fully functional Munki environment locally using Python’s simple HTTP server and a local repo Commit and push changes to branch to the remote repo Create pull request Another engineer is required to check the branch out, test it, and approve/deny the pull request Approved pull requests are merged back to the main development branch downstream we promote the main dev branch to stage, then later on stage to prod This way any Mac can easily be a development environment, AutoPKG server, and local Munki test environment as well. AutoPKG builds locally and outputs to a Munki repo in /Users/Shared which you can point your own Mac client to as well. Managed Software Center should see your Application. There is a simple Python build script our DevOps engineers wrote that runs the whole test end to end for a full test. This is the exact same environment we run on the Mini in AWS, with one minor exception. That minor difference is just that the Munki repo in /Users/Shared will get synchronized to S3 via S3 folder sync. This is done through GitHub actions and remote runners on the AWS Mini. We also use branch promotion, which means engineers can only ever commit and merge changes to the main development branch. The stage and prod branch do not allow for direct commits. This allows us to force a peer review system and when we automate promotion from development to stage, our early adopter group of end users gets the changes in stage first. Lastly, if no problems are reported or found, we then promote the stage branch to production. This is mostly possible due to us using version control. ","date":"2023-01-22","objectID":"/posts/diversifying-your-tools-munki/:0:5","tags":["munki","open source","IT Tools"],"title":"Diversifying Your IT Tools: Integrating Munki","uri":"/posts/diversifying-your-tools-munki/"},{"categories":["automation","patching","software deployment"],"content":"Why Not Just Use MDM? MDM solves problems, but typically only for MDM things or in MDM specific ways. This means it is oftentimes binary, absolute, and lacks options. Also, the UX of MDM options are also not great. One major example is that MDM solutions will often just bulldoze over running apps to patch them, and this is not a good UX, and could also result in data loss. I already had dealt with JSS Importer + AutoPKG + Jamf Pro, and it was a lot of engineering efforts for me to maintain. I also had to have a series of scripts and smart groups with in Jamf Pro to have the proper automation and UX I wanted to provide. Munki allowed me to retire all my API integration, all my bolted on top of the product Python code, and reduce my team’s actual labor by not having to deal with click-ops. MDM also has no real concept of version control, or anything meaningful in that space. It is a highly requested feature that has been around for many years and no MDM vendor really adopted anything around a version control system. If I wanted to enforce peer review, and build a decent and scalable testing process, version control was going to be a hard requirement for this. MDM solutions do offer up an API, but in my experience it typically requires lots of work and effort to build and maintain forever. Also, many MDMs are not API first, nor do they offer every an API endpoint for every feature in their app. Munki for us is server-less (no servers to patch), can be integrated into any MDM we purchase, leverages version control, reduced our labor exponentially, modernized our tech stack, CI/CD, and the out-of-the-box features allowed me to retire half a dozen Python scripts. So, we still very much want to use MDM, but the way we built Munki, we can take this with us if we ever decide to swap MDMs. ","date":"2023-01-22","objectID":"/posts/diversifying-your-tools-munki/:0:6","tags":["munki","open source","IT Tools"],"title":"Diversifying Your IT Tools: Integrating Munki","uri":"/posts/diversifying-your-tools-munki/"},{"categories":["automation","patching","software deployment"],"content":"Avoiding Vendor Lock-in Another pitfall I have both personally done, and witnessed many times in my career, is that avoiding diversifying your tools stack can put you in a bad position. Let’s say an event happens, like a very bad event. A data breach can easily scare your leadership into making you get rid of the tools that suffered the data breach. Commercial MDM tools are a big risk here of vendor lock-in, in my personal opinion, especially if you bolt tons of custom development on top of them. I get that an IT Org probably has their MDM in production, and it has been approved through all security processes. Painful security and procurement processes can deter IT teams from wanting to deploy multiple tools, and this can put you on a path to vendor lock yourself with a MDM. The way MDM is designed in its current state, it naturally puts you into a vendor lock situation. Changing MDMs is very painful with Apple products. Apple has never really built any migration paths, and they leave it up to the MDM vendor to build these. Well, what incentives do commercial MDM vendors have, to write code and features around off-boarding a customer to a competitor? I guess this was Apple’s idea (not sure what their idea is)? Basically, it isn’t going to happen because no commercial MDM vendor is going to spend dev cycles building a tool to help you migrate to one of their competitors. So, with a tool like Munki, that is completely separate from MDM, can slipstream into any MDM we choose. My team can suffer slightly less if we have to migrate to a new MDM. We also don’t have years of tech debt and custom dev piled on top of our MDM that makes migrating even more difficult. So, imagine if you build entire middleware systems of integration into your MDM, and the MDM ends up being not the best tool for your organization? What do you do? Do you suffer with the current MDM and the massive amount of tech debt, or do you decide to take the 2 years it will take to migrate to a new MDM? I have both worked in these environments, and observed these environments. From my experiences and hard lessons learned, I think diversifying your tools is the smart and agile thing any IT shop can do. It also positions the business/Organization in a much better place allowing for negotiation and migration of newer software tools in the future, if applicable. ","date":"2023-01-22","objectID":"/posts/diversifying-your-tools-munki/:0:7","tags":["munki","open source","IT Tools"],"title":"Diversifying Your IT Tools: Integrating Munki","uri":"/posts/diversifying-your-tools-munki/"},{"categories":["automation","patching","software deployment"],"content":"Tools to Explore Here goes some tools we use, or are in the process of using, that are independent of MDM. We can swap out each tool independently, for another tool in that space and we don’t have to tear our entire stack down. This approach allows my team to be modular and agile. It also allows my Organization some flexibility other Organizations may not have, because we can take specific tools out and replace them with other like tools. There is also feature overlap, so in a bad situation of having to sunset a tool, we will have at the very least, some coverage of features while we integrate the new tool. osquery Cross platform data collection, improved data collection GitHub Enterprise Version control, CI/CD via GitHub Actions Nudge Notification tool to notify end userse to upgrade macOS (huge fan of this tool) Gorilla Application state management and deployment tool for Windows that is similar to Munki Munki Application state management and deployment tool ","date":"2023-01-22","objectID":"/posts/diversifying-your-tools-munki/:0:8","tags":["munki","open source","IT Tools"],"title":"Diversifying Your IT Tools: Integrating Munki","uri":"/posts/diversifying-your-tools-munki/"},{"categories":["data","snowflake","IT Operations","asset"],"content":"Gain All the Insights You Never Had Asset and Inventory controls are difficult, and many Orgs spend tons of time and effort to try to solve problems around this very subject. In fact, they will hire humans, and spend lots of time and money to also get a hold of their inventory of assets. This problem is still very much difficult to solve, but the Data Cloud makes this problem at least solvable. I have been in tech for over 20 years, and I have even had jobs where inventory of assets was one of my job duties. I had a local database I built on a Linux desktop and a barcode scanner where I could manage my own little shop of computers and computer parts. I would manually enter data, I would scan barcodes, and I even had a sign-out sheet for people to sign for things they took, so I could balance my books later on. Systems like this do not scale, they are very error-prone, and they take so much labor. Oftentimes I had to play asset detective and dig through our IT software, the home-grown database that I had, and the various paperwork from other systems to get a feel for what our inventory actually was. Also, I was very new to Linux and databases at the time, and I highly doubt my little pet project was really any good at all. It was at least something I suppose. Times have changed since then, and that was back at my second IT job in the early 2000s when I didn’t have our current tech. We have made many advancements in technology and there have been many inventory and asset management suites out there, but there was one thing always missing from these tools. None of these tools have the ability to easily consume data form multiple data sources and allow IT professionals the ability to narrow in on their inventory and asset data. With the data cloud you can do this much easier today than ever before. While some asset and inventory systems do allow for integration, the amount of effort and labor is oftentimes very high and the return diminishing. What once took many labor hours of integrating APIs, building manual reports in software interfaces, trying to work with other teams on centralizing data, sometimes having to email spreadsheets, and a plethora of other things most people did not enjoy doing can now often be solved by writing a single query in the data cloud. Snowflake has focused on making data pipelines easy to manage in the cloud and ensure they are coming in as often as you would like. You can now get all the data from all the sources and leverage them all to get the most complete data around your assets. Things like this haven’t ever really existed to my knowledge until recently. ","date":"2022-08-08","objectID":"/posts/complete-asset-oversight-datacloud/:0:0","tags":["data cloud","analytics","asset management","metrics","big data"],"title":"Complete Asset Oversight with the Data Cloud","uri":"/posts/complete-asset-oversight-datacloud/"},{"categories":["data","snowflake","IT Operations","asset"],"content":"Fix Data Upstream to Improve Quality of Data Downstream I think it is a good idea to remind yourself, and your coworkers that everyone possesses the power to change things. If you don’t like something, you can put in effort to change it. If something is too complex, too labor-intensive, or just something in general that is not great, you can try to improve it. It just takes time, effort, and motivation to do so. If you don’t have great data around your assets in your downstream data warehouse, you can easily fix this by just adding that data into your asset software. If you have existing data pipelines and continuous data ingestion you should be able to add data to your CMDBs or whatever you use for inventory of assets, and that pipeline should pick it right up. I know this works, because I have done it more than one time. A few quick examples of data I needed to get a better picture of our asset states were the following, but not limited to: Adding Tester as an asset type. This means I could flag computers running beta software as testers and filter them accordingly Leave of Absence we have a checkbox for this. Now if an employee takes a leave of absence, we can mark their asset with this Adding Demo as an asset type when employees that need to take computers on road shows, conference booths, and similar These things actually impact numbers, and they can be filtered to meet your contextual needs. Sometimes we will see several points of a metric change by filtering these things out. Like for an example, you filter out tester and demo laptops from your workforce fleet, and it shows you a more accurate number of device compliance with devices that can access production systems. As well as filter out false positives when a computer running a beta version of macOS has a failed security agent on it. As your company grows these problems also scale right along with you. A 5% variance at 100 devices versus 10,000 devices are two very different numbers. A Simple query example could be: select count(*) as total , SUBSTATUS as status from \"DB\".\"SCHEMA\".\"SERVICENOW_ASSETS_V\" where SUBSTATUS in ( 'primary', 'secondary', 'Conference Room') group by SUBSTATUS union all select count(*) as total , 'Grand Total' as status from \"DB\".\"SCHEMA\".\"SERVICENOW_ASSETS_V\" where SUBSTATUS in ( 'primary', 'secondary', 'Conference Room') ; The three asset types I want to filter for in the above query are Primary, Secondary and Conference Room assets. Since we already spent time and effort in our Asset Management System, why not reap those benefits downstream in our data cloud platform? This query is counting all the assets by those three asset types and then giving me a grand total of all of them combined. Seems simple enough right? ","date":"2022-08-08","objectID":"/posts/complete-asset-oversight-datacloud/:0:1","tags":["data cloud","analytics","asset management","metrics","big data"],"title":"Complete Asset Oversight with the Data Cloud","uri":"/posts/complete-asset-oversight-datacloud/"},{"categories":["data","snowflake","IT Operations","asset"],"content":"Never Get Rid of Data, Just Filter It Data with context is very useful, data without context is not very useful. When you are able to ship all of your Inventory and Asset Data, your IT tools data, your security tools data, your HR data, and so on and so forth you can now leverage all of it to get precise contextual data. So, my philosophy is to never get rid of data, but instead filter it in specific buckets with the proper context. In a previous blog post I wrote about our first Nudge campaign, and it is a prime example of filtering, but still keeping all data. I will reuse my Nudge data to show a quick example of filtering. Observe this filtered data in the figure below: Every Org has employees on-boarding and off-boarding every week. The larger the Org the higher the number typically. I even worked at some Orgs that were so large we would see 25,000+ people join the company in a single month, and we could have somewhere in the ballpark of a similar amount of folks leaving. In the above screenshot I can now compare my asset data in the current state, but also apply HR data to it to see if folks are exiting the company. Assuming your Org has a process for exiting employees and that the asset is taken and securely stored for some sort of hold period that data may not be in the most current state as it typically involves humans interacting with other humans and manual data entry. This is why we always want the keep the all the data and apply context. You will notice just filtering out leaving employees the numbers are different. Another reason you keep the data is that Security Teams may want to track exiting employee systems for security events. ","date":"2022-08-08","objectID":"/posts/complete-asset-oversight-datacloud/:0:2","tags":["data cloud","analytics","asset management","metrics","big data"],"title":"Complete Asset Oversight with the Data Cloud","uri":"/posts/complete-asset-oversight-datacloud/"},{"categories":["data","snowflake","IT Operations","asset"],"content":"Inventory and Asset Data is Security Data Security is always a high concern with organizations, and inventory and asset data are in fact also security data. Having an actual grasp of your fleet is step one in understanding your attack surfaces and threat models. If you don’t know how many primary laptops are in your fleet can you accurately plan for securing them at scale? If you are not aware of how many secondary computers are out there how can you accurately assess risks with devices that may not be online to be patched? A secondary computer is likely to not be online every day, and only used when the secondary use case is needed. Out of all those systems you have at your Org, how many of the security agents are working on them? Does your IT and Security staff just install an agent and hope it works? Does your IT staff just write some scripts and call it a day? I hope not, and I once thought this was a near impossible task at previous jobs. I have always been fond of security and try to be security conscious when designing my systems. This has paid off, because now I base my health checks off of data in the data cloud. Before at previous jobs I would write a script, and maybe collect some metadata on the agents to see if they were running. These checks can catch some failures, but they also miss out on catching many other failures. Without diving-down a rabbit hole of edge cases, I can just point out that a security agent on an asset can be running, can have a process ID, can be taking up compute, but that does not mean it is submitting data to that agent’s cloud tenant. Both Security and IT teams can now leverage all the hard labor and efforts folks put into managing assets at your Org. This data is not just useful, it is fundamental for all data queries we run to get exact context of our fleet. Example query: with most_recent_agent_data as ( select CS.* , CS.SERIAL_NUMBER as CS_SERIAL , SN.* , WD.* , JI.jamf_last_contact FROM \"DB\".\"SCHEMA\".\"SECURITY_AGENT_CHECKINS\" as CS INNER JOIN ( SELECT SERIAL_NUMBER , EMAIL , SUBSTATUS FROM DB.SCHEMA.SERVICENOW_ASSETS_V WHERE SUBSTATUS in ('primary', 'secondary', 'Zoom Room') GROUP BY 1, 2, 3 ) as SN on CS.serial_number = SN.SERIAL_NUMBER INNER JOIN ( select event:serialNumber::string as ji_serial , WEBHOOK:eventTimestamp::varchar::timestamp_ltz as jamf_last_contact from \"DB\".\"SCHEMA\".\"JAMF_INVENTORY\" QUALIFY 1 = row_number() over (partition by ji_serial order by jamf_last_contact DESC ) ) as JI on cs.serial_number = ji_serial INNER JOIN \"DB\".\"SCHEMA\".\"HR_DATA\" AS WD on SN.EMAIL = WD.PRIMARY_WORK_EMAIL WHERE WD.LAST_DAY_OF_WORK is Null ) SELECT * FROM most_recent_cs_data QUALIFY 1 = ROW_NUMBER() OVER (PARTITION BY CS_SERIAL ORDER BY MOST_RECENT_CS_CHECKIN DESC) AND MOST_RECENT_CS_CHECKIN \u003c dateadd(day, -14, current_timestamp()) ; Visual Example: Now all teams are able to leverage asset data in their queries and treat our asset management data as a source of truth for all asset states Org wide. You just need to join this data in the data cloud. This is why asset inventory data is fundamental to many teams at any organization that wants to go down the data enabled path. It is a baseline for almost any data you wish to know. The above query takes all the assets we wish to see the status of and join it to security data, MDM data, and HR data to give us very accurate data sets of how healthy our security agents are. Security software cannot do this alone, as it does not have any knowledge of your asset data. If there is an integration that exists, you must build it and hope it works for your needs. With the data cloud you just ship the raw data and figure the rest out in post. So many teams benefit from a good set of asset data in your data warehouse. The query above is letting us know every time an asset hasn’t submitted data to the security agent’s cloud tenant in over 14 days. These queries are also very reusable, and other teams can just simply modify that like dateadd() line for it t","date":"2022-08-08","objectID":"/posts/complete-asset-oversight-datacloud/:0:3","tags":["data cloud","analytics","asset management","metrics","big data"],"title":"Complete Asset Oversight with the Data Cloud","uri":"/posts/complete-asset-oversight-datacloud/"},{"categories":["data","snowflake","IT Operations","asset"],"content":"Use Inventory and Asset Data to Fix Inventory and Asset Data How do you know you are capturing all your assets properly? How can you tell what is an Org-owned asset versus a BYOD asset? How can you catch process failures and get proper data in your asset tracking system that is accurate? Well, when you ship all your data from all of your tech stacks to a data cloud platform this is not difficult to manage. You just join your data sources and delta the things that do not match! That is it, it isn’t some complex algorithm involving tons of API code, middleware servers, complex data tool designed for specific niche purposes, etc. Query example: SELECT JI.WEBHOOK:eventTimestamp::varchar::TIMESTAMP_LTZ as last_contact , JI.EVENT:computer:serialNumber::varchar as j_serial from \"DB\".\"SCHEMA\".\"JAMF_CHECKIN\" as JI WHERE TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(days, -30, current_timestamp()) QUALIFY 1 = ROW_NUMBER() OVER (PARTITION BY JI.EVENT:computer:serialNumber::varchar ORDER BY last_contact DESC) and not exists( select upper(SN.SERIAL_NUMBER)::varchar as sn_serial from DB.SCHEMA.SERVICENOW_ASSETS_V as SN where sn_serial = JI.EVENT:computer:serialNumber::varchar ) ; The query above simply selects the absolute latest webhook data from our MDM service, meaning a device is enrolled into our MDM and sending events. So, a device must be actively enrolled into our MDM to get required settings and software to access Org owned systems and services at most organizations. This is a very common strategy IT leverages. However, how do you catch failures in your processes and automation? Well if a device is sending MDM events and not in our Asset Management Software, I would very much like to know about it. Maybe some manager uses the Org credit card and heads to Best Buy bypassing procurement and all of a sudden you have a dozen rogue systems in your MDM with zero of them in your Asset Manager. The above query captures this and gives you all the info you need to go create a record in your Asset Management Software. This is also a good way to track BYOD systems, as a BYOD system is never procured internally, nor is it Org owned. All of your incoming agent data can be used to find all the missing systems that have fallen through the gaps. I think we can all admit that not every computer hardware vendor offers the same level of service and integration when you buy assets from them. Some of them can get that data fully automated to your asset software, and other vendors can at best email you a spreadsheet of devices. Also, mistakes happen, and what serial numbers of devices are on your paperwork versus what you physically can be different. We can use all of our external systems’ data sources to fix and clean up our asset data. I do not know of any Asset Management Software that can do any of this, and if some can I would be very interested to know if it is this easy to get that sort of data and insights with their software. This is why asset data is so fundamental to every Org out there. When I started my data journey a few years ago I would have laughed if someone told me all I needed to do to get this data was to use not exists in a single query with a few joins! I would have not believed it. ","date":"2022-08-08","objectID":"/posts/complete-asset-oversight-datacloud/:0:4","tags":["data cloud","analytics","asset management","metrics","big data"],"title":"Complete Asset Oversight with the Data Cloud","uri":"/posts/complete-asset-oversight-datacloud/"},{"categories":["data","snowflake","IT Operations","asset"],"content":"Things to Ponder If you are struggling with getting control over your inventory of assets at your organization, consider using something like a data cloud platform to leverage your asset data along with many other data sources. If you truly want your asset software to be your source of truth of what physical assets are assigned to what humans at your organization you will have to put in a lot of effort to maintain this and ensure its accuracy. In doing so, you can also ship all that data and benefit from your labors exponentially across your entire organization. Data Sharing makes this possible, and I feel like I am not stressing this enough, Data Sharing is the future. Legacy systems cannot accomplish this as easily, and they cannot scale like the cloud can. I could probably write a short book on this subject alone as asset data is so useful across so many queries for so many use cases. This is why I keep calling it fundamental as it is the baseline of all your metrics and asset data. Then you can use all the data to literally improve your asset data as well! It is really hard to gauge the value a platform like Snowflake brings to IT and Security teams because it is exponential, quantified by scientific notation, and one single piece of data can be reused in countless queries, metrics, intelligence and more. Making your returns on the total cost of ownership of the data cloud nothing short of fanstatic. We use this data along with other data sources to improve our asset data all the time. Remember failures happen, even Amazon, Google and Microsoft cannot guarantee 100% uptime, and they have more money and resources than all other Orgs on this planet! The data can be rough at times, and it can give ya a good old-fashioned gut punch when it doesn’t look good. This is something you must accept when becoming a data enabled organization. However, it gives you a path forward to fix things always. Without this data you just wouldn’t know you had these problems, you had these failures, and worst of all no data to help you with a path forward. You can only fix what you know you need to fix, and the data cloud helps organizations in many ways. Wouldn’t you rather know these things? ","date":"2022-08-08","objectID":"/posts/complete-asset-oversight-datacloud/:0:5","tags":["data cloud","analytics","asset management","metrics","big data"],"title":"Complete Asset Oversight with the Data Cloud","uri":"/posts/complete-asset-oversight-datacloud/"},{"categories":["automation","tools","CIS 3"],"content":"Data-Enabled: Post Nudge Campaign Results A lot of Organizations have adopted things like CIS Controls and part of those controls are around patching and vulnerability management. Depending on what version of the CIS controls you are using, this could be CIS 3 or CIS 7. This can already be confusing as the controls are very similar between versions but the have moved places in the CIS framework. One thing I think most tech and security professionals can agree upon, is that patching often to the latest operating systems is a great way to avoid your fleet of devices from being exploited. Known vulnerabilities have a shelf life, and bad actors know that some organizations do not patch, or patch extremely slow. So, they develop attacks against known published vulnerabilities. The good news is that this is very easy to mitigate, and all you need to do is to update your operating system and applications across your Org’s fleet of devices. This blog post will be on the specific topic of updating macOS. ","date":"2022-07-01","objectID":"/posts/data-enabled-nudge-results/:0:1","tags":["nudge","open source","IT Tools","software updates"],"title":"Data-Enabled: Post Nudge Campaign Results","uri":"/posts/data-enabled-nudge-results/"},{"categories":["automation","tools","CIS 3"],"content":"Original Problem We originally faced a problem, and that was that softwareupdate on macOS has had some nasty known bugs in the last recent years. While, Apple has patched them at least one time according to the recent release notes, many IT professionals in our communities have still experienced issues with updating macOS. Relevant Release Notes from macOS 12.3 Improves reliability with managed software updates using InstallLater. Resolves an issue where software update scans become unresponsive. MDM software updates lack some features around UI/UX and have inconsistent results in our testing. We first tried to use them to auto update Mac Minis running digital signage and our conference rooms. Given that these were static devices that were always on the network, and we had very inconsistent results we decided to not use MDM commands to update the OS of our fleet. To add fuel to the fire, our MDM vendor also had an issue where if their binary collected available software updates from macOS, it could potentially hang the agent. Thus, leaving the device in a non-managed state. So, we disabled the Software Update collections, and got rid of all existing software update workflows since this bug was impacting other things. Our existing dialog box workflow which relied on this data no longer worked. What were we to do? Wait for vendor fixes, rebuild our entire workflow, or maybe migrate to a different tool or method to get our desired end results? As a quick and dirty solution I hard coded OS versions into a Python script I had written to get us by while we assessed our situation. ","date":"2022-07-01","objectID":"/posts/data-enabled-nudge-results/:0:2","tags":["nudge","open source","IT Tools","software updates"],"title":"Data-Enabled: Post Nudge Campaign Results","uri":"/posts/data-enabled-nudge-results/"},{"categories":["automation","tools","CIS 3"],"content":"Enter Nudge Nudge is an open source tool written by a community member to solve this problem. Nudge is an independent tool that can operate on its own, and is not dependent upon MDM to work. The app runs with a launch agent that will prompt the user to update their OS on a regular interval. Since software updates on macOS have been problematic for the past couple years folks have been looking at alternative ways to accomplish this goal. Out of the box Nudge is honestly a great tool. We did not find much of anything wrong with the default settings it came with. Couple that with the fact that Nudge allows the end user to control when they get “Nudge’d” we felt that giving each end user full control over when Nudge will pop up and ask you to update was the best option. You cannot please everyone in these type of decisions, so giving everyone their own opinions to be enabled is a good route to try. Here is what our Nudge screen was prototyped as, our final production one was slightly tweaked from this: You can see in the circled part of the bottom right an employee can choose when they want to be notified next and this puts all the control in their hands. The headsup display that Nudge gives a human is also a combination of minimal yet very informative. Which we also found perfect. If you click on the custom option it brings up your calendar where you can choose the next time on your calendar when you want to be notified. So my team, and I deployed Nudge to our devices, demoed it to leadership, and took it through our process to release it to our fleet of macOS devices. We also launched a small campaign of Slack announcements and emails to our coworkers informing them we were deprecating Big Sur and older OSes completely. We were going to get our entire fleet up to 12.4 to combat these software update bugs that are impacting other things. ","date":"2022-07-01","objectID":"/posts/data-enabled-nudge-results/:0:3","tags":["nudge","open source","IT Tools","software updates"],"title":"Data-Enabled: Post Nudge Campaign Results","uri":"/posts/data-enabled-nudge-results/"},{"categories":["automation","tools","CIS 3"],"content":"Post Campaign Results The best part about being data-enabled is that we already collect, ship, and ingest all the data from our organization. We are enabled to use the data in our everyday work life, and we share it freely among our teams and departments. I can query OS update trends, what departments patch the fastest, what department patches the slowest, what locations update macOS faster, so on and so forth. With additional data sources I can also filter out contingent workers, interns, terms, new hires, legal/term hold devices, primary and secondary devices, and a plethora of other things that can be highly useful when wanting to look at data and get a clear context of what your data means. example query in Snowflake: with macos_devices as ( select SN.substatus as type , JI.EVENT:emailAddress::varchar as USER , JI.EVENT:osVersion::string as OS_VERS , JI.EVENT:serialNumber::varchar as SERIAL_NUMBER -- little logic to add \"conference room\" as a department to conference room computers , IFF(JI.EVENT:emailAddress::varchar = 'foo@acme.com' AND JI.EVENT:department::varchar = '', 'conference room', JI.EVENT:department::varchar) as department , WEBHOOK:eventTimestamp::varchar::timestamp_ltz as event_date FROM \"DB\".\"SCHEMA\".\"JAMF_INVENTORY\" as JI inner join \"DB\".\"SCHEMA\".\"SERVICENOW_ASSETS_V\" as SN on upper(SN.SERIAL_NUMBER) = JI.EVENT:serialNumber where type in ('primary', 'secondary') ) select count (distinct serial_number) , event_date::date , os_vers from macos_devices -- just modify the versions you want displayed here where os_vers in ('12.3.1', '12.3.0', '12.4.0') group by 2, 3 ; example of a full dashboard: Just being enabled to filter out any exited employee from data is very powerful, and I show the difference in the data in the above screenshot. With the power of Snowflake, data sharing and being data-enabled this is just the tip of the iceberg on data you can dive into. This will give you more insight of your IT assets and your fleet of computers than you have ever had before. It is pure magic as some say. This is what it is like when you are data-enabled, IT professionals can get fine-tuned data with the proper context applied. Data can also help showcase results. What we do know as a matter of fact, is that we did our first Nudge campaign to the entire Org with the release of 12.4. Our campaign to upgrade the fleet when 12.4 was for all Macs in our Org. So, we were also leveraging Nudge for a major OS upgrade from Big Sur to Monterey since we knew Big Sur had existing bugs that were not being addressed. About the data This data is a time series set of data. The reason you might see the numbers change from day to day, is that the query is collecting all versions of the OS for that day. Devices that are not online to submit data that day, won’t have an entry regardless of what OS they are on. Also factor in things like PTO or any other time off like holidays and weekends and you will likely see numbers drop when folks are not working. First we can look at our Big Sur versions over time. This was a downtrend of data due to the fact we were pushing for our fleet to update to Monterey. We had experienced some bugs in Big Sur with software updates, and with other tools. So, we had been campaigning with our previous workflows and tools to get folks to upgrade to Monterey. Big Sur Versions over time: Early Adoption of Monterey over time: Monterey 12.3.x to 12.4 adoption: macOS 12.4 Adoption rate from Nudge: ","date":"2022-07-01","objectID":"/posts/data-enabled-nudge-results/:0:4","tags":["nudge","open source","IT Tools","software updates"],"title":"Data-Enabled: Post Nudge Campaign Results","uri":"/posts/data-enabled-nudge-results/"},{"categories":["automation","tools","CIS 3"],"content":"Conclusions Looking at the historical data, which we can keep at scale in the data cloud, we can observe that our adoption rates did seem to happen faster over time than our previous campaigns to update macOS. Of course this data represents the results of my team’s labor and not necessarily the perception our coworkers on how the new tool worked. The big takeaway for me personally is how much this impacted major OS upgrades since we have been trying to phase out Big Sur here. Nudge really helped us achieve this goal, and we were pretty impressed with the results. I am also happy to say that while we did get minimal feedback on Nudge, it was almost all positive. The non-positive feedback was also mostly neutral and not really negative at all. Users really loved the concept of them being in full control of when they get notified to update and being able to defer when they choose. There is additional data you can collect with Nudge as well. Like tracking how long folks wait to update, how many times they click on the defer button, and more. However, I will not be covering any of that data in this blog post. Overall, our Nudge campaign seems to have been one of our most successful OS update campaigns we have observed. It has also been one of the smoothest I have witnessed personally in my career. I feel this tool is fantastic, and it requires very little customization to meet the end goals of user experience and results. ","date":"2022-07-01","objectID":"/posts/data-enabled-nudge-results/:0:5","tags":["nudge","open source","IT Tools","software updates"],"title":"Data-Enabled: Post Nudge Campaign Results","uri":"/posts/data-enabled-nudge-results/"},{"categories":["data","data driven","culture"],"content":"Creating a Data-enabled Culture I think the term data-driven is common, and a lot of people are aware of the term. It also has a broad area of application across many practices. I feel my Org is already data-driven. We use data in all our metrics, planning, discussions, collaboration and more. However, today during a cross team meeting I realized that we are also a data-enabled organization. When the term popped into my head I thought maybe I was on to something new, or perhaps maybe something a bit esoteric. Turns out, I was wrong, and this term is commonly used in areas already. A quick Google search will show lots of results for data enabled, and here is a blog post I found from the first page of results. I have a previous series of blog posts on the topic of being data driven, which starts here with Part I. This small series of blog posts goes over just some ways we have become a data-driven organization, but during my data journey I have been slowly realizing that we are also very much a data-enabled organization. I think this is very important to observe and reflect upon. Giving your IT staff access to data enables them to just do the work, to just get the answers and to collaborate and share those answers with other people and teams. ","date":"2022-06-24","objectID":"/posts/creating-data-enabled-culture/:0:0","tags":["data cloud","culture","data sharing"],"title":"Creating Data Enabled Culture","uri":"/posts/creating-data-enabled-culture/"},{"categories":["data","data driven","culture"],"content":"Data Sharing is Data Caring ♥️ It all starts with data sharing. With products like Snowflake you can easily share data amongst teams and people. Having access to raw data from many sources allows us to always be data-enabled. You learn to stop making assumptions, anecdotal hypothesis, educated guesses, hunches, or good old-fashioned “gut-feelings” when it comes to your IT and Operations data. If you are an organization that already ships all your data to a platform like Snowflake and you have given your employees the proper access, and opportunity to leverage the data, you are now walking the path of being data-enabled. Some of the data we leverage: MDM Data - Intune and Jamf Pro Inventory \u0026 Asset Data - Asset tracking from ServiceNow Security Agent Data - Vuln scans, EDR/DLP agents, backup logs, etc Systems Data - osquery data, metadata from MDM, results from data collectors, etc Authentication Log Data - If you are authenticating to the IdP we can assume you are working (and not on PTO) Employee Directory Data - Name, department, email address, manager, FTE/Contractor, etc SaaS/PaaS Data - Data from our SaaS apps and Cloud Services When you have all the raw data already available to you, and data pipelines delivering continuous up-to-date data, you can enable your IT Organization to be data-enabled. This is very much a game changer, and even a shift in culture, capabilities, collaboration, and much more. All the data is already there, you just need to work with it. Data sharing among teams and organizations is what helps enable this. This is the future of how Organizations gain IT insight. ","date":"2022-06-24","objectID":"/posts/creating-data-enabled-culture/:0:1","tags":["data cloud","culture","data sharing"],"title":"Creating Data Enabled Culture","uri":"/posts/creating-data-enabled-culture/"},{"categories":["data","data driven","culture"],"content":"Applying this in Everyday Life There have been many times in my data journey so far where I have had to unlearn past things, and I have had to admit that things I thought that were solid answers throughout my career were oftentimes not quite right. Reflecting upon this, there were times when I was mostly-right, and there were times that I was likely way off. Humans can develop a plethora of different biases which can stray us for the actual truth. This is why I have started to train myself to not make assumptions, but instead to put my assumptions in the form of a data query, and ask the data a question. We always trust, but verify the results and ensure the data in our data sources is in fact valid. I was recently in a conversation about a specific piece of data around an application vulnerability. The data was showing around 400 vulnerable instances of this application, and looking through the data I noticed that the vuln scan agent was picking up applications that were in end user’s downloads folder. This is an old problem we discovered through data a few years ago, and I already have a solution to check against our fleet on this. My colleague and I had a discussion about this, and I proposed that there were probably at least equal amount of instances of this vulnerable application in random download folders across our fleet. My colleague guessed that was way too high and the number was likely a lot smaller. So off to our data we went, and I ran my user space application query. The results returned 463 instances of that Application in user space. To me, this is an example of how being data-enabled comes into play in our day-to-day work lives. We knew this was a possibility from previous project work, I had already built a tiny tool to collect user space applications, and we were already ingesting that data into Snowflake. I even already had a dashboard built for this. Having all this data available to us at the tips of our fingers is what really helps us be data-enabled. Data Journey Story I used to assume things like this. In fact, a couple of years ago I would have likely guessed that 463 instances of a specific application located in random downloads folders across our fleet was too high of a number. I have slowly learned to trust the data and make less assumptions. I am still oftentimes reminded of a crucial lesson I have learned in my data journey, and that we are often just wrong when we guess. In my meeting today with my sister teams, we were discussing how to tackle some stragglers in our post Nudge campaign to update macOS across our entire fleet. The campaign was already a large success numbers wise. We had some of the fastest adoption times we have seen by ditching our old macOS update flows in favor of this. We also have all the data for it too! One of the talking points that was brought up, was how to improve the existing data to help enable our support teams to better assess and distribute the work to go troubleshoot our devices that are in a weird state or experiencing the known software bugs we are trying to patch to fix. My colleague asked if we had other data values we could add to the existing dashboard to help make their job easier. Since we already have all this data available we are already enabled to make changes to the queries to give our colleagues more actionable and better data. Sure I now have to rewrite some queries and that is additional work for me or my team to do, but the fact is we already have all the data. We already have all the data pipelines with continuous data ingestion, and we are fully enabled to leverage the data to make it work for us. During our cross team meeting it felt as if we were all on the same team together looking at the data. We collaborated and formulated a plan to make improvements to our existing dashboards and queries. My team is helping provide a service to another team. This was possible due to us all being data-enabled. ","date":"2022-06-24","objectID":"/posts/creating-data-enabled-culture/:0:2","tags":["data cloud","culture","data sharing"],"title":"Creating Data Enabled Culture","uri":"/posts/creating-data-enabled-culture/"},{"categories":["data","data driven","culture"],"content":"Enabling Others Another thing that really reigned in with me this week, is that the dashboards we create are all cross team functional for the most part. Sure, there are definitely a few dashboards that are specific to my team, and I can imagine other teams might have their specific dashboards as well. However, I find that our queries and data are very much applicable to other people and team’s interests. When we rolled out our mTLS password-less authentication app here, I made a dashboard for the project. I had stakeholders from various teams asking me for data often, so I just enabled other people to get the data whenever they wanted. All that had to do was to load the dashboard and hit the play button, and then a few moments later they got the latest up-to-date data. We enabled everyone to get the data on demand when they wanted to. In the above screenshot is just a time series data of the version of the app we deployed over time. There are many other bits of data in this dashboard, so this is just one piece of information from it. In the beginning this very useful to see what versions our fleet was at, how well the self updating mechanism was working from the vendor, and we could correlate bug fixes and enhancements from the release notes to our support tickets. Things like if a bug fix was released in a specific version, we could see what version the fleet was and then tie those to support tickets for the problem that the new release fixed. This allowed us to better forecast our support costs to the app, and track when a problem was fixed by a new release. Project managers, architects, leadership, support folks, and so forth could just go to the dashboard and run the update button. I just enabled them to get the data whenever they wanted it. ","date":"2022-06-24","objectID":"/posts/creating-data-enabled-culture/:0:3","tags":["data cloud","culture","data sharing"],"title":"Creating Data Enabled Culture","uri":"/posts/creating-data-enabled-culture/"},{"categories":["data","data driven","culture"],"content":"Build It! One time at a new employee on-boarding training I heard someone say, “The second most valuable thing in this room is the new laptop you are receiving, with the most valuable thing being you.” People do make up the organization, and hiring the right people is a good start to building a data-driven culture, and also a data-enabled culture. Managers should encourage professional development around the skills that are needed to accomplish this, and the organization should invest in its people to train them. Socialize data sharing and let data be free and open among teams. Gate-keeping data doesn’t help anyone outside the data owners, and it doesn’t enable the rest of the organization to leverage that data. Don’t silo your data, don’t keep other teams in the dark, and instead enable them to be successful with all the data you can. To me this is the future of how IT and Operations shops leverage their data to make the absolute most of it, enable their colleagues to be informed, and to gain the most insight you can about your technology stacks. ","date":"2022-06-24","objectID":"/posts/creating-data-enabled-culture/:0:4","tags":["data cloud","culture","data sharing"],"title":"Creating Data Enabled Culture","uri":"/posts/creating-data-enabled-culture/"},{"categories":["data","osquery","snowflake"],"content":"Snowflake, osquery and Fleet is Pure Magic! Many of you have probably heard of osquery, which is software you can install onto a computer that allows humans to query the OS to return fast and reliable system data. Osquery is not new, and many Organizations have been using it in various capacity for years now. Vendors and developers also ship osquery with their products these days as well. This allows a developer or vendor to collect local data fast and reliably for their software and solutions. My team and I have been evaluating osquery along with a product called Fleet to see what we can accomplish with getting fast and reliable data pipelined to Snowflake. Like all projects, research and development, proof-of-concepts, and so forth one can start by simply stating a problem statement or a story. The data story I used to kick this proof of concept off was simply this: Our Data Story As an IT Engineer, I want fast, reliable and robust data from all the systems we manage. While also getting that data as fast as we possibly can at scale. Our problem statement will probably be things many of us in IT and Operations face every day, especially with endpoint management solutions. MDM is really only good for MDM things, and while those things do bring value to IT organizations, it often falls short of the full scope of what we need. Data collection is a big part of this, and MDM solutions have limitations around data collection and data storage. Honestly, this should be expected as MDM tools primary functions are not data collection, nor are they data warehouses. ","date":"2022-01-13","objectID":"/posts/snowflake-osquery-fleet-magic/:0:0","tags":["data","operations","data-driven","osquery","snowflake"],"title":"Snowflake Osquery Fleet Magic","uri":"/posts/snowflake-osquery-fleet-magic/"},{"categories":["data","osquery","snowflake"],"content":"Data Collection at a Glance MDM typically collects data every 24 hours in most MDM applications out of the box. It is also either a manual process to add inventory data collection into your workflows when you want to collect data on state change, or requires writing a series of data collection scripts. For example, every time your MDM tools install an application, the MDM tool must send that data back to the MDM servers to store that data. This often results in data drift where the data on the actual system and the data in the server side application do not match. The data will match next time that device submits inventory to the MDM service. IT Engineers can sometimes crank up inventory collection, but it is at risk of hitting rate limits, or even DDoS’ing your own MDM service. MDM also does not collect things like Python Packages, homebrew binaries installed, web browser plugins, running process info, and more. Osquery can collect much more data and at a much higher frequency. Since osquery is a completely separate tool chain, it also has no dependencies on your MDM or your MDM infrastructure. MDM Solutions I have not personally tested every MDM solution out there, so this is an anecdotal summarization of my experiences and an overall generalization. Some MDM tools may be able to collect data faster than every 24 hours. Some MDM solutions do allow for customized data collection, but that is not without labor to build and maintain. ","date":"2022-01-13","objectID":"/posts/snowflake-osquery-fleet-magic/:0:1","tags":["data","operations","data-driven","osquery","snowflake"],"title":"Snowflake Osquery Fleet Magic","uri":"/posts/snowflake-osquery-fleet-magic/"},{"categories":["data","osquery","snowflake"],"content":"What is Fleet? Fleet is a centralized management and orchestration tool for osquery. It allows tech professionals to centralize queries, query packs, configurations, and handles secure communications from the endpoints to the service. Fleet also provides a set of tools to manage the infrastructure and the osquery installers for each platform. There are many osquery solutions out there that do similar things, and Fleet was attractive to us because they focused solely on getting the data and managing the queries and configurations. Which is what our initial goal was in this proof-of-concept exercise. Fleet has a channel on the Mac Admins Slack you can join if you are interested in learning more. Additional noteworthy features are (but not limited to) SAML integration for your IdP, support, RBAC for teams, and they are looking to add vulnerability data to their product as well. They also display the osquery table schema in the web application for quick reference, which is a nice quality of life feature add. The two features I want to focus on for this blog post are the live queries and the scheduled queries. Live queries are probably what you would assume they are. It is a feature where you can run a query from the Fleet application and get near-real-time results back from an endpoint (or many endpoints) very quickly. Scheduled queries run at a set increment of time and those query results can be streamed from the application to cloud storage. Live Query Interface: Scheduled Queries Interface: ","date":"2022-01-13","objectID":"/posts/snowflake-osquery-fleet-magic/:0:2","tags":["data","operations","data-driven","osquery","snowflake"],"title":"Snowflake Osquery Fleet Magic","uri":"/posts/snowflake-osquery-fleet-magic/"},{"categories":["data","osquery","snowflake"],"content":"Getting the Data Into Snowflake Running Fleet in AWS means you can leverage all the great cloud tech that exists in most modern cloud platforms. We chose to host this in AWS, and it is highly likely you could do something similar in another cloud provider. At a high level this is how we implemented it: The Fleet Application is self-hosted in a private cloud VPC Configured AWS Kinesis Firehose to stream data to S3 osquery binary data and query results data go into two separate folders with in the S3 bucket Configured Snowpipe to consume data on the event of data being written to S3 Exposed a load balance appliance on the edge so clients could communicate to the service securely over the internet Quick diagram: In this proof-of-concept project we decided to get data into our dev environment as fast as we possibly could. So, queries were running every 15 minutes against a half dozen or so test devices. We also collected data on Linux, macOS and Windows 10 devices. Our observation is we got the data very fast from Fleet and into Snowflake. It seemed to take minutes, and it was fast enough it was tough for us to really time how long it actually took end to end. Every time Fleet would run a scheduled query, those query results would be pipelined into a S3 bucket by streaming the data with Kinesis Firehose. Since Snowpipe can leverage the SQS event API, upon that bucket getting data written to it, the native cloud integration would notify Snowpipe to consume the data automatically. This setup allows for continuous automated data flow from the Fleet application right into Snowflake. ","date":"2022-01-13","objectID":"/posts/snowflake-osquery-fleet-magic/:0:3","tags":["data","operations","data-driven","osquery","snowflake"],"title":"Snowflake Osquery Fleet Magic","uri":"/posts/snowflake-osquery-fleet-magic/"},{"categories":["data","osquery","snowflake"],"content":"Working with the Data in Snowflake Now that our end to end data pipelines were flowing, we needed to next ensure we could use and leverage the data. Fleet will store the query results as a JSON document, which is perfect for our setup. We are simply storing all the query results in a single raw data table. I wanted to start with Windows 10 data, as our Windows MDM solution doesn’t export a lot of data natively. The data we do get from our Windows MDM solution is more of a boolean result of True or False, and not the actual data itself. To explain this simply, we have a set of compliance policies that check against certain states of security settings, and if any one of those states fail, it marks the devices just as Not Compliant, but it does not tell us which state is failing. Then there is no way to get third party application data out of it unless you want to build an API ingest connector. Lastly, like all MDM solutions inventory collection is often once a day. Our data story was to get this data as fast as we possibly could, and we wanted to get it every 15 minutes versus every 24 hours. A quick and easy query to test in Fleet with the live query feature to see if I got the results I wanted was simply this: SELECT * from programs; The above query will result in giving us everything osquery can collect about Windows 10 applications. If you osquery installed on a Windows 10 device you can run the interactive mode binary and just test out the query locally. In Fleet for this scheduled query we named it windows-apps-ingest which later downstream we can use as a primary key in Snowflake. With Snowflake, I can simply run this query to grab the data I need. Remember, we are shipping the raw JSON query results to a single schema/table in one column and then leveraging post data processing features in Snowflake to get the data we need out of the raw JSON query results. Here is a truncated sample of the data you will get from Fleet into Snowflake. data sample: { \"action\": \"snapshot\", \"calendarTime\": \"Sun Dec 12 06:13:52 2021 UTC\", \"counter\": 0, \"decorations\": { \"host_uuid\": \"UUID-of-device\", \"hostname\": \"hostname-of-device\" }, \"epoch\": 0, \"hostIdentifier\": \"device-id\", \"name\": \"pack/Global/windows-apps-ingest\", \"numerics\": false, \"snapshot\": [ { \"name\": \"Brave\", \"source\": \"programs\", \"type\": \"Program (Windows)\", \"version\": \"96.1.32.115\" }, { \"name\": \"Fleet osquery\", \"source\": \"programs\", \"type\": \"Program (Windows)\", \"version\": \"0.0.3\" }, { \"name\": \"Google Chrome\", \"source\": \"programs\", \"type\": \"Program (Windows)\", \"version\": \"96.0.4664.93\" }, Now lets really work with the data now that we know we can use the name key in the JSON data to filter out the exact data we want to work with. One could also model the data off this raw data table into other tables and views, but that will be a future blog post on data modeling. It is just a bit worth of noting now if you are new to working with data. We will use a feature in Snowflake called flatten to essentially turn the JSON keys and values into something similar to columns and rows in a relational database. select od.raw_data['decorations']['hostname']::string as hostname , od.raw_data['decorations']['host_uuid']::string as UUID , f.value['name']::string as app_name , f.value['source']::string as app_source , f.value['version']::string as app_version , try_to_timestamp_tz(left(od.raw_data:calendarTime::string, 25) || '00:00', 'DY MON DD HH:MI:SS YYYY TZH:TZM') as CALENDAR_TIME from \"DB\".\"SCHEMA\".\"IT_DEV_OSQUERY_TABLE_JSON\" as od , lateral flatten(input=\u003eod.raw_data:\"snapshot\") as f where RAW_DATA:\"name\"::string = 'pack/Global/windows-apps-ingest' QUALIFY 1 = ROW_NUMBER() OVER (PARTITION BY UUID, app_name ORDER BY CALENDAR_TIME DESC) ; If you look at the JSON data above the query you will see there is an array with in a dictionary like data structure under the key name of \"snapshot\". This is where osquery will list all the installed applications and other metadata relating to the installed ","date":"2022-01-13","objectID":"/posts/snowflake-osquery-fleet-magic/:0:4","tags":["data","operations","data-driven","osquery","snowflake"],"title":"Snowflake Osquery Fleet Magic","uri":"/posts/snowflake-osquery-fleet-magic/"},{"categories":["data","osquery","snowflake"],"content":"Conclusions and Takeaways Without a doubt we have found this entire proof-of-concept extremely valuable. A product like Fleet that offers centralized managed and orchestration of osquery, integrated with the power and scale of Snowflake is truly something to marvel. We have been collecting fleet data in our test device group every 15 minutes and getting that data into Snowflake in very short periods of time. This is so much faster than MDM, that MDM is not really even comparable. The amount of data we get is also fantastic. Browser extensions have always been a pain point of data collection for example, and osquery makes this so easy to get. The benefits aren’t just with data either, they are with your tools stacks as well. Fleet and osquery are standalone products that have zero dependencies on MDM, sans installing agents and binaries. This means that if an Organization ever decides change MDMs, a stack like Fleet + osquery could easily integrate into the new MDM solution. It also removes the dependency of tossing all your data collection in something like MDM. This stack is also cross-platform, which is either a great or not-so-great thing depending on context. In this specific context, it is a very good thing. I can now manage data collection across our macOS, Linux, and Windows 10 devices. My Org and team manages all three of those platforms, so having a tool stack like this is amazing. The data pipelines are automated, and we publish data to S3 then consume it into Snowflake at a very fast rate. This is another great benefit of this solution. Some IT and Security tools only ship data when an event occurs, meaning the software you are using has to detect a specific event, and then ship that event. This model is great for noise reduction in say an incident and response or monitoring scenario, but it also is not perfect. You can observe data drift or reactive scenarios vs proactive ones without a constant data stream. Now you can have both your fast data stream and your event based data together in one data platform! Lastly, we have scoped out many benefits and use cases for a solution like this. Even though our data story was originally just around getting the best possible data we could, while ingesting it as fast as we possibly could, the use cases have already expanded. Here goes our thoughts on use of a stack like this: Robust IT and Operations Data Incident and Response investigation Threat hunting Licensed software usage Vulnerability data Systems configuration data on security and compliance Extended application data for our software patching story Data sharing this data to many other teams internally Enabling other teams to run live queries to get near-real-time data Remember, data sharing is data caring, and IT teams that collect robust data like this should share it to other teams. Enable everyone to collaborate more and build a better data culture with in your organization. A solution like this could be easily extended into many teams at an organization and Snowflake makes that part pretty easy. When I get more time, and as we progress with solutions like this I will likely share more of our data journeys in my blog. This is mind-blowing to me, as I have never had a setup at any job where I can get data from our end user device fleet this fast. I don’t know how to describe this other than it is just pure magic. ","date":"2022-01-13","objectID":"/posts/snowflake-osquery-fleet-magic/:0:5","tags":["data","operations","data-driven","osquery","snowflake"],"title":"Snowflake Osquery Fleet Magic","uri":"/posts/snowflake-osquery-fleet-magic/"},{"categories":["conference","tech","data"],"content":"JNUC 2021 Recap Hello everyone, this will be a quick recap of my experience from the JNUC 2021 virtual conference, and a companion update to the talk I gave. First, I need to really thank Jamf for putting on a well produced virtual conference. I will admit that I was a bit skeptical of things like audio and video quality, participant interaction, and the look and feel of pre-recorded talks. Everything exceeded my expectations. The audio and video quality were very well done. The recorded sessions were great to watch, and the attendee interaction was a lot better than I expected. We all hopped into various Slack channels to interact with each other on subjects or about specific talks that were given. While this does not come close to seeing everyone in person, it still felt great to see everyone virtually. Some of us even hopped on several web conference calls to interact with each other, talk shop, have a drink, and just in general have some fun. This was the first time I have done a pre-recorded conference talk, and I will admit it did feel awkward to me personally. I am so used to seeing actual people when I have given talks in the past. Recording it felt like I was acting in a used car commercial or something similar. Regardless of those awkward moments the production company was great to work with. ","date":"2021-11-18","objectID":"/posts/post-jnuc-2021/:0:0","tags":["cloud","data-driven","it-ops-data"],"title":"Post JNUC 2021","uri":"/posts/post-jnuc-2021/"},{"categories":["conference","tech","data"],"content":"Being Data-Driven My talk can be viewed on YouTube now that Jamf has uploaded all the content online, which I have linked below. Data can be a magical thing with in your organization, but data itself doesn’t solve the entire problem. Adopting a data culture, enabling IT and Operations folks to grow data skills, and using data to change how you do things is the real magic. I hope this talk at the very least got some of you folks out there thinking about how your IT Org can leverage data. Centralizing your data into a single data platform where data can be freely shared amongst teams is really the big point I wanted to make in my talk. This is such a game changer, and it gives you insights into things outside your normal scope. I did see some other data topic talks at JNUC 2021 as well, and this was fantastic to see. Concepts aren’t always specific to the presenters, and concepts can be altered to fit one’s specific use cases. So, just by hearing the other data stories at JNUC 2021 is a great thing for everyone to go check out. I highly encourage people to dive in and take a look at the returns you can get from having great data, and a collaborative workforce using it. ","date":"2021-11-18","objectID":"/posts/post-jnuc-2021/:1:0","tags":["cloud","data-driven","it-ops-data"],"title":"Post JNUC 2021","uri":"/posts/post-jnuc-2021/"},{"categories":["conference","tech","data"],"content":"Other Presentations on Data and Security Here go some other presentations around security and data that I enjoyed. While, I may not use the same tools stack that the presenters use, the concepts and ideas from the presentations I think were valuable. Especially if you are new to some of these concepts. Since I think IT, Security, and Data all go together here are some videos that align with this. All of these talks are on subjects that also require having good data. An overall good introduction to some security concepts around vulnerabilities How bad configurations can snowball into security issues, you could use data to audit these. Here is another Org who has adopted a data based culture around IT and Security. Again, without robust, reliable, and data sharing these things are a lot more difficult to accomplish. I also had to post this, which is from two old friends of mine from our IT community. Bryson did call me out after all :-) ","date":"2021-11-18","objectID":"/posts/post-jnuc-2021/:2:0","tags":["cloud","data-driven","it-ops-data"],"title":"Post JNUC 2021","uri":"/posts/post-jnuc-2021/"},{"categories":["conference","tech","data"],"content":"Conclusion I think it is pretty clear that adopting a data-driven culture and framework has huge returns on investment. I hope some of these talks inspires others to start their data journey in the IT and Operations world. Having a syslog server is a nice start, and it gives you some data, but it is just the tip of the iceberg when it comes to all the data around how your organization operates. Maybe next year we can see each other in person at the next big conference. I look forward to finally seeing all your faces in person again someday. The rest of the JNUC 2021 Videos can be viewed here ","date":"2021-11-18","objectID":"/posts/post-jnuc-2021/:3:0","tags":["cloud","data-driven","it-ops-data"],"title":"Post JNUC 2021","uri":"/posts/post-jnuc-2021/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Gain Oversight of your Asset Inventory with Snowflake Welcome to Part III of my mini blog series on being data driven with Snowflake. This post will be solely dedicated to how an organization can leverage a platform like Snowflake to improve their asset and inventory controls. Both Part I and Part II can be viewed with those links if you haven’t read them yet. This post will focus on end user devices for the most part and not networking hardware, application catalogs, servers, and so forth. The concepts in this post could be applied to those things though. Every Organization I have worked for has struggled a bit with inventory of assets. It is not an easy task to just solve with a single tool or platform. Many aspects of Asset and Inventory control are also done with manual processes. Meaning, a human has to verify objects in asset trackers or change their states when physically verified. Some asset tools I wouldn’t even really consider products, but I would rather consider them platforms instead. This point of view comes from the vast amount of integration these platforms can have. With more integrations you will naturally have more data. This is where data cloud platforms like Snowflake can really enable IT \u0026 Ops teams to use data to drive change and improvements in their asset tools. This is my personal opinion based off the experiences I have learned while working with data in Snowflake. There are always many ways to accomplish the same goals with data. So, this is not the only way one can be data driven around this topic, but my personal opinion this is a wonderful way to start. ","date":"2021-08-10","objectID":"/posts/ops-data-snowflake-pt3/:0:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part III","uri":"/posts/ops-data-snowflake-pt3/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Building Data Pipelines for Success Inventory and Asset data is great by itself, as it can tell an Organization lots of information around their assets inventory. Inventory and Asset data is a lot better with friends. In a previous post I touched on applying baselines and context to your data sets, and this blog post will completely expand on that. Different tool stacks can act as a source of truth for the function is provides. MDM data for example can be the source of truth for all your active endpoints in your fleet checking in online. An Asset tool probably doesn’t have this data, and if you integrate it, you may end up with data lag. Remember, one of the key things about really being data driven is to get data quickly and reliably. If you have to wait days to get data, then you are days behind the current state of things. MDM data though is probably not really great asset data for several reasons. It probably does not tie into HR systems, and MDM systems typically do not support all the configurations around the many assets an Organization can own. At a high level, this is my recommended starting point for data collection: MDM Data Event Data WebHooks Log data HR Data Employee Directory Employee Data (FTE, contractor, start date, end date, etc) Asset and Inventory Data Asset Type (laptop, desktop, Mac, PC, etc) Asset State (primary, secondary, test device, etc) Assignment Data (what human is assigned this device) If you ship data from tools like the listed above, each will act as a source of truth for a specific data point in context. After you have this, you can start creating your baselines of what the data should mean. After you know it, you will be modeling your data and writing queries to get back the data you want on each point. Now, lets break down each data point and see how we can apply it. Event Based MDM Data Event based data is fantastic to get near-real-time data about your fleet. MDM tools that can ship Webhook Events, or any sort of logging that runs on a regular basis to indicate an asset has checked into your MDM tool and submitted data is pretty clutch. The MDM App data is the source of truth for active online endpoints in your fleet that are online and properly communicating to your MDM. Any MDM that can ship event data is pretty key here, if your MDM vendor does not support this I would recommend you file a feature request with them to get this data. It has a lot of value. In this blog I will use Jamf Pro Webhooks as my event based data set. The specific event we care about is the Iventory Event as this event is only generated when a device submits data to Jamf. Devices that do not submit data are considered in a broken or undesired state. Devices not submitting data can easily be put into a different data set, with a different context. Filter data but never get rid of data While mentioning filtering out devices that are not submitting data, this is important in the context of tracking active devices in your fleet. Devices that are not checking in should be grouped into a different context, so you can leverage that data in different ways. Like opening tickets for remediation, or using that data set in a workflow, etc. The WebHook data must contain some sort of primary key in it for it to be valuable. Typically, a serial number is good enough, but the more Unique IDs a hook can ship the better. That would give the IT \u0026 Ops teams more options to work with. The primary key will be used to run joins to match different data sets you share with in the Snowflake platform. You should ingest these WebHooks as often and quickly as possible. They are typically very minimal, so you can store them historically for a very long time. HR Data Most Organizations have some sort of Employee Directory, which includes some basic information about each employee that is shared throughout the entire Organization. This data should also be available in Snowflake, and it is very useful for many applications. HR data is often high","date":"2021-08-10","objectID":"/posts/ops-data-snowflake-pt3/:1:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part III","uri":"/posts/ops-data-snowflake-pt3/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Turning the Data into Magic Assuming that you have all these data pipelines in place, the data is shared, and you can leverage it to build data sets for intelligence we can start to dive into it. In the beginning the data will likely look bad, and do not worry about this. This really should just be expected. If you have never had this data before, you were in the dark about the overall state of it all. As one of my favorite books stated, \"Don't Panic!\" You shouldn’t have an expectation that everything you were doing was correct. The data will turn into magic you can leverage down the road to fix all the gaps in your systems and processes. The data will tell you a story of what you are missing. One of the first things I would establish is some baseline expectations. Something simple, like that every full time staff member is required to have a primary asset in your asset tracker. This is a great place to start, and it might require you to change how you approach asset management. Devices you deploy to end users should have explicit definitions of what the asset is, and this is why I think requiring every full time staff member having a primary asset is a good place to start. Secondary devices could be allowed, but the expectation of a secondary device is that it is used less frequently compared to a primary device. If we define these states in our asset tracking systems, it is quite trivial to leverage that data in Snowflake. Establishing Baselines and Context When you think about managing your fleet from an asset and inventory perspective, it helps to define a few things to establish some baselines. This helps you define the data sets which you can ingest into Snowflake. These things also can help define process as well. Here is a table of some examples one could use in their asset tracking system: Status Definition Notes Primary Primary work computer Every staff member must have a primary work computer assigned to them Secondary Secondary work computer Staff may request a secondary computer if they need one Conference Room Devices that are used in conference rooms These devices can be seen more like appliances and not really end user computers Lost Devices that have been lost Lost devices should be marked as lost and follow the lost device process Stolen Devices that have been stolen Stolen devices should be marked as stolen and follow the stolen device process In-Transit Devices that are being shipped or received Devices that are in the process of being shipped Out for Repairs Devices that have hardware failures Any device that needs hardware repair should be marked as this Testing Devices that are used for testing Test devices are often in weird and non-compliant states use this state for devices you want to filter from compliance. Think like a laptop running beta software. Test devices should never be used as a production device Retired Devices that are no longer being used by anyone Using a “Retired” status means you can track the historical data and filter it Disposed Devices that have been officially eWasted This state could imply you have disposed your old hardware with a certification of the data being destroyed With the example table above, we can now use these data points to make much safer assumptions. We can assume that actual end user computes will only ever be either Primary or Secondary statuses, and we can leverage that in Snowflake. While Conference Room devices we can safely assume those devices are only used for hardware needs in the conference rooms. The Testing status I think is also important to track. As an IT \u0026 Ops person myself, I often have a few tester laptops in my possession. These laptops run beta versions of macOS, Windows 10, various tools and applications in preview or beta release as well. This means my tester laptops are probably never in compliance, and a high probability they are in a non-working state. Test devices aren’t meant to do production work, they are meant to be used to test new ","date":"2021-08-10","objectID":"/posts/ops-data-snowflake-pt3/:2:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part III","uri":"/posts/ops-data-snowflake-pt3/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Putting the Data Together With Snowflake, we can use the power of SQL, and execute queries with joins and leverage data sharing features. Let’s look at a query example: SELECT JI.WEBHOOK:eventTimestamp::varchar::TIMESTAMP_LTZ as last_contact , JI.EVENT:computer:serialNumber::varchar as j_serial from \"DB\".\"SCHEMA\".\"JAMF_CHECKIN\" as JI WHERE TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(days, -30, current_timestamp()) QUALIFY 1 = ROW_NUMBER() OVER (PARTITION BY JI.EVENT:computer:serialNumber::varchar ORDER BY last_contact DESC) and not exists( select upper(SN.SERIAL_NUMBER)::varchar as sn_serial from \"DB\".\"SCHEMA.SERVICENOW_ASSETS_V as SN where sn_serial = JI.EVENT:computer:serialNumber::varchar ) The above query is a method we leverage to find devices that have somehow failed to get into our asset tracker. We all know failures happen, and if we can catch them and fix them, then typically everything is all good. When we do not catch them, and then an incident happens weeks or months later, it does not reflect well on IT. A lot of times adding an asset into your asset tracker is a manual process, or if there is automation with a reseller/vendor, sometimes that fails, or they make a mistake. I have observed all these edge cases in the real world be actual real failures. MDM webhooks will ship to Snowflake each time the device phones home, so this is where event based data is amazing to have. You don’t have to wait for workflows or code to run, just simply wait until the next time the device checks into MDM, and the event will be shipped to your data platform. Visualization Example: The data isn’t just valuable for insights and metrics, it is extremely valuable to help you find gaps in your tech stacks. Typically, everyone is happy when you are proactive, and this is exactly what we are doing in IT/Ops with data in Snowflake. ","date":"2021-08-10","objectID":"/posts/ops-data-snowflake-pt3/:2:1","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part III","uri":"/posts/ops-data-snowflake-pt3/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Reducing the Noise Once you put in some effort into cleaning up your asset tracking systems, and using the data to help guide you, you will find out the return on this investment is quite large. Using the examples from the table above, I now know that actual employees will only ever have a primary, or a secondary computer. Put in the effort up front to improve and sanitize your data inputs, and then reap the benefits downstream when your data gets ingested. Filtering Example select SN.substatus as type , JI.EVENT:emailAddress::varchar as USER , JI.EVENT:osVersion::varchar as OS_VERS , JI.EVENT:serialNumber::varchar as SERIAL_NUMBER , IFF(JI.EVENT:emailAddress::varchar = 'service-acount@company.com' AND JI.EVENT:department::varchar = '', 'conference room', JI.EVENT:department::varchar) as department , WEBHOOK:eventTimestamp::varchar::timestamp_ltz as event_date FROM \"DB\".\"SCHEMA\".\"JAMF_INVENTORY\" as JI inner join \"DB\".\"SCHEMA\".\"SERVICENOW_ASSETS_V\" as SN on SN.SERIAL_NUMBER = JI.EVENT:serialNumber where type in ('Primary', 'Secondary', 'Conference Room') qualify 1 = row_number() over (partition by serial_number order by event_date desc) ; In the above query, we can now reap all the benefits of using data to fix your asset tracking system and get actual real numbers of devices by type and context. In IT Operations, you are typically responsible for patching the OS. SLAs are also attached to patching the OS many times. So, we know that IT must patch all primary, secondary, and conference room computers. These are the computers that have been defined contextually as in use and in production. MDM solutions have no knowledge at all of your asset systems, and your MDM doesn’t understand that perhaps not all devices are actually used in production workflows. What we are accomplishing here is the following: Devices submit inventory to MDM MDM will ship an inventory webhook when the device submits data to MDM Join that data to Service Now to ensure it is a production use device Note: There is also a bit more going on here. A lot of times you might use a service account for a conference room computer, or just a computer that is not used by an actual human. Service accounts don’t have departments, or belong to Orgs/teams really. So, to mark this in our visualization if you detect the service account and the department info is blank then assign that device as a Conference Room computer as the department ","date":"2021-08-10","objectID":"/posts/ops-data-snowflake-pt3/:2:2","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part III","uri":"/posts/ops-data-snowflake-pt3/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Tracking Broken MDM Clients and Usage Everything has a failure rate in technology. Nothing runs at 100%, even the most highly available and scalable systems humans have ever created cannot guarantee you a 100% uptime. Compound this with secondary devices it adds complexity to the situation. People sometimes have legit business reasons to have more than one computer, and IT should help the people at your Org be as successful as possible. So, how do you track primary and secondary device usage? How would you track when a device stops phoning home to MDM and needs a re-enroll? What if I told you that you could accomplish both of these problems with a single query and a dash of data sharing? with latest_jamf_data as ( SELECT SN.TYPE , SN.SN_EMAIL as email , upper(SN.SERIAL_NUMBER) sn_serial , JI.EVENT:serialNumber::varchar ji_serial , JI.WEBHOOK:eventTimestamp::string::TIMESTAMP_LTZ as ji_last_inv , JI.EVENT:emailAddress::varchar as JI_EMAIL , HR.DEPARTMENT as DEPARTMENT , HR.LAST_DAY_OF_WORK FROM DB.SCHEMA.JAMF_INVENTORY as JI INNER JOIN ( SELECT upper(SERIAL_NUMBER) as SERIAL_NUMBER , SUBSTATUS as type , EMAIL as SN_EMAIL FROM DB.SCHEMA.SERVICENOW_ASSETS_V Where type = 'primary' ) as SN on JI.EVENT:serialNumber::varchar = SN.SERIAL_NUMBER inner join DB.SCHEMA.HR_DATA as HR on SN.SN_EMAIL = HR.PRIMARY_WORK_EMAIL where LAST_DAY_OF_WORK is Null ) SELECT * FROM latest_jamf_data QUALIFY 1 = ROW_NUMBER() over (PARTITION by ji_serial order by ji_last_inv DESC) AND ji_last_inv \u003c dateadd(days, -30, current_timestamp()) ; NOTE: This query is one of several where we made separate dashboard tiles for each status in Service Now. So, you could edit this query to add more states and have a different visual effect if desired. The above query does a few things. It pulls in our MDM data, joins that to service now to ensure we are looking at the devices we want to look at, and then goes even further and joins to HR data to ensure the devices is from an employee that is still actively employed. Why filter out exited employees? So, my philosophy is you never throw away data, but filtering data into specific buckets makes a lot of sense. It makes the data easy to digest and visualize as well. So, while I am only querying devices that are from actively employed employees at my Org, I am also filtering out all exited employees in an entire different set of tiles and data sets. Visual Example Another thing we leveraged from this data, is that it proactively helps us check back in with folks that have secondary devices. Many times an employee just needs a test device to test out a few things for a short period of time. Tracking this data in Snowflake allows us in IT to check back in when the device stops checking in to see if the person still needs it. Many times they are actually done using the device for their secondary purposes and will gladly turn it back into IT. Using Webhook Event data from our MDM tools, along with Service Now data, has allowed us to remediate when things break. It allows us to know very quickly when something is missed. Without a centralized data platform with data sharing I am not sure how one would accomplish this without putting massive amounts of effort and custom dev work into it. Concluding Part III During my data journey so far in IT, I have learned a lot, changed a lot of my views, and used the data to change things with in my organization and collaborate with other teams. Having solid asset data is important for not only audit purposes, but also for it being available to be leveraged. Tracking the failures and gaps we have with the data in Snowflake is invaluable, as it allows us to be proactive and fix data in our asset tracker all the time. The better our data is in our asset tracking systems, the better it is for everyone to use and leverage. It helps cut out the extra noise and enables your Org to focus on the specific problems at hand. I think this will probably conclude my miniseries of the beginning of my data jo","date":"2021-08-10","objectID":"/posts/ops-data-snowflake-pt3/:2:3","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part III","uri":"/posts/ops-data-snowflake-pt3/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Working with Data in Snowflake Welcome to Part II of my miniseries blog around Snowflake with IT \u0026 Ops Data. If you have not read Part I, you may click that link to read it first. Data has always been very important, and Snowflake just makes using the data a ton easier than we have ever had before. This post will focus on some basic ways one can work with data with in Snowflake. For this blog I will be focusing on JSON data stored in Snowflake columns. JSON documents are very common all over tech. Especially in things like REST APIs, so if you haven’t worked with JSON you will work with JSON data most likely at some point. Snowflake can store semi-structured data in columns natively using the variant data type. This allows IT and Operations people to ingest JSON data in their data pipelines without having to transform the data beforehand. Just ship the JSON data as is right to the platform. IT and Operations folks have also most likely dealt with some sort of SQL based database at some point in their career. Things like: MySQL, Postgres, Oracle, Microsoft SQL, or another SQL based database. So, you might already be familiar enough with SQL. This is another great thing about Snowflake is that you can use all the SQL experience you have gained over the years and apply it right to the platform. There is no esoteric proprietary query language used here, just simply SQL. The best part as an IT \u0026 Ops professional is that Snowflake uses the power and scale of the cloud, meaning IT \u0026 Ops folks don’t ever have to worry about managing the service and platform. No knobs to adjust, no database configurations to deploy, so all you have to do is use the product and let Snowflake take all of that heavy lifting off of your plate! ","date":"2021-04-22","objectID":"/posts/ops-data-snowflake-pt2/:0:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part II","uri":"/posts/ops-data-snowflake-pt2/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Jamf Webhooks Right to Snowflake Tables Since Jamf provides example Webhook Data in their online documentation, we will just use their examples in Snowflake for this blog post. Snowflake can parse JSON natively, meaning you can store JSON data in raw form right in your database. So, lets get started! Create A table with JSON Data Let’s create a table first, so we can insert some data into it: Query: CREATE OR REPLACE TABLE DB.TESTING.JAMF_EVENTS_RAW( date varchar, json_data variant ); the above table has two columns. A “date” column which is set to a varchar data type, json_data which is set to a variant type since JSON is a semi-structured data type. I added raw into the table name to represent a table that might receive raw data from a data ingest pipeline. An empty table is no good without any data in it. I am just copy/pasting the actual example data from Jamf’s developer documentation site linked above. Query: INSERT INTO JAMF_EVENTS_RAW(DATE, JSON_DATA) SELECT current_timestamp(), parse_json($${ \"event\": { \"computer\": { \"alternateMacAddress\": \"72:00:01:DD:A0:B9\", \"building\": \"Block D\", \"department\": \"Information Technology\", \"deviceName\": \"John's MacBook Pro\", \"emailAddress\": \"john.smith@company.com\", \"jssID\": 13, \"macAddress\": \"60:03:08:A3:64:9D\", \"model\": \"13-inch Retina MacBook Pro (Late 2013)\", \"osBuild\": \"16G29\", \"osVersion\": \"10.12.6\", \"phone\": \"555-472-9829\", \"position\": \"Desktop Services Specialist\", \"realName\": \"John Smith\", \"room\": \"487\", \"serialNumber\": \"C02M23PJFH50\", \"udid\": \"EBBFF74D-C6B7-5599-93A9-19E8BDDEFE32\", \"userDirectoryID\": \"-1\", \"username\": \"john.smith\" }, \"trigger\": \"CLIENT_CHECKIN\", \"username\": \"John Smith\" }, \"webhook\": { \"eventTimestamp\": 1553550275590, \"id\": 7, \"name\": \"Webhook Documentation\", \"webhookEvent\": \"ComputerCheckIn\" } }$$); I repeated this process a few times to get several rows of data. I just picked a few random examples of the hooks from Jamf’s website and reran the above query. Snowflake has a built in parse_json function that will interpret the string as JSON data, and produce it as a variant data type Let’s check our work but running a SELECT * on our table and see what sort of data we get back. Query: SELECT * FROM JAMF_EVENTS_RAW; Screenshot of rows: Screenshot of some JSON data: Parsing the Jamf Data Now that we have some data in there, lets look at how one could get some data out of it: Query: SELECT JSON_DATA:event.computer.serialNumber as SERIAL_NUMBER , JSON_DATA:event.computer.osVersion as OS FROM JAMF_EVENTS_RAW WHERE JSON_DATA:webhook.webhookEvent = 'ComputerPolicyFinished' ; Results: SERIAL_NUMBER OS \"C02M23PJFH50\" \"10.14.3\" In the above query and results you can see that Snowflake can just natively parse raw JSON/Variant data stored in a column. No big deal, right? The syntax is pretty simple. It is simply one single: to tell Snowflake to interpret the rest as JSON/Variant. After that first : you can swap back to . and, you don’t have to hold down the shift key a ton. It is really that simple and at the same time powerful. ","date":"2021-04-22","objectID":"/posts/ops-data-snowflake-pt2/:1:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part II","uri":"/posts/ops-data-snowflake-pt2/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Organizing Your Data and Leveraging Views With data like Jamf Pro Webhooks, you probably want to organize your data in a way where each event type is in its own table. We will first create a new table, just like our previous one, and this time we will use it for check-in hook events from Jamf Pro. It would not make a ton of sense to toss two or more different types of hook data sets in the same table. Event data is about capturing what events are taking place and mixing two different event types has no benefit and would make your queries way more complex. Since we already have our table from before that worked, we can use like to create another table just like it. Query: CREATE TABLE DB.TESTING.JAMF_EVENTS_CHECKINS like DB.TESTING.JAMF_EVENTS_RAW ; Following the same steps previously mentioned in this blog post, I inserted some check-in webhook events into my new table I just created using the example data from Jamf’s website. Now that the raw JSON example data is in a fresh table I can now look at creating a view. For more information please check out Snowflake’s Documentation on views. I will only share a few pieces of the data in my view that might be relevant to the requester of said data. Data owners can always pick and choose what they share with other people and teams using Snowflake, and this is such a fantastic thing! Query: CREATE OR REPLACE VIEW JAMF_EVENTS_CHECKINS_V AS SELECT JSON_DATA:\"event\":\"computer\":\"building\"::VARCHAR AS BUILDING ,JSON_DATA:\"event\":\"computer\":\"department\"::VARCHAR AS DEPARTMENT ,JSON_DATA:\"event\":\"computer\":\"emailAddress\"::VARCHAR AS EMAIL ,JSON_DATA:\"event\":\"computer\":\"realName\"::VARCHAR AS FULL_NAME ,JSON_DATA:\"webhook\":\"eventTimestamp\"::VARCHAR::TIMESTAMP_LTZ AS TIME FROM DB.TESTING.JAMF_EVENTS_CHECKINS ; NOTE: Jamf Pro Webhooks use epoch timestamps, so I decided to type cast it as more readable time stamp. This is another great quality feature of Snowflake. See the screenshot and query results below, it displays a human-readable time stamp now. This query also is only sharing a relevant data set to the view, and not the entire raw data of the hook itself. I also added a _v to the view name to visually indicate it is a view. The short version is that a view can turn query results into a table. This is a very cool feature, and useful to just convert queries you write into a view. I am also specifying data types here, please refer to the official docs to learn more about data types in Snowflake. Let’s take a look at the data set we just created with a view. Query: SELECT * FROM DB.TESTING.JAMF_EVENTS_CHECKINS_V; Results: BUILDING DEPARTMENT EMAIL FULL_NAME TIME Block D Information Technology john.smith@company.com John Smith 2019-03-25 14:44:35.590 -0700 Block D Information Technology john.smith@company.com John Smith 2019-03-25 14:44:35.590 -0700 Block D Information Technology jane.smith@company.com Jane Smith 2019-03-25 14:44:35.590 -0700 Block D Information Technology jane.smith@company.com Jane Smith 2019-03-25 14:44:35.590 -0700 Block D Information Technology jane.smith@company.com Jane Smith 2019-03-25 14:44:35.590 -0700 Block D Information Technology jane.smith@company.com Jane Smith 2019-03-25 14:44:35.590 -0700 Apple Park Executive tim.cook@company.com Tim Cook 2019-03-25 14:54:35.590 -0700 Apple Park Executive tim.cook@company.com Tim Cook 2019-03-25 14:54:35.590 -0700 Apple Park Executive tim.cook@company.com Tim Cook 2019-03-25 14:54:35.590 -0700 Features like this come in very handy when you want to share data to other people and teams. The view above is a view that just shows some basic data for the webhook event of client check-in. When sharing data to others, they probably do not need to access the raw data. Parts of the data could be confusing, not needed, or even considered confidential. Features like this give the data owners the power to share exactly what they want, nothing more, nothing less. ","date":"2021-04-22","objectID":"/posts/ops-data-snowflake-pt2/:2:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part II","uri":"/posts/ops-data-snowflake-pt2/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Data Sharing with Snowflake Data sharing is such a game changer. It is hard to describe in some ways, because I feel that you have to experience it to really see the absolute beauty that data sharing is. In the past I have had to deal with many APIs, CSV reports, and in some cases I had to log into an app to generate a report and download it. This is time-consuming, and very labor intensive. The end result was typically a bunch of scripts, emails with attachments, and then me having to cobble together data from these sources and documents. I never have to do any of that ever again. That is a thing of the past, a relic, things lost in time like tears in the rain. A quick high level example of how data sharing empowers IT \u0026 Ops workers could be around inventory and asset control. An IT \u0026 Ops professional could have access to basic HR data (think employee directory), asset data in the Org’s asset tracker, and then MDM data from the MDM tool you use. All of which is directly ingested into Snowflake. Zero API calls, zero CSV parsing, and definitely not logging into random apps to run reports manually. With this simple data set I can check if a terminated employee’s laptop is checking into MDM, or if the asset in the asset tracker is not set to some sort of term hold state. One could also ensure that every FTE of the Org has a primary use computer assigned to them by simply joining the HR data to the asset tracker data, and then filtering for every full time employee that is missing a primary use computer. Role Based Access Controls (RBAC) Role Based Access is an industry standard that is used in many things. The general idea is you create roles to do certain functions, and only grant those roles the least amount of permissions and access they need to do their jobs. A user of a system will assume that role when they authenticate to the tech stack they are working in. Here are the official docs for how Snowflake handles access and ownership of objects with in Snowflake itself. Using part of the example above, lets assume Security Engineering has a request into IT to consume data around end user computers checking into the Org’s MDM. For the use case we will also use that they are trying to track employee laptops where the employee has left the company, but the laptop is still acitvely online and checking into MDM. This would be considered a risk the Security Team wants to monitor and alert on. Let’s create a role called SECENG_BFF, since IT and Security should be best friends forever. Grant that role SELECT access to the view we just created before. This will be the view we are data sharing to Security Engineering, so they may audit and monitor data in the scenario described. Query: CREATE ROLE SECENG_BFF; GRANT USAGE ON WAREHOUSE DB_WH TO ROLE SECENG_BFF; GRANT USAGE ON DATABASE DB TO ROLE SECENG_BFF; GRANT SELECT ON VIEW DB.TESTING.JAMF_EVENTS_CHECKINS_V TO ROLE SECENG_BFF; GRANT ROLE SECENG_BFF TO USER \"USER.NAME@ORG.COM\"; DANGER WARNING - this is in no way an example of a best practice when it comes to granting access to data. Please consult your security teams, and the official documentation to ensure you are using the proper security and access controls. Roles do need some level of basic access to warehouses and databases to access views and tables with in those objects. This example is not advice on how anyone should organize and use role based access. However, now we can test our role to see if it can SELECT against the view we just created. USE ROLE SECENG_BFF; SELECT * FROM DB.TESTING.JAMF_EVENTS_CHECKINS_V; If you get a return on the data, then you have now successfully created a view based off raw data, created a role to access that data, and finally granted a person that role. Folks working with in Snowflake can assume roles through the App GUI, or by running a query USE ROLE \u003cROLE_NAME\u003e, and users can have multiple roles. My SELECT * query ran fine, and returned all the same results in the example earlier in this blog p","date":"2021-04-22","objectID":"/posts/ops-data-snowflake-pt2/:3:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part II","uri":"/posts/ops-data-snowflake-pt2/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Data Sharing is Limitless In this post I have only really touched on internal data sharing between teams with in your Organization. Snowflake also has an entire platform for third party data sharing, called the Snowflake Data Marketplace IT and Operations teams now have the ability to directly share and directly consume data from with in your Organization. This enables people to access and consume data freely with your Org once granted access. Often IT \u0026 Ops teams need to share data with Security, and Compliance and Governance teams. Data sharing makes this extremely easy. It also allows data owners to share only the exact data they need to share, and scope the data to specific teams or people. IT \u0026 Ops teams can also easily consume data from other teams as well, and this is such a wonderful thing. When you share the same data across teams, then all teams are looking at the same data. Historically, you might have had an experience where data is not centralized and shared from the same data sources, and ended up with different teams having different data sets. When you have the ability to ship all your raw data to a single data platform and share your data with people and teams across your Org, you get rid of all those legacy methods you used to have to deal with. Concluding Part II Thanks for reading if you got this far. This concludes Part II of my miniseries on being data driven with Snowflake. The thing I would like to close on is that Snowflake makes data ingest extremely easy. Most APIs or data shippers can ship JSON data natively, and Snowflake can just consume that natively. No need to transform your data in the shipping and ingest process. Then having the ability to just work with variant data with in Snowflake is honestly just sort of mind-blowing. This also means the integration opportunity for IT \u0026 Ops tools is exponentially broad. If you can get IT \u0026 Ops JSON data into cloud storage you are pretty much just done at that point. The rest is using the data in Snowflake itself. There will be a Part III, but I have not fully planned out what that will be just yet, so stay tuned please. ","date":"2021-04-22","objectID":"/posts/ops-data-snowflake-pt2/:4:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part II","uri":"/posts/ops-data-snowflake-pt2/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Be Data Driven in IT/Ops with Snowflake Technology changes very fast, and it keeps getting bigger and more complex. We have so many systems these days, our systems have systems. It doesn’t have to be complex and daunting to get reliable, and up-to-date data. Getting accurate data, while getting it fast, is key to developing data driven strategies. We no longer have to live in the constraints of capacity, limited storage, and limited scalability. With the power and scale of the cloud, you can now simply just ship all of your IT/Ops data to a centralized location and share it among your teams. IT/Ops shops often find some sort of niche tool that was designed for their purpose. These tools are fine, but they are often complex, and require specialized knowledge. Then sometimes you are faced with figuring out what data to ship. For example, some data tools might transform the data during the shipping process. Breaking it out into say a JSON doc, and then creating indices for these files, and tagging them with metadata. This made it even more complex as you would have to design all of these things up front, or suffer the rite of passage of remodeling your data ingest and re-indexing your data when you need to change something. Snowflake has solved these problems in a pretty straight forward way. With Snowflake, you just ship all your data. You can then use post processing to make the data more actionable. Ship all the raw data you can, and then store that raw data in a schema with in the Snowflake platform. No need to spend countless hours of labor on this. You also do not have to worry about indexing, or tagging your data. You can just post process all your raw data with in Snowflake. ","date":"2021-04-15","objectID":"/posts/ops-data-snowflake/:0:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part I","uri":"/posts/ops-data-snowflake/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Before you Begin When you get to the point of having multiple data pipelines into a data tool which you can easily share data from multiple sources of truth, you must convince yourself of the following: You will probably be wrong The data will show you when you are wrong (when you are right too!) Use the data to fix what is wrong Fix things upstream, so next data ingest it will be fixed downstream Establish baselines, sources of truth and context to your data Data with out a source of turth or context is sometimes not as valuable I really cannot stress this enough. I had to admit some methods I had been using for years, which I thought to be pretty solid, were in fact wrong. I also stopped treating MDM data as a single source of truth. When you think about it, this makes actual logical sense. MDM admins often only have the MDM tools to work with though, so they build deep knowledge and skills around these tools. So much, that it gives them Tunnel Vision The big lesson learned here is that the MDM collecting third party agent data is at best a nice to have, but the actual source of truth is cloud data from the agent’s SaaS. That is what matters the most in context. Not if some Launch Daemon is running, or an agent binary can return stdout, or the plethora of other tricks us Mac Admins have come up with over the years to verify if local agents are actually healthy. I found out through collecting data that methods I had been using for years really were not as effective as I thought they were. I discovered I was wrong. To define a healthy agent, that agent has to submit data back to its cloud tenant, and if the agent is not submitting data, then it does not matter what the local state data says. So, now I have set my source of truth to the App stack each agent talks to, and the context of the data is around when was the last time that agent phoned home and submitted data? The rest of the data are just nice to haves, and sometimes that data is not as useful as we thought it was. All I had to do was build my context and request that set of data to the teams that owned that data, and they shared that data to me directly. Now I have the ability to query agent connection statuses from the actual source of truth of the agent, and I am not relying on MDM data to detect these failures for me. All queries and data here are samples from actual data. While they are small sample sets, and things have been changed to share in this blog, the concepts are very real world applicable. ","date":"2021-04-15","objectID":"/posts/ops-data-snowflake/:1:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part I","uri":"/posts/ops-data-snowflake/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"What Data Should I ship? If you are new to shipping data, and aren’t quite sure what to ship, here are some high level starting points I recommend: Asset Data (hardware, software, applications, etc) Event Data (webhooks, event based APIs, etc) Log Data (any logs that you need info on, or just ship all the logs) Tools Data (MDM, security agents, etc) ","date":"2021-04-15","objectID":"/posts/ops-data-snowflake/:2:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part I","uri":"/posts/ops-data-snowflake/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Use Asset Tracking Systems as a Source of Truth At my current work, we have Service Now as our asset management system. We track what Org assets are assigned to what human in this system. Also, we spent lots of effort cleaning up our sub statuses of assets in Service Now. Things like classifying systems as primary, or as secondary. We have also classified assets that are designated as Zoom Room computers. Leveraging data like this helps a ton on things you haven’t even planned for yet. Anything you add upstream in Service Now, will be ingested downstream into Snowflake later on. So, we use Service Now as our source of truth for asset tracking. One example is that we query to check if primary assets are checking into MDM in the last 30 days. We chose 30 days as the metric for this simply because employees take time off. Plus there are holidays, and many other reasons an employee would not be working. However, it is probably extremely rare for an employee to be out of the office and offline for over 30 days. Example Query: Primary Assets from Intune that have not checked in for over 30 days // get latest compliance log from Intune on Win10 clients over 30 days SELECT SN.SUBSTATUS as type , SN.EMAIL as email , SN.SERIAL_NUMBER sn_serial , IM.JSON_DATA:properties.SerialNumber::varchar im_serial , IM.JSON_DATA:properties.LastContact::varchar as im_last_inv FROM DB.SCHEMA.SERVICENOW_ASSETS as SN JOIN DB.SCHEMA.INTUNE_DEVICECOMPLIANCE_LOGS as IM on SN.SERIAL_NUMBER = IM.JSON_DATA:properties.SerialNumber Where type = 'primary' QUALIFY 1 = ROW_NUMBER() OVER (PARTITION BY im_serial ORDER BY im_last_inv DESC) AND TO_TIMESTAMP(IM.JSON_DATA:properties.LastContact) \u003c dateadd(days, -30, current_timestamp()) ; Example Query: Primary Assets from Jamf that have not checked in over 30 days // get lastest inventory webhook from jamf on clients over 30 days SELECT SN.SUBSTATUS as type , SN.EMAIL as email , upper(SN.SERIAL_NUMBER) sn_serial , JI.EVENT:serialNumber::varchar ji_serial , JI.WEBHOOK:eventTimestamp::string::TIMESTAMP_LTZ as ji_last_inv FROM DB.SCHEMA.SERVICENOW_ASSETS as SN INNER JOIN DB.SCHEMA.JAMF_INVENTORY as JI on SN.SERIAL_NUMBER = JI.EVENT:serialNumber Where type = 'primary' QUALIFY 1 = ROW_NUMBER() OVER (PARTITION BY ji_serial ORDER BY ji_last_inv DESC) AND TO_TIMESTAMP(JI.WEBHOOK:eventTimestamp::INTEGER/1000) \u003c dateadd(days, -30, current_timestamp()) ; Discoveries from the Data All software agents can break, MDM clients can get into weird states, and even things like an OS update can cause issues. A lot of times you just need to reboot the computer to fix these too. This is just an easy way to report on them and make the data actionable. We are also using Service Now as the source of truth here and not our MDM services. This is really just because MDM solutions aren’t normally true asset management systems. Use the right tool for the right job. Visualization Example: ","date":"2021-04-15","objectID":"/posts/ops-data-snowflake/:3:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part I","uri":"/posts/ops-data-snowflake/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"What If Systems Are Missing From the Asset Tracker? If you are an Org that allows BYOD, you might not have a great way to tell what devices are BYOD. Devices sometimes also may not end up in your asset system for whatever reason. With having multiple data sources in Snowflake you can easily join tables from these sources to craft such data sets. BYOD systems are probably not in your asset tracker as they are not Org owned. A failure also may occur in your process to import assets, or data was entered wrong. This is also pretty easy to solve when you have access to the proper data. Example Query: Devices Checking into MDM but not present in Service Now SELECT JI.WEBHOOK:eventTimestamp::varchar::TIMESTAMP_LTZ as last_contact , JI.EVENT:computer:serialNumber::varchar as j_serial from DB.SCHEMA.JAMF_CHECKIN as JI WHERE TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(days, -30, current_timestamp()) QUALIFY 1 = ROW_NUMBER() OVER (PARTITION BY JI.EVENT:computer:serialNumber::varchar ORDER BY last_contact DESC) and not exists( select upper(SN.SERIAL_NUMBER)::varchar as sn_serial from DB.SCHEMA.SERVICENOW_ASSETS as SN where sn_serial = JI.EVENT:computer:serialNumber::varchar ) ; In the above query we are using each unique device’s last check-in into Jamf from the Webhook Event in the past 30 days, and we only need the event data for this. Then if we cannot join that unique ID (serial number in this case) to the Service Now assets table, then it does not exist in Service Now. You also now have all the serial numbers which you can use to create those asset objects in your asset tracker. Visualization Example: Discoveries from the Data What we discovered is that most of the time when a device does not show up in Service Now, there is likely some sort of data entry issue. Either our automation with our resellers had a glitch, or perhaps a human mistyped a character in for an asset that was manually entered. We also discovered that sometimes if you scan a serial number barcode off of the box the laptop was shipped in, it will append the beginning of the string with a S. On rare occasion we just missed getting it in our asset tracker. My favorite discovery though by far with this data point, was that if a human enters a serial number in Service Now with all lower case characters, it will fail on the table join. The matching is case-sensitive. To fix this, I simply now sanitize my data in post processing by converting all serial numbers to all upper case with the upper() function. This goes to show you how powerful this data platform is. Sure, we could have fixed our data upstream, but will you ever be able to really stop humans from making simple human mistakes? Having this data allowed us to drill down into each edge case and failure point and discover why it was happening. With the best part being is that we have the data to fix it! ","date":"2021-04-15","objectID":"/posts/ops-data-snowflake/:4:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part I","uri":"/posts/ops-data-snowflake/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Using Event Based Data to Monitor Jamf Cloud SaaS is great! I love SaaS Apps, because as an IT/Ops person that means all the management of the App is on the vendor. I can focus on other things, and not have to worry about the app itself, nor the infrastructure required to self host the App. One of the very few downsides of using SaaS is that you often lack access to data. This is especially true when looking at operational data, or access to data that requires direct access to the host. In this case we will be using event based data shipped from Jamf Cloud Webhooks. Example Query: Total Number of Events every hour SELECT ( SELECT COUNT(*) FROM DB.SCHEMA.JAMF_APIOPS where TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(hours, -1, current_timestamp()) ) as APIOPS, ( SELECT COUNT(*) FROM DB.SCHEMA.JAMF_CHECKIN where TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(hours, -1, current_timestamp()) ) as CHECKINS, ( SELECT COUNT(*) FROM DB.SCHEMA.JAMF_INVENTORY where TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(hours, -1, current_timestamp()) ) as INVENTORIES, ( SELECT COUNT(*) FROM DB.SCHEMA.JAMF_POLICIES where TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(hours, -1, current_timestamp()) ) as POLICIES, ( SELECT COUNT(*) FROM DB.SCHEMA.JAMF_CHECKIN where TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(hours, -1, current_timestamp()) AND EVENT:trigger = 'enrollmentComplete' ) as ENROLLMENT ; In the above query I am tracking the total count of each event type by the common events we want to track from our Jamf Cloud instance. The above query also is tracking the number of events per the last hour, and you could adjust this part of the query dateadd(hours, -1, current_timestamp()) to expand the time threshold. Example Query: Total API Operations in last 24 hours select count(*) as total , EVENT:authorizedUsername::varchar as usr , EVENT:operationSuccessful as state from DB.SCHEMA.JAMF_APIOPS where TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(days, -1, current_timestamp()) group by state, usr ; Caveat - Come to find out Jamf does not ship event hooks on failed API operations unfortunately. I have brought this to their attention and asked for this as a feature request. In this case I think tracking failures is more important than the successful operations. Visualization Example: Discoveries from the Data This data is useful to monitor our Jamf Cloud instance. If events start to spike heavily in either an uptrend, or a downtrend pattern, something is probably wrong. I did use this data to detect an impacted workflow from an API bug a year or so back. Basically, an API call from JSS-Importer had failed updating a smart group. The actual failure (after tracking it down) was that the Jamf API had timed out and returned a 502. However, it actually did complete the PUT to the smart group, but failed to update the package payload of a policy. This resulted in a policy to install one version of an app, and a smart group based on a different version of the same app. I only caught this because my fleet was constantly spamming my Jamf Cloud with over 80,000 recon in a 24-hour period. ","date":"2021-04-15","objectID":"/posts/ops-data-snowflake/:5:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part I","uri":"/posts/ops-data-snowflake/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Security and Compliance Data Every Org has a responsibility to their stakeholders, employees (or staff/students), and definitely their customers around security. This is a very big and encompassing subject. One could write entire books on this subject alone. So, for this blog post I am going to focus on disk encryption. Which is a part of the CIS framework, and a very common requirement. Example Query: FV2 Status from Jamf SELECT JI.EMAIL , JI.SERIAL_NUMBER , PART.VALUE:\"filevault2_status\"::VARCHAR AS FV2_STATUS , JI.GENERAL:\"last_contact_time_epoch\"::string::TIMESTAMP_LTZ AS jamf_last_contact FROM DB.SCHEMA.JAMF_INVENTORY JI ,LATERAL FLATTEN(JI.HARDWARE:\"storage\") AS VOL ,LATERAL FLATTEN(VOL.VALUE:\"partitions\") AS PART WHERE PART.VALUE:\"type\"::VARCHAR = 'boot' AND EMAIL != 'exclusion-user@company.com' AND jamf_last_contact \u003e dateadd(day, -30, current_timestamp()) QUALIFY 1 = ROW_NUMBER() OVER (PARTITION BY JI.SERIAL_NUMBER ORDER BY jamf_last_contact DESC) ; Example Query: FV2 Overall Score from Jamf WITH X AS ( SELECT JI.EMAIL , JI.SERIAL_NUMBER , PART.VALUE:\"filevault2_status\"::VARCHAR AS FV2_STATUS , JI.GENERAL:\"last_contact_time_epoch\"::string::TIMESTAMP_LTZ AS jamf_last_contact FROM DB.SCHEMA.JAMF_INVENTORY JI ,LATERAL FLATTEN(JI.HARDWARE:\"storage\") AS VOL ,LATERAL FLATTEN(VOL.VALUE:\"partitions\") AS PART WHERE PART.VALUE:\"type\"::VARCHAR = 'boot' AND EMAIL != 'exclusion-user@company.com' AND jamf_last_contact \u003e dateadd(day, -30, current_timestamp()) QUALIFY 1 = ROW_NUMBER() OVER (PARTITION BY JI.SERIAL_NUMBER ORDER BY jamf_last_contact DESC) ) SELECT COUNT_IF(FV2_STATUS != 'Not Encrypted') / NULLIFZERO(COUNT(*)) AS PERCENTAGE FROM X ; This will return the current FV2 status for every device record that has checked into Jamf in the last 30 days. Devices that have not submitted inventory or checked into Jamf in greater than 30 days go into a separate violation, and will be remediated in a separate flow. Any device returning Pending, Encrypting, and Encrypted are considered in a desired state, and Not Encrypted will be used to calculate the percentage of devices in an undesired state. NOTE: These queries do include an example of how to exclude say a service account that is used on non-human used devices that may not require full disk encryption for their specific use case. Discoveries from the Data This data was one of the easiest ones to address. Jamf will report 4 different states of disk encryption from a device record. They include: Encrypted, Encrypting, Pending, Not Encrypted, and each of these is very self-explanatory. In this data point Not Encrypted is the violation we are looking for. If devices stay in Pending or in Encrypting for long periods of time, then we consider that a violation. The failure point is almost always FV2 was deployed, but the user never rebooted, or in rare instance the MDM just failed to fully enable it. ","date":"2021-04-15","objectID":"/posts/ops-data-snowflake/:6:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part I","uri":"/posts/ops-data-snowflake/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Closing of Part I This is still the beginning of my data journey, and I have much to learn and do still. What I would like to close with, is that having all these data points shared to me and my teams, has really changed the game. This data ends debates, answers questions, shows area of improvement, and is actionable. The last thing I would like to point out is that data drives collaboration. I partner with our security teams here, and we data share to each other. I share our fleet data with them, they share their security and agent data with me. Now when I engage with security, we both look at what the data is telling us, and we collaborate on it. ","date":"2021-04-15","objectID":"/posts/ops-data-snowflake/:7:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part I","uri":"/posts/ops-data-snowflake/"},{"categories":["macOS","scripting"],"content":"Spotlight is a system wide metadata indexing system that Apple has been shipping since OS X 10.4 Tiger, and has been improved over the years with each OS. I like using Spotlight for some tasks and general searching for files in Terminal.app. I also use it for finding anything on my Mac. Typically, I do not use my mouse or trackpad to find files or navigate to folders. One just needs to hit cmd + spacebar on their Mac to pull up the Spotlight search menu and start typing. If you use a Mac, there is a good chance you also use this regularly like I do. This isn’t anything groundbreaking or new, but I have enjoyed using Spotlight over the years when it fits as a good tool. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:0","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Spotlight in the Terminal getting attributes There are two binaries Apple supplies on macOS which you can leverage. They are mdls and mdfind, there is also another tool called xattr which also allows you to manipulate metadata in code. To start using these binaries we can just take a look at an application like say Firefox. Refer to their man pages for the documentation. % mdls /Applications/Firefox.app _kMDItemDisplayNameWithExtensions = \"Firefox.app\" kMDItemAlternateNames = ( \"Firefox.app\" ) kMDItemAppStoreCategory = \"Productivity\" kMDItemAppStoreCategoryType = \"public.app-category.productivity\" kMDItemCFBundleIdentifier = \"org.mozilla.firefox\" kMDItemContentCreationDate = 2020-08-31 18:05:40 +0000 kMDItemContentCreationDate_Ranking = 2020-08-31 00:00:00 +0000 kMDItemContentModificationDate = 2020-09-08 17:37:19 +0000 kMDItemContentModificationDate_Ranking = 2020-09-08 00:00:00 +0000 kMDItemContentType = \"com.apple.application-bundle\" kMDItemContentTypeTree = ( \"com.apple.application-bundle\", \"com.apple.application\", \"public.executable\", \"com.apple.localizable-name-bundle\", \"com.apple.bundle\", \"public.directory\", \"public.item\", \"com.apple.package\" ) kMDItemDateAdded = 2020-09-08 17:35:57 +0000 kMDItemDateAdded_Ranking = 2020-09-08 00:00:00 +0000 kMDItemDisplayName = \"Firefox\" kMDItemDocumentIdentifier = 0 kMDItemExecutableArchitectures = ( \"x86_64\" ) kMDItemFSContentChangeDate = 2020-09-08 17:37:19 +0000 kMDItemFSCreationDate = 2020-08-31 18:05:40 +0000 kMDItemFSCreatorCode = \"\" kMDItemFSFinderFlags = 0 kMDItemFSHasCustomIcon = (null) kMDItemFSInvisible = 0 kMDItemFSIsExtensionHidden = 1 kMDItemFSIsStationery = (null) kMDItemFSLabel = 0 kMDItemFSName = \"Firefox.app\" kMDItemFSNodeCount = 1 kMDItemFSOwnerGroupID = 80 kMDItemFSOwnerUserID = 0 kMDItemFSSize = 210713091 kMDItemFSTypeCode = \"\" kMDItemInterestingDate_Ranking = 2020-09-08 00:00:00 +0000 kMDItemKind = \"Application\" kMDItemLastUsedDate = 2020-09-08 17:37:09 +0000 kMDItemLastUsedDate_Ranking = 2020-09-08 00:00:00 +0000 kMDItemLogicalSize = 210713091 kMDItemPhysicalSize = 211038208 kMDItemUseCount = 1 kMDItemUsedDates = ( \"2020-09-08 07:00:00 +0000\" ) kMDItemVersion = \"80.0.1\" There are many metadata tags we can get right from the shell as well. % mdls /Applications/Firefox.app -name kMDItemVersion kMDItemVersion = \"80.0.1\" You can see this is easy to get the metadata from an object on disk, it is also very fast as it searches the indexes and returns the indexed data. % mdls /Applications/Firefox.app -name kMDItemVersion -raw 80.0.1 There is also no need to pipe to awk or grep as the tool gives you an argument of -raw to just return the data. Easily get multiple metadata attributes by passing multiple arguments. % mdls /Applications/Firefox.app -name kMDItemVersion -name kMDItemFSName -name kMDItemLastUsedDate kMDItemFSName = \"Firefox.app\" kMDItemLastUsedDate = 2020-09-08 17:37:09 +0000 kMDItemVersion = \"80.0.1\" ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:1","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Spotlight in the Terminal searching mdls is great for listing the metadata for objects on disk, and you can probably guess what mdfind is used for. After getting the metadata tags from mdls we can use them in mdfind you can also pass strings to mdfind and it performs searches similar to when you use it in the Finder. % mdfind \"using-spotlight\" /Users/tlarkin/blog/tlark/content/posts/using-spotlight-macos.md /Users/tlarkin/Library/Application Support/JetBrains/IdeaIC2020.1/workspace/1hksXRhYWu8MI2RnUXoH3oitxRW.xml Like finding the blog post I am currently working on, which my IDE is also storing data about on disk. This output makes sense considering I do all my blog work in my IDE with hugo along with built in tools. We can also use the metadata tags which we see in mdls output. % mdfind \"kMDItemFSName = Firefox.app\" /Applications/Firefox.app You can specify file paths if you have an idea or what to limit search scope for metadata searching. % mdfind -onlyin /Users/tlarkin \"kMDItemContentType = public.python-script\" | wc -l 11582 I have a lot of Python scripts on my Mac, so I just did a line count of the output. I have a lot of repos cloned, open source tools cloned, and there are a lot of apps that use Python, so a big chunk of these results are from the software I have installed. However, every object in my home folder that has kMDItemContentType = public.python-script tag will return from this search. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:2","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Using mdfind with mdls Of course with things like pipes in the shell you can easily use these tools together. % mdfind -0 -onlyin /Applications \"kMDItemContentType = com.apple.application-bundle\" | xargs -0 mdls -name kMDItemCFBundleIdentifier -name kMDItemDisplayName -name kMDItemVersion There is a lot of output I won’t paste into a code block here, but you can play around with this to see the different output you can get from these binaries. Last year there was a pretty nasty iTerm2 Vuln that we detected through our security tools, and the fact it was all over Twitter and the rest of the Internet. We wanted to detect how many vulnerable versions we had and then plan on patching after assessing how many vuln versions we had. A simple spotlight script was deployed, and anything that wasn’t the current version got flagged. Since iTerm2 is just a zipped App bundle, we had no idea where the user installed it. We also do not block users from installing their own software and tools, so there could be multiple versions present. Tools like Jamf Pro will not search for Apps outside of a default path unless you specify so in the inventory collection preferences. A quick Spotlight script made sense and it didn’t require much effort. There are tools out there like OSquery which is probably a better answer to get data about your fleet on a regular basis. #!/bin/zsh results=$(mdfind -name \"kMDItemCFBundleIdentifier = com.googlecode.iterm2 \u0026\u0026 kMDItemVersion != 3.3.6\") if [[ \"${results}\" == \"\" ]] then echo \"\u003cresult\u003efalse\u003c/result\u003e\" else echo \"\u003cresult\u003etrue\u003c/result\u003e\" fi With a day we had the data back, and we had very few users with a vulnerable version on their system as most users were updating their versions on their own. So, we just had to send an update to a few systems. I do think there are better ways and better tools to manage application state out there, but if you don’t have those every Mac has Spotlight on it. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:3","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Metadata Tagging None of this is new, and I am certainly not the first person to use these tools or blog about this stuff. The linked blog has some pretty good reading material you can reference. A quick reference how you can do this below. # create a file touch file.txt # add a custom tag xattr -w com.apple.metadata:\"_customTag\" \"customVal1\" file.txt # get the attribute mdls -name _customTag file.txt -raw customVal1 # search for it mdfind \"_customTag = customVal1\" /Users/tlarkin/file.txt There are a lot of things you could do with metadata tagging in macOS. You can programmatically search for metadata tags, as well as set your own tags. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:4","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Caveats with Custom Tagging When tagging things on disk that get overwritten, like contents of an installer package, the tags could be deleted. So, if you were to say tag an app you installed with your automation tools, then a user downloads another version and overwrites it, your tags, are now probably gone. Some file paths are not indexed by Spotlight. Paths like /tmp are not indexed. So, make sure you test the paths you are using if you use these tools. I have definitely forgotten this before and tested custom tags with xattr in /tmp and completely spaced that this doesn’t work. Some file names are error prone with certain characters. Apple, I think has fixed this bug as I can now tag a file located in a folder with a . in the folder name. This was broken at some point in time, Mac Mule discovered it. I was not able to locate his blog post on it, but will update with a link if I can find it. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:5","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Python Objc Bridge You can also do all of these neat things in Python as well. I wrote this script not too long ago to search and find all the Parallels VMs on a Mac. The end goal was to collect the OS versions of all VMs and ship that data to Snowflake, so we could report on them for vulnerabilities on the OS. #!/usr/bin/python \"\"\" this is a script to detect what VMs are on a system and escrow what OS they are You can run this daily or in another workflow It will use Spotlight to find files with the .pvm extension to locate where the files are on the file system then parse the PvInfo file for Parallels VMs credit: https://github.com/munki/munki/blob/b6a4b015297c262124fb80086b6b55e329a4fec0/code/client/munkilib/info.py#L362-L396 \"\"\" # import modules import xml.etree.ElementTree as et from Foundation import NSMetadataQuery, NSPredicate, NSRunLoop, NSDate # start functions def get_vms(): \"\"\"use spotlight to find parallels VM files\"\"\" file_list = [] query = NSMetadataQuery.alloc().init() query.setPredicate_( NSPredicate.predicateWithFormat_( \"(kMDItemContentType == 'com.parallels.vm.vmpackage')\" ) ) query.setSearchScopes_([\"/Applications\", \"/Users\"]) query.startQuery() start_time = 0 max_time = 20 while query.isGathering() and start_time \u003c= max_time: start_time += 0.3 NSRunLoop.currentRunLoop().runUntilDate_( NSDate.dateWithTimeIntervalSinceNow_(0.3) ) query.stopQuery() # get the results of the file names, and find their file path via spotlight attribute for item in query.results(): pathname = item.valueForAttribute_(\"kMDItemPath\") if pathname: file_list.append(pathname) return file_list def get_vm_os(vm_list): \"\"\"feed this function a list of results from Spotlight to parse what VM is running what OS\"\"\" # blank list to populate data in os_list = [] # loop through return from spotlight and grab needed info for vm in vm_list: path = str(vm + \"/VmInfo.pvi\") # load XML file into parser tree = et.ElementTree(file=path) root = tree.getroot() # find the text of the tags we want for element in root.iter(): if element.tag == \"RealOsType\" and element.text is not None: # only want the data up until the first comma os_type = element.text.split(\",\")[0] if element.tag == \"RealOsVersion\" and element.text is not None: os_ver = element.text # combine the two pieces of data into single string for the list cstring = os_type + \" \" + os_ver os_list.append(cstring) return os_list def main(): \"\"\"main to rule them all\"\"\" # get VM XML files files = get_vms() # parse XML files results = get_vm_os(files) # loop through and print out multi value EA for jamf inventory print(\"\u003cresult\u003e\") for item in results: print(item) print(\"\u003c/result\u003e\") if __name__ == \"__main__\": main() The above code was pretty much stolen from the Munki Project Munki is probably the best online resource for Python ObjectiveC code. It is filled with tons of gems from the maintainer as well as the community. I think that BYOD deployments are a very bad idea for many reasons. To sum up the biggest two reasons I will simply put: Good luck putting that BYOD device on term hold Good luck putting that BYOD device on legal hold That being stated, over a few beers a year or two ago I was chatting with a buddy of mine about this topic. Spotlight tagging came into the conversation and I found myself bored about a week later one evening so decided to just add something like this to a DEP Notify workflow. So, I posted it here on my GitHub That project is not tested (except for like once in a VM), nor maintained and was really just meant as a code example. Use at your own risk, and think about not doing BYOD if you can. The code uses similar tooling as the binaries above, but allows for the flexibility of Python. It also interacts with the macOS APIs, so some extended functionality is there. So, there are many uses for Spotlight one can use in IT/Ops workflows. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:6","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","Apple","python"],"content":" With the announcement at Catalina’s release that some third party run times will be removed, and the fact that Python 2 is end of life it is time to ship your own. Just like everything in tech, there are many ways to accomplish this. I have been using a tool for about over a year now called relocatable python. The reasons I chose to use relocatable python were pretty good ones in my opinion. They are: Easy to use Builds full self-contained Python environment Easily able to wrap it up in a standard installer PKG Once you have it in an installer package, you can use whatever tools you want to distribute it. Every management tool and application deployment tool should be able to deploy an installer pkg. quick start guide Download the repo from the link above from the directory you wish to download it to git clone https://github.com/gregneagle/relocatable-python.git Look at the --help argument to see what we can do. Ensure you are in the repo folder. % ./make_relocatable_python_framework.py --help Usage: make_relocatable_python_framework.py [options] Options: -h, --help show this help message and exit --destination=DESTINATION Directory destination for the Python.framework --baseurl=BASEURL Override the base URL used to download the framework. --os-version=OS_VERSION Override the macOS version of the downloaded pkg. Current supported versions are \"10.6\" and \"10.9\". Not all Python version and macOS version combinations are valid. --python-version=PYTHON_VERSION Override the version of the Python framework to be downloaded. See available versions at https://www.python.org/downloads/mac-osx/ --pip-requirements=PIP_REQUIREMENTS Path to a pip freeze requirements.txt file that describes extra Python modules to be installed. If not provided, certain useful modules for macOS will be installed. Lets make the folders where we want to create our Python environment % sudo mkdir -p /usr/local/acme/ /usr/local/acme/bin Note: I made two directories there and will explain later why. You can put this anywhere you want. In this example I am using /usr/local but if you want it away from user space you can place it in say something like /opt Now lets build our first Relocatable Python Package sudo ./make_relocatable_python_framework.py --destination=/usr/local/acme --python-version=3.8.5 Note: you will see the tool output a bunch fo stuff in the terminal, let it finish Next we will create our symbolic link to make this easier when we want to call this environment in code # go to the bin folder we created cd /usr/local/acme/bin # create a symbolic link to our new framework sudo ln -s /usr/local/acme/Python.framework/Versions/3.8/bin/python3 python3 # now test it ./python3 Python 3.8.5 (v3.8.5:580fbb018f, Jul 20 2020, 12:11:27) [Clang 6.0 (clang-600.0.57)] on darwin Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. # exit when you are done \u003e\u003e\u003e exit() Finally, you just need to use your preferred packaging tool to add this folder path, including the symbolic link in an Apple Installer Package. ","date":"2020-09-20","objectID":"/posts/shipping-python/:0:0","tags":["macOS","python","python3"],"title":"Shipping Your Own Python to macOS","uri":"/posts/shipping-python/"},{"categories":["macOS","Apple","python"],"content":"Caveats, and things to consider As XKCD has already pointed out, managing a Python Environment can be a chore Installation Path In the above example there are definitely some things to know and consider. To keep the quick start guide less esoteric I just used /usr/local as the directory to deploy my Python 3 environment into. In practice, I actually do not do this. I deploy to /opt/myorg instead to my fleet. I have worked with lots of clever developers over the years, and they need to often install tools to do their job. Those tools often go into /usr/local, thus I stay out of that area. I let user’s have that space to themselves. Which Python? I also have Xcode on this Mac, and I also installed the Python3 Xcode tools as well. % which python3 /usr/bin/python3 When you install that, it does put python3 in the standard $PATH and if your code calls that path or if you just type python3 in the shell, without typing the full path, you will get that Python environment. Some planning will be needed as well as some decision-making by the IT Admin/Engineer that wants to deploy their own Python. There is no gold standard answer, so you will need to figure out what is the best answer for your team and Org. Requirements.txt file The author of Relocatable Python did something clever, but some may have missed it. They made an assumption that if you were to build a Python3 environment to deploy to your macOS fleet you might want the objc bridge, and tools like xattr So, those tools are just included in the default argument. You should note this just in case you plan on customizing your Python3 package to add more Python packages to it. You will need to add those back. One can view the documentation on how this works. To see what packages you have you can use pip to do so: % cd /usr/local/acme/Python.framework/Versions/3.8/bin % ./pip3 list This will output a giant list of packages you have installed. If you want to add more packages you should also include the ones the author of Relocatable Python gave you. When using a requirements.txt file it strictly follows that file. Anything not listed will not get installed. Refer to the --help output we used earlier to add the requirements.txt file sudo ./make_relocatable_python_framework.py --destination=/usr/local/acme --python-version=3.8.5 --pip-requirements=/path/to/requirements.txt Mac Admin Community Python There also those in the Mac Admin community who are already maintaining a public Python3 package which anyone can just go download and/or contribute to. You can find the repo here. This method does make design choices for you. If these design choices are okay with you, then this would be the easiest and fastest way to just ship your own Python3 environment to your fleet. Virtual Environments This is also an option, but it seems most Orgs want to ship their own isolated Python and if they need to create a venv they can do so from the Python they have shipped. Personally, I hae never used a venv outside of my on personal development. I chose to ship the environments, so I can control it. If you want to control venv to a fleet I think shipping your own is the best to start and then build your venv off of that. ","date":"2020-09-20","objectID":"/posts/shipping-python/:0:1","tags":["macOS","python","python3"],"title":"Shipping Your Own Python to macOS","uri":"/posts/shipping-python/"},{"categories":["macOS","Apple","python"],"content":"Tracking Versions, upgrades, and remediation I think in the past ~3 years I have found 2-4 systems total that had bad Python environments I was shipping. I am not exactly sure why they broke, it could have just been a failed installation. So, I started tracking the status and version I was shipping in a simple Jamf EA. Here is a quick example I wrote in the shell: if results=$(/usr/local/acme/bin/python3 -V | awk '{ print $2 }') if\u003e then echo \"\u003cresult\u003e${results}\u003c/result\u003e\" then\u003e else echo \"\u003cresult\u003efalse\u003c/result\u003e\" else\u003e fi \u003cresult\u003e3.8.5\u003c/result\u003e This script will return a false value for any broken Python environment. So, I have an ongoing policy that will reinstall my Python3 environment scoped to this EA with a value of false so this is intentional. I highly recommend you never use blank values in anything you use. Always be explicit with your data, you will never know what a null or blank string value will affect, down or up stream. So, you can track it by the version and create policies to auto remediate any broken Python3 environment you encounter ","date":"2020-09-20","objectID":"/posts/shipping-python/:0:2","tags":["macOS","python","python3"],"title":"Shipping Your Own Python to macOS","uri":"/posts/shipping-python/"},{"categories":["macOS","Apple","python"],"content":"Conclusion and acknowledgements You have many options to choose from. It really is a lot easier than one might think, and if you have any questions or trouble, please join us in #python and #python-beginners on MacAdmins Slack and various community members can assist Acknowledgements: Greg Neagle Mac Admins Slack Mac Admin Community ","date":"2020-09-20","objectID":"/posts/shipping-python/:0:3","tags":["macOS","python","python3"],"title":"Shipping Your Own Python to macOS","uri":"/posts/shipping-python/"},{"categories":["webhooks","jamf","snowflake"],"content":"Shipping Jamf Pro Webhooks to Snowflake Jamf Pro has a builtin feature where you have the ability to ship a webhook event when specific events happen in their application. This allows for a near real time feed of event based data which you can consume and leverage to gain insights about your fleet. Jamf provides some documentation online, found here. Webhooks are generated automatically, and near instantly when an event occurs. In the linked documentation above you will see all the possible webhooks and sample data of what each webhook ships. There are many ways to ship webhooks, and you can choose from many paths to take. You can roll your own webhook receiver, you can ship webhooks directly to cloud storage (S3, Azure Blob, GCP Cloud Storage), use one of many data ingestion tools, and so forth. Right now we are leveraging a tool called Fivetran. Fivetran has built in integration to many SaaS and cloud services. What tools work best for your Organization will be up to your Org to decide. Here is how the data flow looks: First we need to configure Fivetran Snowflake Connector After that create the database in Snowflake, for reference here is the documentation This blog post assumes you have this operating and working, as every Org might have different configurations or requirements in their apps, please refer to the official documentation to ensure this is working. With in Snowflake you can have many warehouses, databases, schemas, and a wide range of RBAC controls in place. So, you may need to adjust some knobs to get this to work. Here is what I have created in my dev account: If you have your database setup now we need to log into Fivetran and create our connector. Find the Webhook Conector with in Fivetran and create a new one. You will want to make sure you already have the database in place and Fivetran has the proper RBAC on that table for data ingestion. Here is an example: There will be a Webhook URL once created which you will need to configure in the Jamf Pro Server. Navigate to Settings \u003e Global Management \u003e Webhooks. Create a new webhook, select the event you want to ship (above example was ComputerCheckin), and make sure you select JSON as the data type. Input the URL you generated in Fivetran and any other options you would like to tweak. You can see that I have my schema set and the table set that matches my dev account in the screenshots. For reference: In Fivetran you can select how often data synchronizes with Snowflake. Since Webhooks are event based data, it is my opinion that the faster you can consume the event, the more valuable that data is. So, I have chosen to ingest every 5 minutes. If you want to pick a different time, you may do so. Your Org may have different needs or requirements, but I do strongly suggest you ingest the webhooks as fast as you can. For example, if you are building data around events, the event based data is more valuable if you can get it in near-realtime. Below are the settings I have configured. So that is it! Just repeat this process for each webhook event you wish to ship to Snowflake. Now you can let the data flow right in, and you are ready to query some of the data and get intelligence off of it. Now for some fun. If you want to find out how many times a specific Policy ID has ran on your fleet with in a specific time frame you can do this quite easily. Since Snowflake is highly scalable and allows one to store massive amounts of data you can keep all your historic data as you see fit. Here is an example query: ALTER SESSION SET TIMEZONE = 'UTC'; select count(*), EVENT:policyId as policy_id , EVENT:computer.jssID as jss_id , EVENT:successful as state from \"JAMF_EVENTSDB\".\"JAMF_EVENTS\".\"POLICIES\" where policy_id = '256' and TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(days, -1, current_timestamp()) group by state, policy_id, jss_id; NOTE: Jamf Pro Webhooks ship with UTC millisecond epoch time stamps COUNT(*) POLICY_ID JSS_ID STATE 2 256 2240 true In the ab","date":"2020-09-19","objectID":"/posts/shipping-jamf-webhooks/:0:0","tags":["data","webhooks","fivetran","jamf","snowflake"],"title":"Shipping Jamf Webhooks with Fivetran","uri":"/posts/shipping-jamf-webhooks/"},{"categories":["data sharing","snowflake"],"content":"Data Sharing is Data Caring ❤️ I have come to the conclusion that there are essentially two types of people when it comes to data. The people that have data, and the people that wish they had the data. Another thought is that I would rather have the data and not need it versus need the data and not have it. For almost the past two years I have had the privilege to work for a great data platform company. The amount of data we have access to increases our return on what we do in the IT/Ops space exponentially. It also helps drive collaboration between teams, allows for centralized data sharing, and enables everyone to make data driven decisions. In a previous life I did have some opportunity to work with various data tools, and use data to help me accomplish my goals. However, I have never had data like I do now. When I worked in vendor space a lot of my exposure to data tools was around what our customer’s wanted. Many of the customers I engaged with wanted to get all their data from my employer’s product. This typically resulted in large projects, and many labor hours to accomplish. There were also almost always caveats with most of the tools we were using that made us make hard decisions. At one previous job I did have more direct exposure and responsibility around a data tool. It took me four to six weeks to get our test instance setup. I had to configure TLS communication for the elastic cluster. Generate certificates, which was a trial by fire process. Then setup the filebeat \u003e logstash \u003e elastic pipeline. At the Logstash level you had to grok your data and create multiple indices, so you were shaping the data before ingesting it. This had a pretty high learning curve and took a lot of time and effort to just proof of concept. Do not get me wrong here, I do think Elastic is a great tool. When I first showed up at Snowflake, I honestly did not know too much about the product other than the basic PR stuff that I had read online, some blog posts, and some Internet posts on various sites. When I wanted to ship webhooks from our MDM solution, I got access to our Fivetran Instance and with in 30-45 minutes of looking at some documentation and tinkering in my dev environment I had webhooks shipping. I could not believe it took me under a hour to figure this out. I was prepared for it to take weeks from previous experiences. One can also just ship the raw data. No need to grok, no need to transform my data before ingesting it, and I have all of it. Enter Data Sharing ❄️ All of our business units ship their data to our data platform. So, every department has their own database, with their own schemas, and their own tables containing all the data they need access to. Since Snowflake’s data platform allows one to have as many databases, warehouses, tables, views, schemas and so forth as they see fit, it allows for easy data sharing. This means we can all share data to each other, and only the data we want to share. All of the data is on the same platform, so you aren’t spinning up a plethora of servers and then configuring them to access each other. The return you get on saved time and labor is already worth it. In the past, I dealt with gatekeepers of data. I was a data gatekeeper myself. IT and Security typically work with each other at most Organizations. Their goals often align with what the business wants. So, to get data you had to deal with each gatekeeper of each system. IT/Ops and Security typically own several systems if not more on each side. Often you would end up with the data gatekeeper emailing you a spreadsheet of the data you requested. If you were lucky you got API access to consume the data on your own. This is not a good experience, and it was definitely not efficient. With Snowflake, we can freely share data between IT/Ops and Security. When the raw data is updated from ingest, all the data shares among our teams is also updated. There is no more always dealing with a gatekeeper and getting a spreadsheet emailed ","date":"2020-09-19","objectID":"/posts/data-sharing/:0:0","tags":["data sharing","snowflake"],"title":"Data Sharing","uri":"/posts/data-sharing/"}]