[{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Gain Oversight of your Asset Inventory with Snowflake Welcome to Part III of my mini blog series on being data driven with Snowflake. This post will be solely dedicated to how an organization can leverage a platform like Snowflake to improve their asset and inventory controls. Both Part I and Part II can be viewed with those links if you haven’t read them yet. This post will focus on end user devices for the most part and not networking hardware, application catalogs, servers, and so forth. The concepts in this post could be applied to those things though. Every Organization I have worked for has struggled a bit with inventory of assets. It is not an easy task to just solve with a single tool or platform. Many aspects of Asset and Inventory control are also done with manual processes. Meaning, a human has to verify objects in asset trackers or change their states when physically verified. Some asset tools I wouldn’t even really consider products, but I would rather consider them platforms instead. This point of view comes from the vast amount of integration these platforms can have. With more integrations you will naturally have more data. This is where data cloud platforms like Snowflake can really enable IT \u0026 Ops teams to use data to drive change and improvements in their asset tools. This is my personal opinion based off the experiences I have learned while working with data in Snowflake. There are always many ways to accomplish the same goals with data. So, this is not the only way one can be data driven around this topic, but my personal opinion this is a wonderful way to start. ","date":"2021-08-10","objectID":"/posts/ops-data-snowflake-pt3/:0:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part III","uri":"/posts/ops-data-snowflake-pt3/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Building Data Pipelines for Success Inventory and Asset data is great by itself, as it can tell and Organization lots of information around their assets inventory. Inventory and Asset data is a lot better with friends. In a previous post I touched on applying baselines and context to your data sets, and this blog post will completely expand on that. Different tool stacks can act as a source of truth for the function is provides. MDM data for example can be the source of truth for all your active endpoints in your fleet checking in online. An Asset tool probably doesn’t have this data, and if you integrate it, you may end up with data lag. Remember, one of the key things about really being data driven is to get data quickly and reliably. If you have to wait days to get data, then you are days behind the current state of things. MDM data though is probably not really great asset data for several reasons. It probably does not tie into HR systems, and MDM systems typically do not support all the configurations around the many assets and Organization can own. At a high level, this is my recommended starting point for data collection: MDM Data Event Data WebHooks Log data HR Data Employee Directory Employee Data (FTE, contractor, start date, end date, etc) Asset and Inventory Data Asset Type (laptop, desktop, Mac, PC, etc) Asset State (primary, secondary, test device, etc) Assignment Data (what human is assigned this device) If you ship data from tools like the listed above, each will act as a source of truth for a specific data point in context. After you have this, you can start creating your baselines of what the data should mean. After you know it, you will be modeling your data and writing queries to get back the data you want on each point. Now, lets break down each data point and see how we can apply it. Event Based MDM Data Event based data is fantastic to get near-real-time data about your fleet. MDM tools that can ship Webhook Events, or any sort of logging that runs on a regular basis to indicate an asset has checked into your MDM tool and submitted data is pretty clutch. The MDM App data is the source of truth for active online endpoints in your fleet that are online and properly communicating to your MDM. Any MDM that can ship event data is pretty key here, if your MDM vendor does not support this I would recommend you file a feature request with them to get this data. It has a lot of value. In this blog I will use Jamf Pro Webhooks as my event based data set. The specific event we care about is the Iventory Event as this event is only generated when a device submits data to Jamf. Devices that do not submit data are considered in a broken or undesired state. Devices not submitting data can easily be put into a different data set, with a different context. Filter data but never get rid of data While mentioning filtering out devices that are not submitting data, this is important in the context of tracking active devices in your fleet. Devices that are not checking in should be grouped into a different context, so you can leverage that data in different ways. Like opening tickets for remediation, or using that data set in a workflow, etc. The WebHook data must contain some sort of primary key in it for it to be valuable. Typically, a serial number is good enough, but the more Unique IDs a hook can ship the better. That would give the IT \u0026 Ops teams more options to work with. The primary key will be used to run joins to match different data sets you share with in the Snowflake platform. You should ingest these WebHooks as often and quickly as possible. They are typically very minimal, so you can store them historically a very long time. HR Data Most Organizations have some sort of Employee Directory, which includes some basic information about each employee that is shared throughout the entire Organization. This data should also be available in Snowflake, and it is very useful for many applications. HR data is often highly","date":"2021-08-10","objectID":"/posts/ops-data-snowflake-pt3/:1:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part III","uri":"/posts/ops-data-snowflake-pt3/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Turning the Data into Magic Assuming that you have all these data pipelines in place, the data is shared, and you can leverage it to build data sets for intelligence we can start to dive into it. In the beginning the data will likely look bad, and do not worry about this. This really should just be expected. If you have never had this data before, you were in the dark about the overall state of it all. As one of my favorite books stated, \"Don't Panic!\" You shouldn’t have an expectation that everything you were doing was correct. The data will turn into magic you can leverage down the road to fix all the gaps in your systems and processes. The data will tell you a story of what you are missing. One of the first things I would establish is some baseline expectations. Something simple, like that every full time staff member is required to have a primary asset in your asset tracker. This is a great place to start, and it might require you to change how you approach asset management. Devices you deploy to end users should have explicit definitions of what the asset is, and this is why I think requiring every full time staff member having a primary asset is a good place to start. Secondary devices could be allowed, but the expectation of a secondary device is that it is used less frequently compared to a primary device. If we define these states in our asset tracking systems, it is quite trivial to leverage that data in Snowflake. Establishing Baselines and Context When you think about managing your fleet from an asset and inventory perspective, it helps to define a new things to establish some baselines. This helps you define the data sets which you can ingest into Snowflake. These things also can help define process as well. Here is a table of some examples one could use in their asset tracking system: Status Definition Notes Primary Primary work computer Every staff member must have a primary work computer assigned to them Secondary Secondary work computer Staff may request a secondary computer if they need one Conference Room Devices that are used in conference rooms These devices can be seen more like appliances and not really end user computers Lost Devices that have been lost Lost devices should be marked as lost and follow the lost device process Stolen Devices that have been stolen Stolen devices should be marked as stolen and follow the stolen device process In-Transit Devices that are being shipped or received Devices that are in the process of being shipped Out for Repairs Devices that have hardware failures Any device that needs hardware repair should be marked as this Testing Devices that are used for testing Test devices are often in weird and non-compliant states use this state for devices you want to filter from compliance. Think like a laptop running beta software. Test devices should never be used as a production device Retired Devices that are no longer being used by anyone Using a “Retired” status means you can track the historical data and filter it Disposed Devices that have been officially eWasted This state could imply you have disposed your old hardware with a certification of the data being destroyed With the example table above, we can now use these data points to make much safer assumptions. We can assume that actual end user computes will only ever be either Primary or Secondary statuses, and we can leverage that in Snowflake. While Conference Room devices we can safely assume those devices are only used for hardware needs in the conference rooms. The Testing status I think is also important to track. As an IT \u0026 Ops person myself, I often have a few tester laptops in my possession. These laptops run beta versions of macOS, Windows 10, various tools and applications in preview or beta release as well. This means my tester laptops are probably never in compliance, and a high probability they are in a non-working state. Test devices aren’t meant to do production work, they are meant to be used to test new ","date":"2021-08-10","objectID":"/posts/ops-data-snowflake-pt3/:2:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part III","uri":"/posts/ops-data-snowflake-pt3/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Putting the Data Together With Snowflake, we can use the power of SQL, and execute queries with joins and leverage data sharing features. Let’s look at a query example: SELECT JI.WEBHOOK:eventTimestamp::varchar::TIMESTAMP_LTZ as last_contact , JI.EVENT:computer:serialNumber::varchar as j_serial from \"DB\".\"SCHEMA\".\"JAMF_CHECKIN\" as JI WHERE TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(days, -30, current_timestamp()) QUALIFY 1 = ROW_NUMBER() OVER (PARTITION BY JI.EVENT:computer:serialNumber::varchar ORDER BY last_contact DESC) and not exists( select upper(SN.SERIAL_NUMBER)::varchar as sn_serial from \"DB\".\"SCHEMA.SERVICENOW_ASSETS_V as SN where sn_serial = JI.EVENT:computer:serialNumber::varchar ) The above query is a method we leverage to find devices that have somehow failed to get into our asset tracker. We all know failures happen, and if we can catch them and fix them, then typically everything is all good. When we do not catch them, and then an incident happens weeks or months later, it does not reflect well on IT. A lot of times adding an asset into your asset tracker is a manual process, or if there is automation with a reseller/vendor, sometimes that fails, or they make a mistake. I have observed all these edge cases in the real world be actual real failures. MDM webhooks will ship to Snowflake each time the device phones home, so this is where event based data is amazing to have. You don’t have to wait for workflows or code to run, just simply wait until the next time the device checks into MDM, and the event will be shipped to your data platform. Visualization Example: The data isn’t just valuable for insights and metrics, it is extremely valuable to help you find gaps in your tech stacks. Typically, everyone is happy when you are proactive, and this is exactly what we are doing in IT/Ops with data in Snowflake. ","date":"2021-08-10","objectID":"/posts/ops-data-snowflake-pt3/:2:1","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part III","uri":"/posts/ops-data-snowflake-pt3/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Reducing the Noise Once you put in some effort into cleaning up your asset tracking systems, and using the data to help guide you, you will find out the return on this investment is quite large. Using the examples from the table above, I now know that actual employees will only ever have a primary, or a secondary computer. Put in the effort up front to improve and sanitize your data inputs, and then reap the benefits downstream when your data gets ingested. Filtering Example select SN.substatus as type , JI.EVENT:emailAddress::varchar as USER , JI.EVENT:osVersion::varchar as OS_VERS , JI.EVENT:serialNumber::varchar as SERIAL_NUMBER , IFF(JI.EVENT:emailAddress::varchar = 'service-acount@company.com' AND JI.EVENT:department::varchar = '', 'conference room', JI.EVENT:department::varchar) as department , WEBHOOK:eventTimestamp::varchar::timestamp_ltz as event_date FROM \"DB\".\"SCHEMA\".\"JAMF_INVENTORY\" as JI inner join \"DB\".\"SCHEMA\".\"SERVICENOW_ASSETS_V\" as SN on SN.SERIAL_NUMBER = JI.EVENT:serialNumber where type in ('Primary', 'Secondary', 'Conference Room') qualify 1 = row_number() over (partition by serial_number order by event_date desc) ; In the above query, we can now reap all the benefits of using data to fix your asset tracking system and get actual real numbers of devices by type and context. In IT Operations, you are typically responsible for patching the OS. SLAs are also attached to patching the OS many times. So, we know that IT must patch all primary, secondary, and conference room computers. These are the computers that have been defined contextually as in use and in production. MDM solutions have no knowledge at all of your asset systems, and your MDM doesn’t understand that perhaps not all devices are actually used in production workflows. What we are accomplishing here is the following: Devices submit inventory to MDM MDM will ship an inventory webhook when the device submits data to MDM Join that data to Service Now to ensure it is a production use device Note: There is also a bit more going on here. A lot of times you might use a service account for a conference room computer, or just a computer that is not used by an actual human. Service accounts don’t have departments, or belong to Orgs/teams really. So, to mark this in our visualization if you detect the service account adn the department info is blank then assign that device as a Conference Room computer as the department ","date":"2021-08-10","objectID":"/posts/ops-data-snowflake-pt3/:2:2","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part III","uri":"/posts/ops-data-snowflake-pt3/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Tracking Broken MDM Clients and Usage Everything has a failure rate in technology. Nothing runs at 100%, even the most highly available and scalable systems humans have ever created cannot guarantee you a 100% uptime. Compound this with secondary devices it adds complexity to the situation. People sometimes have legit business reasons to have more than one computer, and IT should help the people at your Org be as successful as possible. So, how do you track primary and secondary device usage? How would you track when a device stops phoning home to MDM and needs a re-enroll? What if I told you that you could accomplish both of these problems with a single query and a dash of data sharing? with latest_jamf_data as ( SELECT SN.TYPE , SN.SN_EMAIL as email , upper(SN.SERIAL_NUMBER) sn_serial , JI.EVENT:serialNumber::varchar ji_serial , JI.WEBHOOK:eventTimestamp::string::TIMESTAMP_LTZ as ji_last_inv , JI.EVENT:emailAddress::varchar as JI_EMAIL , HR.DEPARTMENT as DEPARTMENT , HR.LAST_DAY_OF_WORK FROM DB.SCHEMA.JAMF_INVENTORY as JI INNER JOIN ( SELECT upper(SERIAL_NUMBER) as SERIAL_NUMBER , SUBSTATUS as type , EMAIL as SN_EMAIL FROM DB.SCHEMA.SERVICENOW_ASSETS_V Where type = 'primary' ) as SN on JI.EVENT:serialNumber::varchar = SN.SERIAL_NUMBER inner join DB.SCHEMA.HR_DATA as HR on SN.SN_EMAIL = HR.PRIMARY_WORK_EMAIL where LAST_DAY_OF_WORK is Null ) SELECT * FROM latest_jamf_data QUALIFY 1 = ROW_NUMBER() over (PARTITION by ji_serial order by ji_last_inv DESC) AND ji_last_inv \u003c dateadd(days, -30, current_timestamp()) ; NOTE: This query is one of several where we made separate dashboard tiles for each status in Service Now. So, you could edit this query to add more states and have a different visual effect if desired. The above query does a few things. It pulls in our MDM data, joins that to service now to ensure we are looking at the devices we want to look at, and then goes even further and joins to HR data to ensure the devices is from an employee that is still actively employed. Why filter out exited employees? So, my philosophy is you never throw away data, but filtering data into specific buckets makes a lot of sense. It makes the data easy to digest and visualize as well. So, while I am only querying devices that are from actively employed employees at my Org, I am also filtering out all exited employees in an entire different set of tiles and data sets. Visual Example Another thing we leveraged from this data, is that it proactively helps us check back in with folks that have secondary devices. Many times an employee just needs a test device to test out a few things for a short period of time. Tracking this data in Snowflake allows us in IT to check back in when the device stops checking in to see if the person still needs it. Many times they are actually done using the device for their secondary purposes and will gladly turn it back into IT. Using Webhook Event data from our MDM tools, along with Service Now data, has allowed us to remediate when things break. It allows us to know very quickly when something is missed. Without a centralized data platform with data sharing I am not sure how one would accomplish this without putting massive amounts of effort and custom dev work into it. Concluding Part III During my data journey so far in IT, I have learned a lot, changed a lot of my views, and used the data to change things with in my organization and collaborate with other teams. Having solid asset data is important for not only audit purposes, but also for it being available to leverage. Tracking the failures and gaps we have with the data in Snowflake is invaluable, as it allows us to be proactive and fix data in our asset tracker all the time. The better our data is in our asset tracking systems, the better it is for everyone to use and leverage. It helps cut out the extra noise and enables your Org to focus on the specific problems at hand. I think this will probably conclude my miniseries of the beginning of my data journe","date":"2021-08-10","objectID":"/posts/ops-data-snowflake-pt3/:2:3","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part III","uri":"/posts/ops-data-snowflake-pt3/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Working with Data in Snowflake Welcome to Part II of my miniseries blog around Snowflake with IT \u0026 Ops Data. If you have not read Part I, you may click that link to read it first. Data has always been very important, and Snowflake just makes using the data a ton easier than we have ever had before. This post will focus on some basic ways one can work with data with in Snowflake. For this blog I will be focusing on JSON data stored in Snowflake columns. JSON documents are very common all over tech. Especially in things like REST APIs, so if you haven’t worked with JSON you will work with JSON data most likely at some point. Snowflake can store semi-structured data in columns natively using the variant data type. This allows IT and Operations people to ingest JSON data in their data pipelines without having to transform the data beforehand. Just ship the JSON data as is right to the platform. IT and Operations folks have also most likely dealt with some sort of SQL based database at some point in their career. Things like: MySQL, Postgres, Oracle, Microsoft SQL, or another SQL based database. So, you might already be familiar enough with SQL. This is another great thing about Snowflake is that you can use all the SQL experience you have gained over the years and apply it right to the platform. There is no esoteric proprietary query language used here, just simply SQL. The best part as an IT \u0026 Ops professional is that Snowflake uses the power and scale of the cloud, meaning IT \u0026 Ops folks don’t ever have to worry about managing the service and platform. No knobs to adjust, no database configurations to deploy, so all you have to do is use the product and let Snowflake take all of that heavy lifting off of your plate! ","date":"2021-04-22","objectID":"/posts/ops-data-snowflake-pt2/:0:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part II","uri":"/posts/ops-data-snowflake-pt2/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Jamf Webhooks Right to Snowflake Tables Since Jamf provides example Webhook Data in their online documentation, we will just use their examples in Snowflake for this blog post. Snowflake can parse JSON natively, meaning you can store JSON data in raw form right in your database. So, lets get started! Create A table with JSON Data Let’s create a table first, so we can insert some data into it: Query: CREATE OR REPLACE TABLE DB.TESTING.JAMF_EVENTS_RAW( date varchar, json_data variant ); the above table has two columns. A “date” column which is set to a varchar data type, json_data which is set to a variant type since JSON is a semi-structured data type. I added raw into the table name to represent a table that might receive raw data from a data ingest pipeline. An empty table is no good without any data in it. I am just copy/pasting the actual example data from Jamf’s developer documentation site linked above. Query: INSERT INTO JAMF_EVENTS_RAW(DATE, JSON_DATA) SELECT current_timestamp(), parse_json($${ \"event\": { \"computer\": { \"alternateMacAddress\": \"72:00:01:DD:A0:B9\", \"building\": \"Block D\", \"department\": \"Information Technology\", \"deviceName\": \"John's MacBook Pro\", \"emailAddress\": \"john.smith@company.com\", \"jssID\": 13, \"macAddress\": \"60:03:08:A3:64:9D\", \"model\": \"13-inch Retina MacBook Pro (Late 2013)\", \"osBuild\": \"16G29\", \"osVersion\": \"10.12.6\", \"phone\": \"555-472-9829\", \"position\": \"Desktop Services Specialist\", \"realName\": \"John Smith\", \"room\": \"487\", \"serialNumber\": \"C02M23PJFH50\", \"udid\": \"EBBFF74D-C6B7-5599-93A9-19E8BDDEFE32\", \"userDirectoryID\": \"-1\", \"username\": \"john.smith\" }, \"trigger\": \"CLIENT_CHECKIN\", \"username\": \"John Smith\" }, \"webhook\": { \"eventTimestamp\": 1553550275590, \"id\": 7, \"name\": \"Webhook Documentation\", \"webhookEvent\": \"ComputerCheckIn\" } }$$); I repeated this process a few times to get several rows of data. I just picked a few random examples of the hooks from Jamf’s website and reran the above query. Snowflake has a built in parse_json function that will interpret the string as JSON data, and produce it as a variant data type Let’s check our work but running a SELECT * on our table and see what sort of data we get back. Query: SELECT * FROM JAMF_EVENTS_RAW; Screenshot of rows: Screenshot of some JSON data: Parsing the Jamf Data Now that we have some data in there, lets look at how one could get some data out of it: Query: SELECT JSON_DATA:event.computer.serialNumber as SERIAL_NUMBER , JSON_DATA:event.computer.osVersion as OS FROM JAMF_EVENTS_RAW WHERE JSON_DATA:webhook.webhookEvent = 'ComputerPolicyFinished' ; Results: SERIAL_NUMBER OS \"C02M23PJFH50\" \"10.14.3\" In the above query and results you can see that Snowflake can just natively parse raw JSON/Variant data stored in a column. No big deal, right? The syntax is pretty simple. It is simply one single: to tell Snowflake to interpret the rest as JSON/Variant. After that first : you can swap back to . and, you don’t have to hold down the shift key a ton. It is really that simple and at the same time powerful. ","date":"2021-04-22","objectID":"/posts/ops-data-snowflake-pt2/:1:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part II","uri":"/posts/ops-data-snowflake-pt2/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Organizing Your Data and Leveraging Views With data like Jamf Pro Webhooks, you probably want to organize your data in a way where each event type is in its own table. We will first create a new table, just like our previous one, and this time we will use it for check-in hook events from Jamf Pro. It would not make a ton of sense to toss two or more different types of hook data sets in the same table. Event data is about capturing what events are taking place and mixing two different event types has no benefit and would make your queries way more complex. Since we already have our table from before that worked, we can use like to create another table just like it. Query: CREATE TABLE DB.TESTING.JAMF_EVENTS_CHECKINS like DB.TESTING.JAMF_EVENTS_RAW ; Following the same steps previously mentioned in this blog post, I inserted some check-in webhook events into my new table I just created using the example data from Jamf’s website. Now that the raw JSON example data is in a fresh table I can now look at creating a view. For more information please check out Snowflake’s Documentation on views. I will only share a few pieces of the data in my view that might be relevant to the requester of said data. Data owners can always pick and choose what they share with other people and teams using Snowflake, and this is such a fantastic thing! Query: CREATE OR REPLACE VIEW JAMF_EVENTS_CHECKINS_V AS SELECT JSON_DATA:\"event\":\"computer\":\"building\"::VARCHAR AS BUILDING ,JSON_DATA:\"event\":\"computer\":\"department\"::VARCHAR AS DEPARTMENT ,JSON_DATA:\"event\":\"computer\":\"emailAddress\"::VARCHAR AS EMAIL ,JSON_DATA:\"event\":\"computer\":\"realName\"::VARCHAR AS FULL_NAME ,JSON_DATA:\"webhook\":\"eventTimestamp\"::VARCHAR::TIMESTAMP_LTZ AS TIME FROM DB.TESTING.JAMF_EVENTS_CHECKINS ; NOTE: Jamf Pro Webhooks use epoch timestamps, so I decided to type cast it as more readable time stamp. This is another great quality feature of Snowflake. See the screenshot and query results below, it displays a human-readable time stamp now. This query also is only sharing a relevant data set to the view, and not the entire raw data of the hook itself. I also added a _v to the view name to visually indicate it is a view. The short version is that a view can turn query results into a table. This is a very cool feature, and useful to just convert queries you write into a view. I am also specifying data types here, please refer to the official docs to learn more about data types in Snowflake. Let’s take a look at the data set we just created with a view. Query: SELECT * FROM DB.TESTING.JAMF_EVENTS_CHECKINS_V; Results: BUILDING DEPARTMENT EMAIL FULL_NAME TIME Block D Information Technology john.smith@company.com John Smith 2019-03-25 14:44:35.590 -0700 Block D Information Technology john.smith@company.com John Smith 2019-03-25 14:44:35.590 -0700 Block D Information Technology jane.smith@company.com Jane Smith 2019-03-25 14:44:35.590 -0700 Block D Information Technology jane.smith@company.com Jane Smith 2019-03-25 14:44:35.590 -0700 Block D Information Technology jane.smith@company.com Jane Smith 2019-03-25 14:44:35.590 -0700 Block D Information Technology jane.smith@company.com Jane Smith 2019-03-25 14:44:35.590 -0700 Apple Park Executive tim.cook@company.com Tim Cook 2019-03-25 14:54:35.590 -0700 Apple Park Executive tim.cook@company.com Tim Cook 2019-03-25 14:54:35.590 -0700 Apple Park Executive tim.cook@company.com Tim Cook 2019-03-25 14:54:35.590 -0700 Features like this come in very handy when you want to share data to other people and teams. The view above is a view that just shows some basic data for the webhook event of client check-in. When sharing data to others, they probably do not need to access the raw data. Parts of the data could be confusing, not needed, or even considered confidential. Features like this give the data owners the power to share exactly what they want, nothing more, nothing less. ","date":"2021-04-22","objectID":"/posts/ops-data-snowflake-pt2/:2:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part II","uri":"/posts/ops-data-snowflake-pt2/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Data Sharing with Snowflake Data sharing is such a game changer. It is hard to describe in some ways, because I feel that you have to experience it to really see the absolute beauty that data sharing is. In the past I have had to deal with many APIs, CSV reports, and in some cases I had to log into an app to generate a report and download it. This is time-consuming, and very labor intensive. The end result was typically a bunch of scripts, emails with attachments, and then me having to cobble together data from these sources and documents. I never have to do any of that ever again. That is a thing of the past, a relic, things lost in time like tears in the rain. A quick high level example of how data sharing empowers IT \u0026 Ops workers could be around inventory and asset control. An IT \u0026 Ops professional could have access to basic HR data (think employee directory), asset data in the Org’s asset tracker, and then MDM data from the MDM tool you use. All of which is directly ingested into Snowflake. Zero API calls, zero CSV parsing, and definitely not logging into random apps to run reports manually. With this simple data set I can check if a terminated employee’s laptop is checking into MDM, or if the asset in the asset tracker is not set to some sort of term hold state. One could also ensure that every FTE of the Org has a primary use computer assigned to them by simply joining the HR data to the asset tracker data, and then filtering for every full time employee that is missing a primary use computer. Role Based Access Controls (RBAC) Role Based Access is an industry standard that is used in many things. The general idea is you create roles to do certain functions, and only grant those roles the least amount of permissions and access they need to do their jobs. A user of a system will assume that role when they authenticate to the tech stack they are working in. Here are the official docs for how Snowflake handles access and ownership of objects with in Snowflake itself. Using part of the example above, lets assume Security Engineering has a request into IT to consume data around end user computers checking into the Org’s MDM. For the use case we will also use that they are trying to track employee laptops where the employee has left the company, but the laptop is still acitvely online and checking into MDM. This would be considered a risk the Security Team wants to monitor and alert on. Let’s create a role called SECENG_BFF, since IT and Security should be best friends forever. Grant that role SELECT access to the view we just created before. This will be the view we are data sharing to Security Engineering, so they may audit and monitor data in the scenario described. Query: CREATE ROLE SECENG_BFF; GRANT USAGE ON WAREHOUSE DB_WH TO ROLE SECENG_BFF; GRANT USAGE ON DATABASE DB TO ROLE SECENG_BFF; GRANT SELECT ON VIEW DB.TESTING.JAMF_EVENTS_CHECKINS_V TO ROLE SECENG_BFF; GRANT ROLE SECENG_BFF TO USER \"USER.NAME@ORG.COM\"; DANGER WARNING - this is in no way an example of a best practice when it comes to granting access to data. Please consult your security teams, and the official documentation to ensure you are using the proper security and access controls. Roles do need some level of basic access to warehouses and databases to access views and tables with in those objects. This example is not advice on how anyone should organize and use role based access. However, now we can test our role to see if it can SELECT against the view we just created. USE ROLE SECENG_BFF; SELECT * FROM DB.TESTING.JAMF_EVENTS_CHECKINS_V; If you get a return on the data, then you have now successfully created a view based off raw data, created a role to access that data, and finally granted a person that role. Folks working with in Snowflake can assume roles through the App GUI, or by running a query USE ROLE \u003cROLE_NAME\u003e, and users can have multiple roles. My SELECT * query ran fine, and returned all the same results in the example earlier in this blog p","date":"2021-04-22","objectID":"/posts/ops-data-snowflake-pt2/:3:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part II","uri":"/posts/ops-data-snowflake-pt2/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Data Sharing is Limitless In this post I have only really touched on internal data sharing between teams with in your Organization. Snowflake also has an entire platform for third party data sharing, called the Snowflake Data Marketplace IT and Operations teams now have the ability to directly share and directly consume data from with in your Organization. This enables people to access and consume data freely with your Org once granted access. Often IT \u0026 Ops teams need to share data with Security, and Compliance and Governance teams. Data sharing makes this extremely easy. It also allows data owners to share only the exact data they need to share, and scope the data to specific teams or people. IT \u0026 Ops teams can also easily consume data from other teams as well, and this is such a wonderful thing. When you share the same data across teams, then all teams are looking at the same data. Historically, you might have had an experience where data is not centralized and shared from the same data sources, and ended up with different teams having different data sets. When you have the ability to ship all your raw data to a single data platform and share your data with people and teams across your Org, you get rid of all those legacy methods you used to have to deal with. Concluding Part II Thanks for reading if you got this far. This concludes Part II of my miniseries on being data driven with Snowflake. The thing I would like to close on is that Snowflake makes data ingest extremely easy. Most APIs or data shippers can ship JSON data natively, and Snowflake can just consume that natively. No need to transform your data in the shipping and ingest process. Then having the ability to just work with variant data with in Snowflake is honestly just sort of mind-blowing. This also means the integration opportunity for IT \u0026 Ops tools is exponentially broad. If you can get IT \u0026 Ops JSON data into cloud storage you are pretty much just done at that point. The rest is using the data in Snowflake itself. There will be a Part III, but I have not fully planned out what that will be just yet, so stay tuned please. ","date":"2021-04-22","objectID":"/posts/ops-data-snowflake-pt2/:4:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part II","uri":"/posts/ops-data-snowflake-pt2/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Be Data Driven in IT/Ops with Snowflake Technology changes very fast, and it keeps getting bigger and more complex. We have so many systems these days, our systems have systems. It doesn’t have to be complex and daunting to get reliable, and up-to-date data. Getting accurate data, while getting it fast, is key to developing data driven strategies. We no longer have to live in the constraints of capacity, limited storage, and limited scalability. With the power and scale of the cloud, you can now simply just ship all of your IT/Ops data to a centralized location and share it among your teams. IT/Ops shops often find some sort of niche tool that was designed for their purpose. These tools are fine, but they are often complex, and require specialized knowledge. Then sometimes you are faced with figuring out what data to ship. For example, some data tools might transform the data during the shipping process. Breaking it out into say a JSON doc, and then creating indices for these files, and tagging them with metadata. This made it even more complex as you would have to design all of these things up front, or suffer the rite of passage of remodeling your data ingest and re-indexing your data when you need to change something. Snowflake has solved these problems in a pretty straight forward way. With Snowflake, you just ship all your data. You can then use post processing to make the data more actionable. Ship all the raw data you can, and then store that raw data in a schema with in the Snowflake platform. No need to spend countless hours of labor on this. You also do not have to worry about indexing, or tagging your data. You can just post process all your raw data with in Snowflake. ","date":"2021-04-15","objectID":"/posts/ops-data-snowflake/:0:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part I","uri":"/posts/ops-data-snowflake/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Before you Begin When you get to the point of having multiple data pipelines into a data tool which you can easily share data from multiple sources of truth, you must convince yourself of the following: You will probably be wrong The data will show you when you are wrong (when you are right too!) Use the data to fix what is wrong Fix things upstream, so next data ingest it will be fixed downstream Establish baselines, sources of truth and context to your data Data with out a source of turth or context is sometimes not as valuable I really cannot stress this enough. I had to admit some methods I had been using for years, which I thought to be pretty solid, were in fact wrong. I also stopped treating MDM data as a single source of truth. When you think about it, this makes actual logical sense. MDM admins often only have the MDM tools to work with though, so they build deep knowledge and skills around these tools. So much, that it gives them Tunnel Vision The big lesson learned here is that the MDM collecting third party agent data is at best a nice to have, but the actual source of truth is cloud data from the agent’s SaaS. That is what matters the most in context. Not if some Launch Daemon is running, or an agent binary can return stdout, or the plethora of other tricks us Mac Admins have come up with over the years to verify if local agents are actually healthy. I found out through collecting data that methods I had been using for years really were not as effective as I thought they were. I discovered I was wrong. To define a healthy agent, that agent has to submit data back to its cloud tenant, and if the agent is not submitting data, then it does not matter what the local state data says. So, now I have set my source of truth to the App stack each agent talks to, and the context of the data is around when was the last time that agent phoned home and submitted data? The rest of the data are just nice to haves, and sometimes that data is not as useful as we thought it was. All I had to do was build my context and request that set of data to the teams that owned that data, and they shared that data to me directly. Now I have the ability to query agent connection statuses from the actual source of truth of the agent, and I am not relying on MDM data to detect these failures for me. All queries and data here are samples from actual data. While they are small sample sets, and things have been changed to share in this blog, the concepts are very real world applicable. ","date":"2021-04-15","objectID":"/posts/ops-data-snowflake/:1:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part I","uri":"/posts/ops-data-snowflake/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"What Data Should I ship? If you are new to shipping data, and aren’t quite sure what to ship, here are some high level starting points I recommend: Asset Data (hardware, software, applications, etc) Event Data (webhooks, event based APIs, etc) Log Data (any logs that you need info on, or just ship all the logs) Tools Data (MDM, security agents, etc) ","date":"2021-04-15","objectID":"/posts/ops-data-snowflake/:2:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part I","uri":"/posts/ops-data-snowflake/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Use Asset Tracking Systems as a Source of Truth At my current work, we have Service Now as our asset management system. We track what Org assets are assigned to what human in this system. Also, we spent lots of effort cleaning up our sub statuses of assets in Service Now. Things like classifying systems as primary, or as secondary. We have also classified assets that are designated as Zoom Room computers. Leveraging data like this helps a ton on things you haven’t even planned for yet. Anything you add upstream in Service Now, will be ingested downstream into Snowflake later on. So, we use Service Now as our source of truth for asset tracking. One example is that we query to check if primary assets are checking into MDM in the last 30 days. We chose 30 days as the metric for this simply because employees take time off. Plus there are holidays, and many other reasons an employee would not be working. However, it is probably extremely rare for an employee to be out of the office and offline for over 30 days. Example Query: Primary Assets from Intune that have not checked in for over 30 days // get latest compliance log from Intune on Win10 clients over 30 days SELECT SN.SUBSTATUS as type , SN.EMAIL as email , SN.SERIAL_NUMBER sn_serial , IM.JSON_DATA:properties.SerialNumber::varchar im_serial , IM.JSON_DATA:properties.LastContact::varchar as im_last_inv FROM DB.SCHEMA.SERVICENOW_ASSETS as SN JOIN DB.SCHEMA.INTUNE_DEVICECOMPLIANCE_LOGS as IM on SN.SERIAL_NUMBER = IM.JSON_DATA:properties.SerialNumber Where type = 'primary' QUALIFY 1 = ROW_NUMBER() OVER (PARTITION BY im_serial ORDER BY im_last_inv DESC) AND TO_TIMESTAMP(IM.JSON_DATA:properties.LastContact) \u003c dateadd(days, -30, current_timestamp()) ; Example Query: Primary Assets from Jamf that have not checked in over 30 days // get lastest inventory webhook from jamf on clients over 30 days SELECT SN.SUBSTATUS as type , SN.EMAIL as email , upper(SN.SERIAL_NUMBER) sn_serial , JI.EVENT:serialNumber::varchar ji_serial , JI.WEBHOOK:eventTimestamp::string::TIMESTAMP_LTZ as ji_last_inv FROM DB.SCHEMA.SERVICENOW_ASSETS as SN INNER JOIN DB.SCHEMA.JAMF_INVENTORY as JI on SN.SERIAL_NUMBER = JI.EVENT:serialNumber Where type = 'primary' QUALIFY 1 = ROW_NUMBER() OVER (PARTITION BY ji_serial ORDER BY ji_last_inv DESC) AND TO_TIMESTAMP(JI.WEBHOOK:eventTimestamp::INTEGER/1000) \u003c dateadd(days, -30, current_timestamp()) ; Discoveries from the Data All software agents can break, MDM clients can get into weird states, and even things like an OS update can cause issues. A lot of times you just need to reboot the computer to fix these too. This is just an easy way to report on them and make the data actionable. We are also using Service Now as the source of truth here and not our MDM services. This is really just because MDM solutions aren’t normally true asset management systems. Use the right tool for the right job. Visualization Example: ","date":"2021-04-15","objectID":"/posts/ops-data-snowflake/:3:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part I","uri":"/posts/ops-data-snowflake/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"What If Systems Are Missing From the Asset Tracker? If you are an Org that allows BYOD, you might not have a great way to tell what devices are BYOD. Devices sometimes also may not end up in your asset system for whatever reason. With having multiple data sources in Snowflake you can easily join tables from these sources to craft such data sets. BYOD systems are probably not in your asset tracker as they are not Org owned. A failure also may occur in your process to import assets, or data was entered wrong. This is also pretty easy to solve when you have access to the proper data. Example Query: Devices Checking into MDM but not present in Service Now SELECT JI.WEBHOOK:eventTimestamp::varchar::TIMESTAMP_LTZ as last_contact , JI.EVENT:computer:serialNumber::varchar as j_serial from DB.SCHEMA.JAMF_CHECKIN as JI WHERE TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(days, -30, current_timestamp()) QUALIFY 1 = ROW_NUMBER() OVER (PARTITION BY JI.EVENT:computer:serialNumber::varchar ORDER BY last_contact DESC) and not exists( select upper(SN.SERIAL_NUMBER)::varchar as sn_serial from DB.SCHEMA.SERVICENOW_ASSETS as SN where sn_serial = JI.EVENT:computer:serialNumber::varchar ) ; In the above query we are using each unique device’s last check-in into Jamf from the Webhook Event in the past 30 days, and we only need the event data for this. Then if we cannot join that unique ID (serial number in this case) to the Service Now assets table, then it does not exist in Service Now. You also now have all the serial numbers which you can use to create those asset objects in your asset tracker. Visualization Example: Discoveries from the Data What we discovered is that most of the time when a device does not show up in Service Now, there is likely some sort of data entry issue. Either our automation with our resellers had a glitch, or perhaps a human mistyped a character in for an asset that was manually entered. We also discovered that sometimes if you scan a serial number barcode off of the box the laptop was shipped in, it will append the beginning of the string with a S. On rare occasion we just missed getting it in our asset tracker. My favorite discovery though by far with this data point, was that if a human enters a serial number in Service Now with all lower case characters, it will fail on the table join. The matching is case-sensitive. To fix this, I simply now sanitize my data in post processing by converting all serial numbers to all upper case with the upper() function. This goes to show you how powerful this data platform is. Sure, we could have fixed our data upstream, but will you ever be able to really stop humans from making simple human mistakes? Having this data allowed us to drill down into each edge case and failure point and discover why it was happening. With the best part being is that we have the data to fix it! ","date":"2021-04-15","objectID":"/posts/ops-data-snowflake/:4:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part I","uri":"/posts/ops-data-snowflake/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Using Event Based Data to Monitor Jamf Cloud SaaS is great! I love SaaS Apps, because as an IT/Ops person that means all the management of the App is on the vendor. I can focus on other things, and not have to worry about the app itself, nor the infrastructure required to self host the App. One of the very few downsides of using SaaS is that you often lack access to data. This is especially true when looking at operational data, or access to data that requires direct access to the host. In this case we will be using event based data shipped from Jamf Cloud Webhooks. Example Query: Total Number of Events every hour SELECT ( SELECT COUNT(*) FROM DB.SCHEMA.JAMF_APIOPS where TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(hours, -1, current_timestamp()) ) as APIOPS, ( SELECT COUNT(*) FROM DB.SCHEMA.JAMF_CHECKIN where TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(hours, -1, current_timestamp()) ) as CHECKINS, ( SELECT COUNT(*) FROM DB.SCHEMA.JAMF_INVENTORY where TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(hours, -1, current_timestamp()) ) as INVENTORIES, ( SELECT COUNT(*) FROM DB.SCHEMA.JAMF_POLICIES where TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(hours, -1, current_timestamp()) ) as POLICIES, ( SELECT COUNT(*) FROM DB.SCHEMA.JAMF_CHECKIN where TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(hours, -1, current_timestamp()) AND EVENT:trigger = 'enrollmentComplete' ) as ENROLLMENT ; In the above query I am tracking the total count of each event type by the common events we want to track from our Jamf Cloud instance. The above query also is tracking the number of events per the last hour, and you could adjust this part of the query dateadd(hours, -1, current_timestamp()) to expand the time threshold. Example Query: Total API Operations in last 24 hours select count(*) as total , EVENT:authorizedUsername::varchar as usr , EVENT:operationSuccessful as state from DB.SCHEMA.JAMF_APIOPS where TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(days, -1, current_timestamp()) group by state, usr ; Caveat - Come to find out Jamf does not ship event hooks on failed API operations unfortunately. I have brought this to their attention and asked for this as a feature request. In this case I think tracking failures is more important than the successful operations. Visualization Example: Discoveries from the Data This data is useful to monitor our Jamf Cloud instance. If events start to spike heavily in either an uptrend, or a downtrend pattern, something is probably wrong. I did use this data to detect an impacted workflow from an API bug a year or so back. Basically, an API call from JSS-Importer had failed updating a smart group. The actual failure (after tracking it down) was that the Jamf API had timed out and returned a 502. However, it actually did complete the PUT to the smart group, but failed to update the package payload of a policy. This resulted in a policy to install one version of an app, and a smart group based on a different version of the same app. I only caught this because my fleet was constantly spamming my Jamf Cloud with over 80,000 recon in a 24-hour period. ","date":"2021-04-15","objectID":"/posts/ops-data-snowflake/:5:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part I","uri":"/posts/ops-data-snowflake/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Security and Compliance Data Every Org has a responsibility to their stakeholders, employees (or staff/students), and definitely their customers around security. This is a very big and encompassing subject. One could write entire books on this subject alone. So, for this blog post I am going to focus on disk encryption. Which is a part of the CIS framework, and a very common requirement. Example Query: FV2 Status from Jamf SELECT JI.EMAIL , JI.SERIAL_NUMBER , PART.VALUE:\"filevault2_status\"::VARCHAR AS FV2_STATUS , JI.GENERAL:\"last_contact_time_epoch\"::string::TIMESTAMP_LTZ AS jamf_last_contact FROM DB.SCHEMA.JAMF_INVENTORY JI ,LATERAL FLATTEN(JI.HARDWARE:\"storage\") AS VOL ,LATERAL FLATTEN(VOL.VALUE:\"partitions\") AS PART WHERE PART.VALUE:\"type\"::VARCHAR = 'boot' AND EMAIL != 'exclusion-user@company.com' AND jamf_last_contact \u003e dateadd(day, -30, current_timestamp()) QUALIFY 1 = ROW_NUMBER() OVER (PARTITION BY JI.SERIAL_NUMBER ORDER BY jamf_last_contact DESC) ; Example Query: FV2 Overall Score from Jamf WITH X AS ( SELECT JI.EMAIL , JI.SERIAL_NUMBER , PART.VALUE:\"filevault2_status\"::VARCHAR AS FV2_STATUS , JI.GENERAL:\"last_contact_time_epoch\"::string::TIMESTAMP_LTZ AS jamf_last_contact FROM DB.SCHEMA.JAMF_INVENTORY JI ,LATERAL FLATTEN(JI.HARDWARE:\"storage\") AS VOL ,LATERAL FLATTEN(VOL.VALUE:\"partitions\") AS PART WHERE PART.VALUE:\"type\"::VARCHAR = 'boot' AND EMAIL != 'exclusion-user@company.com' AND jamf_last_contact \u003e dateadd(day, -30, current_timestamp()) QUALIFY 1 = ROW_NUMBER() OVER (PARTITION BY JI.SERIAL_NUMBER ORDER BY jamf_last_contact DESC) ) SELECT COUNT_IF(FV2_STATUS != 'Not Encrypted') / NULLIFZERO(COUNT(*)) AS PERCENTAGE FROM X ; This will return the current FV2 status for every device record that has checked into Jamf in the last 30 days. Devices that have not submitted inventory or checked into Jamf in greater than 30 days go into a separate violation, and will be remediated in a separate flow. Any device returning Pending, Encrypting, and Encrypted are considered in a desired state, and Not Encrypted will be used to calculate the percentage of devices in an undesired state. NOTE: These queries do include an example of how to exclude say a service account that is used on non-human used devices that may not require full disk encryption for their specific use case. Discoveries from the Data This data was one of the easiest ones to address. Jamf will report 4 different states of disk encryption from a device record. They include: Encrypted, Encrypting, Pending, Not Encrypted, and each of these is very self-explanatory. In this data point Not Encrypted is the violation we are looking for. If devices stay in Pending or in Encrypting for long periods of time, then we consider that a violation. The failure point is almost always FV2 was deployed, but the user never rebooted, or in rare instance the MDM just failed to fully enable it. ","date":"2021-04-15","objectID":"/posts/ops-data-snowflake/:6:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part I","uri":"/posts/ops-data-snowflake/"},{"categories":["IT data","snowflake","asset management","Ops Data"],"content":"Closing of Part I This is still the beginning of my data journey, and I have much to learn and do still. What I would like to close with, is that having all these data points shared to me and my teams, has really changed the game. This data ends debates, answers questions, shows area of improvement, and is actionable. The last thing I would like to point out is that data drives collaboration. I partner with our security teams here, and we data share to each other. I share our fleet data with them, they share their security and agent data with me. Now when I engage with security, we both look at what the data is telling us, and we collaborate on it. ","date":"2021-04-15","objectID":"/posts/ops-data-snowflake/:7:0","tags":["data sharing","snowflake","data driven","IT tools"],"title":"IT \u0026 Ops Data with Snowflake Part I","uri":"/posts/ops-data-snowflake/"},{"categories":["macOS","scripting"],"content":"Spotlight is a system wide metadata indexing system that Apple has been shipping since OS X 10.4 Tiger, and has been improved over the years with each OS. I like using Spotlight for some tasks and general searching for files in Terminal.app. I also use it for finding anything on my Mac. Typically, I do not use my mouse or trackpad to find files or navigate to folders. One just needs to hit cmd + spacebar on their Mac to pull up the Spotlight search menu and start typing. If you use a Mac, there is a good chance you also use this regularly like I do. This isn’t anything groundbreaking or new, but I have enjoyed using Spotlight over the years when it fits as a good tool. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:0","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Spotlight in the Terminal getting attributes There are two binaries Apple supplies on macOS which you can leverage. They are mdls and mdfind, there is also another tool called xattr which also allows you to manipulate metadata in code. To start using these binaries we can just take a look at an application like say Firefox. Refer to their man pages for the documentation. % mdls /Applications/Firefox.app _kMDItemDisplayNameWithExtensions = \"Firefox.app\" kMDItemAlternateNames = ( \"Firefox.app\" ) kMDItemAppStoreCategory = \"Productivity\" kMDItemAppStoreCategoryType = \"public.app-category.productivity\" kMDItemCFBundleIdentifier = \"org.mozilla.firefox\" kMDItemContentCreationDate = 2020-08-31 18:05:40 +0000 kMDItemContentCreationDate_Ranking = 2020-08-31 00:00:00 +0000 kMDItemContentModificationDate = 2020-09-08 17:37:19 +0000 kMDItemContentModificationDate_Ranking = 2020-09-08 00:00:00 +0000 kMDItemContentType = \"com.apple.application-bundle\" kMDItemContentTypeTree = ( \"com.apple.application-bundle\", \"com.apple.application\", \"public.executable\", \"com.apple.localizable-name-bundle\", \"com.apple.bundle\", \"public.directory\", \"public.item\", \"com.apple.package\" ) kMDItemDateAdded = 2020-09-08 17:35:57 +0000 kMDItemDateAdded_Ranking = 2020-09-08 00:00:00 +0000 kMDItemDisplayName = \"Firefox\" kMDItemDocumentIdentifier = 0 kMDItemExecutableArchitectures = ( \"x86_64\" ) kMDItemFSContentChangeDate = 2020-09-08 17:37:19 +0000 kMDItemFSCreationDate = 2020-08-31 18:05:40 +0000 kMDItemFSCreatorCode = \"\" kMDItemFSFinderFlags = 0 kMDItemFSHasCustomIcon = (null) kMDItemFSInvisible = 0 kMDItemFSIsExtensionHidden = 1 kMDItemFSIsStationery = (null) kMDItemFSLabel = 0 kMDItemFSName = \"Firefox.app\" kMDItemFSNodeCount = 1 kMDItemFSOwnerGroupID = 80 kMDItemFSOwnerUserID = 0 kMDItemFSSize = 210713091 kMDItemFSTypeCode = \"\" kMDItemInterestingDate_Ranking = 2020-09-08 00:00:00 +0000 kMDItemKind = \"Application\" kMDItemLastUsedDate = 2020-09-08 17:37:09 +0000 kMDItemLastUsedDate_Ranking = 2020-09-08 00:00:00 +0000 kMDItemLogicalSize = 210713091 kMDItemPhysicalSize = 211038208 kMDItemUseCount = 1 kMDItemUsedDates = ( \"2020-09-08 07:00:00 +0000\" ) kMDItemVersion = \"80.0.1\" There are many metadata tags we can get right from the shell as well. % mdls /Applications/Firefox.app -name kMDItemVersion kMDItemVersion = \"80.0.1\" You can see this is easy to get the metadata from an object on disk, it is also very fast as it searches the indexes and returns the indexed data. % mdls /Applications/Firefox.app -name kMDItemVersion -raw 80.0.1 There is also no need to pipe to awk or grep as the tool gives you an argument of -raw to just return the data. Easily get multiple metadata attributes by passing multiple arguments. % mdls /Applications/Firefox.app -name kMDItemVersion -name kMDItemFSName -name kMDItemLastUsedDate kMDItemFSName = \"Firefox.app\" kMDItemLastUsedDate = 2020-09-08 17:37:09 +0000 kMDItemVersion = \"80.0.1\" ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:1","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Spotlight in the Terminal searching mdls is great for listing the metadata for objects on disk, and you can probably guess what mdfind is used for. After getting the metadata tags from mdls we can use them in mdfind you can also pass strings to mdfind and it performs searches similar to when you use it in the Finder. % mdfind \"using-spotlight\" /Users/tlarkin/blog/tlark/content/posts/using-spotlight-macos.md /Users/tlarkin/Library/Application Support/JetBrains/IdeaIC2020.1/workspace/1hksXRhYWu8MI2RnUXoH3oitxRW.xml Like finding the blog post I am currently working on, which my IDE is also storing data about on disk. This output makes sense considering I do all my blog work in my IDE with hugo along with built in tools. We can also use the metadata tags which we see in mdls output. % mdfind \"kMDItemFSName = Firefox.app\" /Applications/Firefox.app You can specify file paths if you have an idea or what to limit search scope for metadata searching. % mdfind -onlyin /Users/tlarkin \"kMDItemContentType = public.python-script\" | wc -l 11582 I have a lot of Python scripts on my Mac, so I just did a line count of the output. I have a lot of repos cloned, open source tools cloned, and there are a lot of apps that use Python, so a big chunk of these results are from the software I have installed. However, every object in my home folder that has kMDItemContentType = public.python-script tag will return from this search. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:2","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Using mdfind with mdls Of course with things like pipes in the shell you can easily use these tools together. % mdfind -0 -onlyin /Applications \"kMDItemContentType = com.apple.application-bundle\" | xargs -0 mdls -name kMDItemCFBundleIdentifier -name kMDItemDisplayName -name kMDItemVersion There is a lot of output I won’t paste into a code block here, but you can play around with this to see the different output you can get from these binaries. Last year there was a pretty nasty iTerm2 Vuln that we detected through our security tools, and the fact it was all over Twitter and the rest of the Internet. We wanted to detect how many vulnerable versions we had and then plan on patching after assessing how many vuln versions we had. A simple spotlight script was deployed, and anything that wasn’t the current version got flagged. Since iTerm2 is just a zipped App bundle, we had no idea where the user installed it. We also do not block users from installing their own software and tools, so there could be multiple versions present. Tools like Jamf Pro will not search for Apps outside of a default path unless you specify so in the inventory collection preferences. A quick Spotlight script made sense and it didn’t require much effort. There are tools out there like OSquery which is probably a better answer to get data about your fleet on a regular basis. #!/bin/zsh results=$(mdfind -name \"kMDItemCFBundleIdentifier = com.googlecode.iterm2 \u0026\u0026 kMDItemVersion != 3.3.6\") if [[ \"${results}\" == \"\" ]] then echo \"\u003cresult\u003efalse\u003c/result\u003e\" else echo \"\u003cresult\u003etrue\u003c/result\u003e\" fi With a day we had the data back, and we had very few users with a vulnerable version on their system as most users were updating their versions on their own. So, we just had to send an update to a few systems. I do think there are better ways and better tools to manage application state out there, but if you don’t have those every Mac has Spotlight on it. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:3","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Metadata Tagging None of this is new, and I am certainly not the first person to use these tools or blog about this stuff. The linked blog has some pretty good reading material you can reference. A quick reference how you can do this below. # create a file touch file.txt # add a custom tag xattr -w com.apple.metadata:\"_customTag\" \"customVal1\" file.txt # get the attribute mdls -name _customTag file.txt -raw customVal1 # search for it mdfind \"_customTag = customVal1\" /Users/tlarkin/file.txt There are a lot of things you could do with metadata tagging in macOS. You can programmatically search for metadata tags, as well as set your own tags. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:4","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Caveats with Custom Tagging When tagging things on disk that get overwritten, like contents of an installer package, the tags could be deleted. So, if you were to say tag an app you installed with your automation tools, then a user downloads another version and overwrites it, your tags, are now probably gone. Some file paths are not indexed by Spotlight. Paths like /tmp are not indexed. So, make sure you test the paths you are using if you use these tools. I have definitely forgotten this before and tested custom tags with xattr in /tmp and completely spaced that this doesn’t work. Some file names are error prone with certain characters. Apple, I think has fixed this bug as I can now tag a file located in a folder with a . in the folder name. This was broken at some point in time, Mac Mule discovered it. I was not able to locate his blog post on it, but will update with a link if I can find it. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:5","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","scripting"],"content":"Python Objc Bridge You can also do all of these neat things in Python as well. I wrote this script not too long ago to search and find all the Parallels VMs on a Mac. The end goal was to collect the OS versions of all VMs and ship that data to Snowflake, so we could report on them for vulnerabilities on the OS. #!/usr/bin/python \"\"\" this is a script to detect what VMs are on a system and escrow what OS they are You can run this daily or in another workflow It will use Spotlight to find files with the .pvm extension to locate where the files are on the file system then parse the PvInfo file for Parallels VMs credit: https://github.com/munki/munki/blob/b6a4b015297c262124fb80086b6b55e329a4fec0/code/client/munkilib/info.py#L362-L396 \"\"\" # import modules import xml.etree.ElementTree as et from Foundation import NSMetadataQuery, NSPredicate, NSRunLoop, NSDate # start functions def get_vms(): \"\"\"use spotlight to find parallels VM files\"\"\" file_list = [] query = NSMetadataQuery.alloc().init() query.setPredicate_( NSPredicate.predicateWithFormat_( \"(kMDItemContentType == 'com.parallels.vm.vmpackage')\" ) ) query.setSearchScopes_([\"/Applications\", \"/Users\"]) query.startQuery() start_time = 0 max_time = 20 while query.isGathering() and start_time \u003c= max_time: start_time += 0.3 NSRunLoop.currentRunLoop().runUntilDate_( NSDate.dateWithTimeIntervalSinceNow_(0.3) ) query.stopQuery() # get the results of the file names, and find their file path via spotlight attribute for item in query.results(): pathname = item.valueForAttribute_(\"kMDItemPath\") if pathname: file_list.append(pathname) return file_list def get_vm_os(vm_list): \"\"\"feed this function a list of results from Spotlight to parse what VM is running what OS\"\"\" # blank list to populate data in os_list = [] # loop through return from spotlight and grab needed info for vm in vm_list: path = str(vm + \"/VmInfo.pvi\") # load XML file into parser tree = et.ElementTree(file=path) root = tree.getroot() # find the text of the tags we want for element in root.iter(): if element.tag == \"RealOsType\" and element.text is not None: # only want the data up until the first comma os_type = element.text.split(\",\")[0] if element.tag == \"RealOsVersion\" and element.text is not None: os_ver = element.text # combine the two pieces of data into single string for the list cstring = os_type + \" \" + os_ver os_list.append(cstring) return os_list def main(): \"\"\"main to rule them all\"\"\" # get VM XML files files = get_vms() # parse XML files results = get_vm_os(files) # loop through and print out multi value EA for jamf inventory print(\"\u003cresult\u003e\") for item in results: print(item) print(\"\u003c/result\u003e\") if __name__ == \"__main__\": main() The above code was pretty much stolen from the Munki Project Munki is probably the best online resource for Python ObjectiveC code. It is filled with tons of gems from the maintainer as well as the community. I think that BYOD deployments are a very bad idea for many reasons. To sum up the biggest two reasons I will simply put: Good luck putting that BYOD device on term hold Good luck putting that BYOD device on legal hold That being stated, over a few beers a year or two ago I was chatting with a buddy of mine about this topic. Spotlight tagging came into the conversation and I found myself bored about a week later one evening so decided to just add something like this to a DEP Notify workflow. So, I posted it here on my GitHub That project is not tested (except for like once in a VM), nor maintained and was really just meant as a code example. Use at your own risk, and think about not doing BYOD if you can. The code uses similar tooling as the binaries above, but allows for the flexibility of Python. It also interacts with the macOS APIs, so some extended functionality is there. So, there are many uses for Spotlight one can use in IT/Ops workflows. ","date":"2020-09-23","objectID":"/posts/using-spotlight-macos/:0:6","tags":["macOS","spotlight","scripting"],"title":"Using Spotlight with macOS","uri":"/posts/using-spotlight-macos/"},{"categories":["macOS","Apple","python"],"content":"With the announcement at Catalina’s release that some third party run times will be removed, and the fact that Python 2 is end of life it is time to ship your own. Just like everything in tech, there are many ways to accomplish this. I have been using a tool for about over a year now called relocatable python. The reasons I chose to use relocatable python were pretty good ones in my opinion. They are: Easy to use Builds full self contained Python environment Easily able to wrap it up in a standard installer PKG Once you have it in an installer package, you can use whatever tools you want to distribute it. Every management tool and application deployment tool should be able to deploy an installer pkg. quick start guide Download the repo from the link above from the directory you wish to download it to git clone https://github.com/gregneagle/relocatable-python.git Look at the --help argument to see what we can do. Ensure you are in the repo folder. % ./make_relocatable_python_framework.py --help Usage: make_relocatable_python_framework.py [options] Options: -h, --help show this help message and exit --destination=DESTINATION Directory destination for the Python.framework --baseurl=BASEURL Override the base URL used to download the framework. --os-version=OS_VERSION Override the macOS version of the downloaded pkg. Current supported versions are \"10.6\" and \"10.9\". Not all Python version and macOS version combinations are valid. --python-version=PYTHON_VERSION Override the version of the Python framework to be downloaded. See available versions at https://www.python.org/downloads/mac-osx/ --pip-requirements=PIP_REQUIREMENTS Path to a pip freeze requirements.txt file that describes extra Python modules to be installed. If not provided, certain useful modules for macOS will be installed. Lets make the folders where we want to create our Python environment % sudo mkdir -p /usr/local/acme/ /usr/local/acme/bin Note: I made two directories there and will explain later why. You can put this anywhere you want. In this example I am using /usr/local but if you want it away from user space you can place it in say something like /opt Now lets build our first Relocatable Python Package sudo ./make_relocatable_python_framework.py --destination=/usr/local/acme --python-version=3.8.5 Note: you will see the tool output a bunch fo stuff in the terminal, let it finish Next we will create our symbolic link to make this easier when we want to call this environment in code # go to the bin folder we created cd /usr/local/acme/bin # create a symbolic link to our new framework sudo ln -s /usr/local/acme/Python.framework/Versions/3.8/bin/python3 python3 # now test it ./python3 Python 3.8.5 (v3.8.5:580fbb018f, Jul 20 2020, 12:11:27) [Clang 6.0 (clang-600.0.57)] on darwin Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. # exit when you are done \u003e\u003e\u003e exit() Finally, you just need to use your preferred packaging tool to add this folder path, including the symbolic link in an Apple Installer Package. ","date":"2020-09-20","objectID":"/posts/shipping-python/:0:0","tags":["macOS","python","python3"],"title":"Shipping Your Own Python to macOS","uri":"/posts/shipping-python/"},{"categories":["macOS","Apple","python"],"content":"Caveats, and things to consider As XKCD has already pointed out, managing a Python Environment can be a chore Installation Path In the above example there are definitely some things to know and consider. To keep the quick start guide less esoteric I just used /usr/local as the directory to deploy my Python 3 environment into. In practice, I actually do not do this. I deploy to /opt/myorg instead to my fleet. I have worked with lots of clever developers over the years, and they need to often install tools to do their job. Those tools often go into /usr/local, thus I stay out of that area. I let user’s have that space to themselves. Which Python? I also have Xcode on this Mac, and I also installed the Python3 Xcode tools as well. % which python3 /usr/bin/python3 When you install that, it does put python3 in the standard $PATH and if your code calls that path or if you just type python3 in the shell, without typing the full path, you will get that Python environment. Some planning will be needed as well as some decision-making by the IT Admin/Engineer that wants to deploy their own Python. There is no gold standard answer, so you will need to figure out what is the best answer for your team and Org. Requirements.txt file The author of Relocatable Python did something clever, but some may have missed it. They made an assumption that if you were to build a Python3 environment to deploy to your macOS fleet you might want the objc bridge, and tools like xattr So, those tools are just included in the default argument. You should note this just in case you plan on customizing your Python3 package to add more Python packages to it. You will need to add those back. One can view the documentation on how this works. To see what packages you have you can use pip to do so: % cd /usr/local/acme/Python.framework/Versions/3.8/bin % ./pip3 list This will output a giant list of packages you have installed. If you want to add more packages you should also include the ones the author of Relocatable Python gave you. When using a requirements.txt file it strictly follows that file. Anything not listed will not get installed. Refer to the --help output we used earlier to add the requirements.txt file sudo ./make_relocatable_python_framework.py --destination=/usr/local/acme --python-version=3.8.5 --pip-requirements=/path/to/requirements.txt Mac Admin Community Python There also those in the Mac Admin community who are already maintaining a public Python3 package which anyone can just go download and/or contribute to. You can find the repo here. This method does make design choices for you. If these design choices are okay with you, then this would be the easiest and fastest way to just ship your own Python3 environment to your fleet. Virtual Environments This is also an option, but it seems most Orgs want to ship their own isolated Ptyhon and if they need to create a venv they can do so from the Python they have shipped. Personally, I hae never used a venv outside of my on personal development. I chose to ship the environments, so I can control it. If you want to control venv to a fleet I think shipping your own is the best to start and then build your venv off of that. ","date":"2020-09-20","objectID":"/posts/shipping-python/:0:1","tags":["macOS","python","python3"],"title":"Shipping Your Own Python to macOS","uri":"/posts/shipping-python/"},{"categories":["macOS","Apple","python"],"content":"Tracking Versions, upgrades, and remediation I think in the past ~2 years I have found 2-4 systems total that had bad Python environments I was shipping. I am not exactly sure why they broke, it could have just been a failed install. So, I started tracking the status and version I was shipping in a simple Jamf EA. Here is a quick example I wrote in the shell: if results=$(/usr/local/acme/bin/python3 -V | awk '{ print $2 }') if\u003e then echo \"\u003cresult\u003e${results}\u003c/result\u003e\" then\u003e else echo \"\u003cresult\u003efalse\u003c/result\u003e\" else\u003e fi \u003cresult\u003e3.8.5\u003c/result\u003e This script will return a false value for any broken Python environment. So, I have an ongoing policy that will reinstall my Python3 environment scoped to this EA with a value of false so this is intentional. I highly recommend you never use blank values in anything you use. Always be explicit with your data, you will never know what a null or blank string value will affect, down or up stream. So, you can track it by the version and create policies to auto remediate any broken Python3 environment you encounter ","date":"2020-09-20","objectID":"/posts/shipping-python/:0:2","tags":["macOS","python","python3"],"title":"Shipping Your Own Python to macOS","uri":"/posts/shipping-python/"},{"categories":["macOS","Apple","python"],"content":"Conclusion and acknowledgements You have many options to choose from. This is not too difficult as I have been doing this for coming up on 2 years now. If I can do this, so can everyone else. Acknowledgements: Greg Neagle Mac Admins Slack Mac Admin Community ","date":"2020-09-20","objectID":"/posts/shipping-python/:0:3","tags":["macOS","python","python3"],"title":"Shipping Your Own Python to macOS","uri":"/posts/shipping-python/"},{"categories":["webhooks","jamf","snowflake"],"content":"Shipping Jamf Pro Webhooks to Snowflake Jamf Pro has a builtin feature where you have the ability to ship a webhook event when specific events happen in their application. This allows for a near real time feed of event based data which you can consume and leverage to gain insights about your fleet. Jamf provides some documentation online, found here. Webhooks are generated automatically, and near instantly when an event occurs. In the linked documentation above you will see all the possible webhooks and sample data of what each webhook ships. There are many ways to ship webhooks, and you can choose from many paths to take. You can roll your own webhook receiver, you can ship webhooks directly to cloud storage (S3, Azure Blob, GCP Cloud Storage), use one of many data ingestion tools, and so forth. Right now we are leveraging a tool called Fivetran. Fivetran has built in integration to many SaaS and cloud services. What tools work best for your Organization will be up to your Org to decide. Here is how the data flow looks: First we need to configure Fivetran Snowflake Connector After that create the database in Snowflake, for reference here is the documentation This blog post assumes you have this operating and working, as every Org might have different configurations or requirements in their apps, please refer to the official documentation to ensure this is working. With in Snowflake you can have many warehouses, databases, schemas, and a wide range of RBAC controls in place. So, you may need to adjust some knobs to get this to work. Here is what I have created in my dev account: If you have your database setup now we need to log into Fivetran and create our connector. Find the Webhook Conector with in Fivetran and create a new one. You will want to make sure you already have the database in place and Fivetran has the proper RBAC on that table for data ingestion. Here is an example: There will be a Webhook URL once created which you will need to configure in the Jamf Pro Server. Navigate to Settings \u003e Global Management \u003e Webhooks. Create a new webhook, select the event you want to ship (above example was ComputerCheckin), and make sure you select JSON as the data type. Input the URL you generated in Fivetran and any other options you would like to tweak. You can see that I have my schema set and the table set that matches my dev account in the screenshots. For reference: In Fivetran you can select how often data synchronizes with Snowflake. Since Webhooks are event based data, it is my opinion that the faster you can consume the event, the more valuable that data is. So, I have chosen to ingest every 5 minutes. If you want to pick a different time, you may do so. Your Org may have different needs or requirements, but I do strongly suggest you ingest the webhooks as fast as you can. For example, if you are building data around events, the event based data is more valuable if you can get it in near-realtime. Below are the settings I have configured. So that is it! Just repeat this process for each webhook event you wish to ship to Snowflake. Now you can let the data flow right in, and you are ready to query some of the data and get intelligence off of it. Now for some fun. If you want to find out how many times a specific Policy ID has ran on your fleet with in a specific time frame you can do this quite easily. Since Snowflake is highly scalable and allows one to store massive amounts of data you can keep all your historic data as you see fit. Here is an example query: ALTER SESSION SET TIMEZONE = 'UTC'; select count(*), EVENT:policyId as policy_id , EVENT:computer.jssID as jss_id , EVENT:successful as state from \"JAMF_EVENTSDB\".\"JAMF_EVENTS\".\"POLICIES\" where policy_id = '256' and TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(days, -1, current_timestamp()) group by state, policy_id, jss_id; NOTE: Jamf Pro Webhooks ship with UTC millisecond epoch time stamps COUNT(*) POLICY_ID JSS_ID STATE 2 256 2240 true In the ab","date":"2020-09-19","objectID":"/posts/shipping-jamf-webhooks/:0:0","tags":["data","webhooks","fivetran","jamf","snowflake"],"title":"Shipping Jamf Webhooks with Fivetran","uri":"/posts/shipping-jamf-webhooks/"},{"categories":["data sharing","snowflake"],"content":"Data Sharing is Data Caring ❤️ I have come to the conclusion that there are essentially two types of people when it comes to data. The people that have data, and the people that wish they had the data. Another thought is that I would rather have the data and not need it versus need the data and not have it. For almost the past two years I have had the privilege to work for a great data platform company. The amount of data we have access to increases our return on what we do in the IT/Ops space exponentially. It also helps drive collaboration between teams, allows for centralized data sharing, and enables everyone to make data driven decisions. In a previous life I did have some opportunity to work with various data tools, and use data to help me accomplish my goals. However, I have never had data like I do now. When I worked in vendor space a lot of my exposure to data tools was around what our customer’s wanted. Many of the customers I engaged with wanted to get all their data from my employer’s product. This typically resulted in large projects, and many labor hours to accomplish. There were also almost always caveats with most of the tools we were using that made us make hard decisions. At one previous job I did have more direct exposure and responsibility around a data tool. It took me four to six weeks to get our test instance setup. I had to configure TLS communication for the elastic cluster. Generate certificates, which was a trial by fire process. Then setup the filebeat \u003e logstash \u003e elastic pipeline. At the Logstash level you had to grok your data and create multiple indices, so you were shaping the data before ingesting it. This had a pretty high learning curve and took a lot of time and effort to just proof of concept. Do not get me wrong here, I do think Elastic is a great tool. When I first showed up at Snowflake, I honestly did not know too much about the product other than the basic PR stuff that I had read online, some blog posts, and some Internet posts on various sites. When I wanted to ship webhooks from our MDM solution, I got access to our Fivetran Instance and with in 30-45 minutes of looking at some documentation and tinkering in my dev environment I had webhooks shipping. I could not believe it took me under a hour to figure this out. I was prepared for it to take weeks from previous experiences. One can also just ship the raw data. No need to grok, no need to transform my data before ingesting it, and I have all of it. Enter Data Sharing ❄️ All of our business units ship their data to our data platform. So, every department has their own database, with their own schemas, and their own tables containing all the data they need access to. Since Snowflake’s data platform allows one to have as many databases, warehouses, tables, views, schemas and so forth as they see fit, it allows for easy data sharing. This means we can all share data to each other, and only the data we want to share. All of the data is on the same platform, so you aren’t spinning up a plethora of servers and then configuring them to access each other. The return you get on saved time and labor is already worth it. In the past, I dealt with gatekeepers of data. I was a data gatekeeper myself. IT and Security typically work with each other at most Organizations. Their goals often align with what the business wants. So, to get data you had to deal with each gatekeeper of each system. IT/Ops and Security typically own several systems if not more on each side. Often you would end up with the data gatekeeper emailing you a spreadsheet of the data you requested. If you were lucky you got API access to consume the data on your own. This is not a good experience, and it was definitely not efficient. With Snowflake, we can freely share data between IT/Ops and Security. When the raw data is updated from ingest, all the data shares among our teams is also updated. There is no more always dealing with a gatekeeper and getting a spreadsheet emailed ","date":"2020-09-19","objectID":"/posts/data-sharing/:0:0","tags":["data sharing","snowflake"],"title":"Data Sharing","uri":"/posts/data-sharing/"}]