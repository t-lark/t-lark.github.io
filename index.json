<<<<<<< HEAD
[{"categories":["webhooks","jamf","snowflake"],"content":"Shipping Jamf Pro Webhooks to Snowflake Jamf Pro has a builtin feature where you have the ability to ship a webhook event when specific events happen in their application. This allows for a near real time feed of event based data which you can consume and leverage to gain insights about your fleet. Jamf provides some documentation online, found here. Webhooks are generated automatically, and near instantly when an event occurs. In the linked documentation above you will see all the possible webhooks and sample data of what each webhook ships. There are many ways to ship webhooks, and you can choose from many paths to take. You can roll your own webhook receiver, you can ship webhooks directly to cloud storage (S3, Azure Blob, GCP Cloud Storage), use one of many data ingestion tools, and so forth. Right now we are leveraging a tool called Fivetran. Fivetran has built in integration to many SaaS and cloud services. What tools work best for your Organization will be up to your Org to decide. Here is how the data flow looks: First we need to configure Fivetran Snowflake Connector After that create the database in Snowflake, for reference here is the documentation This blog post assumes you have this operating and working, as every Org might have different configurations or requirements in their apps, please refer to the official documentation to ensure this is working. With in Snowflake you can have many warehouses, databases, schemas, and a wide range of RBAC controls in place. So, you may need to adjust some knobs to get this to work. Here is what I have created in my dev account: If you have your database setup now we need to log into Fivetran and create our connector. Find the Webhook Conector with in Fivetran and create a new one. You will want to make sure you already have the database in place and Fivetran has the proper RBAC on that table for data ingestion. Here is an example: There will be a Webhook URL once created which you will need to configure in the Jamf Pro Server. Navigate to Settings \u003e Global Management \u003e Webhooks. Create a new webhook, select the event you want to ship (above example was ComputerCheckin), and make sure you select JSON as the data type. Input the URL you generated in Fivetran and any other options you would like to tweak. You can see that I have my schema set and the table set that matches my dev account in the screenshots. For reference: In Fivetran you can select how often data synchronizes with Snowflake. Since Webhooks are event based data, it is my opinion that the faster you can consume the event, the more valuable that data is. So, I have chosen to ingest every 5 minutes. If you want to pick a different time, you may do so. Your Org may have different needs or requirements, but I do strongly suggest you ingest the webhooks as fast as you can. For example, if you are building data around events, the event based data is more valuable if you can get it in near-realtime. Below are the settings I have configured. So that is it! Just repeat this process for each webhook event you wish to ship to Snowflake. Now you can let the data flow right in, and you are ready to query some of the data and get intelligence off of it. Now for some fun. If you want to find out how many times a specific Policy ID has ran on your fleet with in a specific time frame you can do this quite easily. Since Snowflake is highly scalable and allows one to store massive amounts of data you can keep all your historic data as you see fit. Here is an example query: ALTER SESSION SET TIMEZONE = 'UTC'; select count(*), EVENT:policyId as policy_id , EVENT:computer.jssID as jss_id , EVENT:successful as state from \"JAMF_EVENTSDB\".\"JAMF_EVENTS\".\"POLICIES\" where policy_id = '256' and TO_TIMESTAMP(WEBHOOK:eventTimestamp::INTEGER/1000) \u003e dateadd(days, -1, current_timestamp()) group by state, policy_id, jss_id; NOTE: Jamf Pro Webhooks ship with UTC millisecond epoch time stamps COUNT(*) POLICY_ID JSS_ID STATE 2 256 2240 true In the ab","date":"2020-09-19","objectID":"/posts/shipping-jamf-webhooks/:0:0","tags":["data","webhooks","fivetran","jamf","snowflake"],"title":"Shipping Jamf Webhooks with Fivetran","uri":"/posts/shipping-jamf-webhooks/"},{"categories":["data sharing","snowflake"],"content":"Data Sharing is Data Caring ❤️ I have come to the conclusion that there are essentially two types of people when it comes to data. The people that have data, and the people that wish they had the data. Another thought is that I would rather have the data and not need it versus need the data and not have it. For almost the past two years I have had the privilege to work for a great data platform company. The amount of data we have access to increases our return on what we do in the IT/Ops space exponentially. It also helps drive collaboration between teams, allows for centralized data sharing, and enables everyone to make data driven decisions. In a previous life I did have some opportunity to work with various data tools, and use data to help me accomplish my goals. However, I have never had data like I do now. When I worked in vendor space a lot of my exposure to data tools was around what our customer’s wanted. Many of the customers I engaged with wanted to get all their data from my employer’s product. This typically resulted in large projects, and many labor hours to accomplish. There were also almost always caveats with most of the tools we were using that made us make hard decisions. At one previous job I did have more direct exposure and responsibility around a data tool. It took me four to six weeks to get our test instance setup. I had to configure TLS communication for the elastic cluster. Generate certificates, which was a trial by fire process. Then setup the filebeat \u003e logstash \u003e elastic pipeline. At the Logstash level you had to grok your data and create multiple indices, so you were shaping the data before ingesting it. This had a pretty high learning curve and took a lot of time and effort to just proof of concept. Do not get me wrong here, I do think Elastic is a great tool. When I first showed up at Snowflake, I honestly did not know too much about the product other than the basic PR stuff that I had read online, some blog posts, and some Internet posts on various sites. When I wanted to ship webhooks from our MDM solution, I got access to our Fivetran Instance and with in 30-45 minutes of looking at some documentation and tinkering in my dev environment I had webhooks shipping. I could not believe it took me under a hour to figure this out. I was prepared for it to take weeks from previous experiences. One can also just ship the raw data. No need to grok, no need to transform my data before ingesting it, and I have all of it. Enter Data Sharing ❄️ All of our business units ship their data to our data platform. So, every department has their own database, with their own schemas, and their own tables containing all the data they need access to. Since Snowflake’s data platform allows one to have as many databases, warehouses, tables, views, schemas and so forth as they see fit, it allows for easy data sharing. This means we can all share data to each other, and only the data we want to share. All of the data is on the same platform, so you aren’t spinning up a plethora of servers and then configuring them to access each other. The return you get on saved time and labor is already worth it. In the past, I dealt with gatekeepers of data. I was a data gatekeeper myself. IT and Security typically work with each other at most Organizations. Their goals often align with what the business wants. So, to get data you had to deal with each gatekeeper of each system. IT/Ops and Security typically own several systems if not more on each side. Often you would end up with the data gatekeeper emailing you a spreadsheet of the data you requested. If you were lucky you got API access to consume the data on your own. This is not a good experience, and it was definitely not efficient. With Snowflake, we can freely share data between IT/Ops and Security. When the raw data is updated from ingest, all the data shares among our teams is also updated. There is no more always dealing with a gatekeeper and getting a spreadsheet emailed ","date":"2020-09-19","objectID":"/posts/data-sharing/:0:0","tags":["data sharing","snowflake"],"title":"Data Sharing","uri":"/posts/data-sharing/"}]
=======
[{"categories":["data sharing","snowflake"],"content":"Data Sharing is Data Caring ❤️ I have come to the conclusion that there are essentially two types of people when it comes to data. The people that have data, and the people that don’t have data. Another thought is that I would rather have the data and not need it versus need the data and not have it. For almost the past two years I have had the privilege to work for a great data platform company. The amount of data we have access to increases our return on what we do in the IT/Ops space exponentially. It also helps drive collaboration between teams, allows for centralized data sharing, and enables everyone to make data driven decisions. In a previous life I did have some opportunity to work with various data tools, and use data to help me accomplish my goals. However, I have never had data like I do now. When I worked in vendor space a lot of my exposure to data tools was around what our customer’s wanted. Many of the customers I engaged with wanted to get all their data from my employer’s product. This typically resulted in large projects, and many labor hours to accomplish. There were also almost always caveats with most of the tools we were using that made us make hard decisions. At one previous job I did have more direct exposure and responsibility around a data tool. It took me four to six weeks to get our test instance setup. I had to configure TLS communication for the elastic cluster. Generate certificates, which was a trial by fire process. Then setup the filebeat \u003e logstash \u003e elastic pipeline. At the Logstash level you had to grok your data and create multiple indices, so you were shaping the data before ingesting it. This had a pretty high learning curve and took a lot of time and effort to just proof of concept. Do not get me wrong here, I do think Elastic is a great tool. When I first showed up at Snowflake, I honestly did not know too much about the product other than the basic PR stuff that I had read online, some blog posts, and some Internet posts on various sites. When I wanted to ship webhooks from our MDM solution, I got access to our Fivetran Instance and with in 30-45 minutes of looking at some documentation and tinkering in my dev environment I had webhooks shipping. I could not believe it took me under a hour to figure this out. I was prepared for it to take weeks from previous experiences. One can also just ship the raw data. No need to grok, no need to transform my dat before ingesting it, and I have all of it. Enter Data Sharing ❄️ All of our business units ship their data to our data platform. So, every department has their own database, with their own schemas, and their own tables containing all the data they need access to. Since Snowflake’s data platform allows one to have as many databases, warehouses, tables, views, schemas and so forth as they see fit, it allows for easy data sharing. This means we can all share data to each other, and only the data we want to share. All of the data is on the same platform, so you aren’t spinning up a plethora of servers and then configuring them to access each other. The return you get on saved time and labor is already worth it. In the past, I dealt with gatekeepers of data. I was a data gatekeeper myself. IT and Security typically work with each other at most Organizations. Their goals often align with what the business wants. So, to get data you had to deal with each gatekeeper of each system. IT/Ops and Security typically own several systems if not more on each side. Often you would end up with the data gatekeeper emailing you a spreadsheet of the data you requested. If you were lucky you got API access to consume the data on your own. This is not a good experience, and it was definitely not efficient. With Snowflake, we can freely share data between IT/Ops and Security. When the raw data is updated from ingest, all the data shares among our teams is also updated. There is no more always dealing with a gatekeeper and getting a spreadsheet emailed to you. ","date":"2020-09-19","objectID":"/posts/data-sharing/:0:0","tags":["data sharing","snowflake"],"title":"Data Sharing","uri":"/posts/data-sharing/"}]
>>>>>>> ec900ab6b5266eec529115e37d8c849b992f3d75
